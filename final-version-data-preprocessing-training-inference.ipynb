{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2091b131",
   "metadata": {
    "papermill": {
     "duration": 0.003789,
     "end_time": "2025-11-17T10:21:31.535665",
     "exception": false,
     "start_time": "2025-11-17T10:21:31.531876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **AN2DL First Challenge: Preprocessing & Modeling**\n",
    "> ## ***Ibuprofen*** **Team**\n",
    ">\n",
    "> **Team Members:**\n",
    "> * Angelo Notarnicola (279710)\n",
    "> * Daniele Piano (249385)\n",
    "> * Luca Spreafico (303871)\n",
    "> * Michele Leggieri (244615)\n",
    ">\n",
    "> This notebook contains the final pipeline that achieved our top score (0.9425). It implements the preprocessing, feature engineering, and the hybrid CNN-LSTM model architecture derived from our data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8120b77e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T10:21:31.543109Z",
     "iopub.status.busy": "2025-11-17T10:21:31.542590Z",
     "iopub.status.idle": "2025-11-17T10:21:37.809887Z",
     "shell.execute_reply": "2025-11-17T10:21:37.808934Z"
    },
    "papermill": {
     "duration": 6.272509,
     "end_time": "2025-11-17T10:21:37.811233",
     "exception": false,
     "start_time": "2025-11-17T10:21:31.538724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Kaggle...\n",
      "Using Device: cuda\n",
      "Working Directory: /kaggle/working\n",
      "Input Directory: /kaggle/input/piratt\n"
     ]
    }
   ],
   "source": [
    "### 1.1. Environment Setup (Colab & Kaggle)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "import re\n",
    "import warnings\n",
    "import math\n",
    "import random\n",
    "\n",
    "# --- Reproducibility & Hardware ---\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Environment Paths ---\n",
    "DATASET_NAME = \"piratt\"  # Kaggle dataset name\n",
    "COLAB_PATH = \"/gdrive/My Drive/Colab Notebooks/[2025-2026] AN2DL/First Challenge - Data Analysis\" # Adjust this path\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print(\"Running on Google Colab...\")\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/gdrive\", force_remount=True)\n",
    "    WORKING_DIR = COLAB_PATH\n",
    "    INPUT_DIR = WORKING_DIR  # Assuming data is in the same Colab folder\n",
    "    %cd $WORKING_DIR\n",
    "    \n",
    "elif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "    print(\"Running on Kaggle...\")\n",
    "    WORKING_DIR = \"/kaggle/working\"\n",
    "    # The \"golden script\" used /kaggle/input/pirate/\n",
    "    # We will try a common path first, then fallback to that\n",
    "    if os.path.exists(f\"/kaggle/input/{DATASET_NAME}\"):\n",
    "        INPUT_DIR = f\"/kaggle/input/{DATASET_NAME}\"\n",
    "    else:\n",
    "        INPUT_DIR = \"/kaggle/input/pirate\" # Fallback to the golden script's path\n",
    "    \n",
    "else:\n",
    "    print(\"Running locally...\")\n",
    "    WORKING_DIR = os.getcwd()\n",
    "    INPUT_DIR = \"./\" # Adjust as needed\n",
    "\n",
    "print(f\"Using Device: {device}\")\n",
    "print(f\"Working Directory: {WORKING_DIR}\")\n",
    "print(f\"Input Directory: {INPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7566c3",
   "metadata": {
    "papermill": {
     "duration": 0.002786,
     "end_time": "2025-11-17T10:21:37.817213",
     "exception": false,
     "start_time": "2025-11-17T10:21:37.814427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.2. Global Parameters & Design Choices\n",
    "\n",
    "Based on the `data_analysis.ipynb` notebook, we define our global hyperparameters.\n",
    "\n",
    "#### INSIGHT 1: CLASS IMBALANCE\n",
    "* **Analysis:** Severe imbalance (`no_pain` ~77%, `high_pain` ~4.5%).\n",
    "* **Action:** Implement `WeightedRandomSampler` (balance batches) and `FocalLoss` (focus on hard examples).\n",
    "\n",
    "#### INSIGHT 2: STATIC FEATURES RELIABILITY\n",
    "* **Analysis:** Static features (`n_legs`, `n_hands`, `n_eyes`) showed spurious correlations (e.g., <1% 'pirates' in train/test).\n",
    "* **Action:** **Drop all static features**. They are noise. `STATIC_COLS` is empty.\n",
    "\n",
    "#### INSIGHT 3: TEMPORAL DYNAMICS & WINDOWING\n",
    "* **Analysis:** Autocorrelation (ACF) plots showed a signal \"memory\" of ~30-40 time steps.\n",
    "* **Action:** Set **`WINDOW_SIZE = 40`** to capture these long-term patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10e90687",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T10:21:37.824082Z",
     "iopub.status.busy": "2025-11-17T10:21:37.823740Z",
     "iopub.status.idle": "2025-11-17T10:21:37.827926Z",
     "shell.execute_reply": "2025-11-17T10:21:37.827383Z"
    },
    "papermill": {
     "duration": 0.008903,
     "end_time": "2025-11-17T10:21:37.829055",
     "exception": false,
     "start_time": "2025-11-17T10:21:37.820152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Columns definition\n",
    "JOINT_COLS = [f'joint_{i:02d}' for i in range(30)] # joint_30 is excluded\n",
    "SURVEY_COLS = ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
    "STATIC_COLS = []  # Dropped based on EDA\n",
    "TIME_COL = 'time'\n",
    "\n",
    "# 2. Hyperparameters\n",
    "WINDOW_SIZE = 40\n",
    "STRIDE = 10\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 200\n",
    "GRADIENT_CLIP_VALUE = 1.0\n",
    "K_FOLDS = 5\n",
    "LABEL_SMOOTHING = 0.1\n",
    "EARLY_STOPPING_PATIENCE = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f02e0d1",
   "metadata": {
    "papermill": {
     "duration": 0.002803,
     "end_time": "2025-11-17T10:21:37.834808",
     "exception": false,
     "start_time": "2025-11-17T10:21:37.832005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Data Loading & Initial Cleaning\n",
    "\n",
    "We load the raw data and perform initial cleaning, specifically encoding the text-based 'Team Name' column, which we will treat as an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7b89148",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T10:21:37.842070Z",
     "iopub.status.busy": "2025-11-17T10:21:37.841550Z",
     "iopub.status.idle": "2025-11-17T10:21:42.361014Z",
     "shell.execute_reply": "2025-11-17T10:21:42.360178Z"
    },
    "papermill": {
     "duration": 4.524902,
     "end_time": "2025-11-17T10:21:42.362501",
     "exception": false,
     "start_time": "2025-11-17T10:21:37.837599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2. Loading & Initial Cleaning ---\n",
      "Data loaded successfully.\n",
      "Found text column: n_legs\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 2. Loading & Initial Cleaning ---\")\n",
    "try:\n",
    "    df_features_raw = pd.read_csv(os.path.join(INPUT_DIR, 'pirate_pain_train.csv'))\n",
    "    df_labels_raw = pd.read_csv(os.path.join(INPUT_DIR, 'pirate_pain_train_labels.csv'))\n",
    "    df_test_raw = pd.read_csv(os.path.join(INPUT_DIR, 'pirate_pain_test.csv'))\n",
    "    sample_sub_df = pd.read_csv(os.path.join(INPUT_DIR, 'sample_submission.csv'))\n",
    "    print(\"Data loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Please check INPUT_DIR path.\")\n",
    "\n",
    "# --- Text Column (Team Name) Handling ---\n",
    "# We treat the Team Name as a categorical entity via Label Encoding\n",
    "exclude_cols = ['label', 'sample_index']\n",
    "string_cols = df_features_raw.select_dtypes(include=['object']).columns.tolist()\n",
    "string_cols = [c for c in string_cols if c not in exclude_cols]\n",
    "\n",
    "TEXT_COL = None\n",
    "TEXT_VOCAB_SIZE = 0\n",
    "\n",
    "if len(string_cols) > 0:\n",
    "    TEXT_COL = string_cols[0] \n",
    "    print(f\"Found text column: {TEXT_COL}\")\n",
    "    \n",
    "    def clean_team_name(text):\n",
    "        if pd.isna(text): return \"unknown\"\n",
    "        return re.sub(r'[^a-z0-9]', '', str(text).lower())\n",
    "\n",
    "    df_features_raw[TEXT_COL] = df_features_raw[TEXT_COL].apply(clean_team_name)\n",
    "    df_test_raw[TEXT_COL] = df_test_raw[TEXT_COL].apply(clean_team_name)\n",
    "    \n",
    "    le_text = LabelEncoder()\n",
    "    all_text = pd.concat([df_features_raw[TEXT_COL], df_test_raw[TEXT_COL]], axis=0)\n",
    "    le_text.fit(all_text)\n",
    "    \n",
    "    df_features_raw[TEXT_COL] = le_text.transform(df_features_raw[TEXT_COL])\n",
    "    df_test_raw[TEXT_COL] = le_text.transform(df_test_raw[TEXT_COL])\n",
    "    \n",
    "    TEXT_VOCAB_SIZE = len(le_text.classes_)\n",
    "else:\n",
    "    print(\"No text column (Team Name) found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874ce7a6",
   "metadata": {
    "papermill": {
     "duration": 0.003047,
     "end_time": "2025-11-17T10:21:42.369189",
     "exception": false,
     "start_time": "2025-11-17T10:21:42.366142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "We transform the raw data into model-ready features based on our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "967a49d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T10:21:42.376048Z",
     "iopub.status.busy": "2025-11-17T10:21:42.375817Z",
     "iopub.status.idle": "2025-11-17T10:21:42.854063Z",
     "shell.execute_reply": "2025-11-17T10:21:42.853086Z"
    },
    "papermill": {
     "duration": 0.483346,
     "end_time": "2025-11-17T10:21:42.855469",
     "exception": false,
     "start_time": "2025-11-17T10:21:42.372123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3. Applying Feature Engineering ---\n",
      "Total continuous features: 62\n",
      "Feature engineering complete.\n"
     ]
    }
   ],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Transforms raw data into model-ready features.\n",
    "    \"\"\"\n",
    "    df_eng = df.copy()\n",
    "    grouped = df_eng.groupby('sample_index')\n",
    "    \n",
    "    # 1. DELTA FEATURES (Velocity)\n",
    "    # Raw joint data is non-stationary. We compute the first difference (velocity)\n",
    "    # to help the model learn motion patterns instead of absolute positions.\n",
    "    for col in JOINT_COLS:\n",
    "        df_eng[f'd_{col}'] = grouped[col].diff().fillna(0)\n",
    "    \n",
    "    # 2. CYCLIC TIME ENCODING\n",
    "    # 'time' is encoded cyclically to help the model understand\n",
    "    # the start vs. end of a sequence without linear bias.\n",
    "    max_time_val = 160 # Hardcoded to 160 (max steps)\n",
    "    df_eng['sin_time'] = np.sin(2 * np.pi * df_eng[TIME_COL] / max_time_val)\n",
    "    df_eng['cos_time'] = np.cos(2 * np.pi * df_eng[TIME_COL] / max_time_val)\n",
    "\n",
    "    # 3. DROP CONSTANT FEATURE\n",
    "    # 'joint_30' was found to be a constant value (0.5) and is noise.\n",
    "    if 'joint_30' in df_eng.columns:\n",
    "        df_eng = df_eng.drop(columns=['joint_30'])\n",
    "        \n",
    "    return df_eng\n",
    "\n",
    "print(\"--- 3. Applying Feature Engineering ---\")\n",
    "df_features_engineered = engineer_features(df_features_raw)\n",
    "df_test_engineered = engineer_features(df_test_raw)\n",
    "\n",
    "# --- Define final feature sets ---\n",
    "DELTA_JOINT_COLS = [f'd_{col}' for col in JOINT_COLS]\n",
    "CONTINUOUS_COLS = JOINT_COLS + DELTA_JOINT_COLS + ['sin_time', 'cos_time']\n",
    "print(f\"Total continuous features: {len(CONTINUOUS_COLS)}\")\n",
    "\n",
    "# --- Prepare Categorical Vocabularies for Embeddings ---\n",
    "# Pain surveys are categorical (0,1,2), not continuous scalars.\n",
    "survey_vocab_sizes = [int(df_features_engineered[c].max() + 1) for c in SURVEY_COLS]\n",
    "time_vocab_size = int(df_features_engineered[TIME_COL].max() + 1)\n",
    "\n",
    "# --- Map Targets ---\n",
    "label_mapping = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "df_labels_raw['label_encoded'] = df_labels_raw['label'].map(label_mapping)\n",
    "\n",
    "print(\"Feature engineering complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40caa466",
   "metadata": {
    "papermill": {
     "duration": 0.003719,
     "end_time": "2025-11-17T10:21:42.863605",
     "exception": false,
     "start_time": "2025-11-17T10:21:42.859886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Dataset & Sampling Strategy\n",
    "\n",
    "We define the custom `Dataset` class to handle windowing and a `WeightedRandomSampler` to address class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d09cb6ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T10:21:42.871456Z",
     "iopub.status.busy": "2025-11-17T10:21:42.871228Z",
     "iopub.status.idle": "2025-11-17T10:21:42.879379Z",
     "shell.execute_reply": "2025-11-17T10:21:42.878799Z"
    },
    "papermill": {
     "duration": 0.013432,
     "end_time": "2025-11-17T10:21:42.880519",
     "exception": false,
     "start_time": "2025-11-17T10:21:42.867087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PiratePainDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset to handle windowing of time-series data.\n",
    "    - Applies windowing (size 40, stride 10).\n",
    "    - Separates Continuous inputs (for Scaler) from Categorical inputs (for Embeddings).\n",
    "    - Applies Gaussian noise augmentation during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, features_df, labels_df, sample_indices, window_size, stride, text_col=None, augment=False):\n",
    "        self.features_df = features_df\n",
    "        self.labels_df = labels_df.set_index('sample_index') if labels_df is not None else None\n",
    "        self.sample_indices = sample_indices\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.text_col = text_col\n",
    "        self.augment = augment \n",
    "        \n",
    "        # Grouping for O(1) access\n",
    "        self.grouped_features = dict(tuple(features_df.groupby('sample_index')))\n",
    "        self.indices = self._create_indices()\n",
    "\n",
    "    def _create_indices(self):\n",
    "        # Creates a list of valid (sample_idx, start, end) tuples\n",
    "        indices = []\n",
    "        for sample_idx in self.sample_indices:\n",
    "            if sample_idx not in self.grouped_features: continue\n",
    "            data = self.grouped_features[sample_idx]\n",
    "            n_timesteps = len(data)\n",
    "            # Create windows\n",
    "            for start in range(0, n_timesteps - self.window_size + 1, self.stride):\n",
    "                indices.append((sample_idx, start, start + self.window_size))\n",
    "        return indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_idx, start, end = self.indices[idx]\n",
    "        window_data = self.grouped_features[sample_idx].iloc[start:end]\n",
    "\n",
    "        # 1. Continuous Data + Gaussian Noise Jittering (Augmentation)\n",
    "        vals = window_data[CONTINUOUS_COLS].values\n",
    "        if self.augment:\n",
    "            noise = np.random.normal(0, 0.02, vals.shape) \n",
    "            vals = vals + noise\n",
    "        x_cont = torch.tensor(vals, dtype=torch.float)\n",
    "        \n",
    "        # 2. Categorical Data (Surveys + Time)\n",
    "        # We add +1 to reserve 0 for padding/unknown\n",
    "        x_survey = torch.tensor((window_data[SURVEY_COLS].values + 1), dtype=torch.long)\n",
    "        x_time = torch.tensor((window_data[TIME_COL].values + 1), dtype=torch.long)\n",
    "        \n",
    "        # 3. Text Data\n",
    "        x_text = torch.tensor(0, dtype=torch.long)\n",
    "        if self.text_col:\n",
    "            # Text is static, so we take the first value\n",
    "            val = window_data[self.text_col].iloc[0]\n",
    "            # --- MODIFICA: Bug corretto. Ripristinata la logica originale ---\n",
    "            x_text = torch.tensor(val, dtype=torch.long) \n",
    "            # --- Fine Modifica ---\n",
    "\n",
    "        # 4. Target\n",
    "        label = torch.tensor(-1, dtype=torch.long) # For test set\n",
    "        if self.labels_df is not None:\n",
    "            label = torch.tensor(self.labels_df.loc[sample_idx, 'label_encoded'], dtype=torch.long)\n",
    "\n",
    "        return x_cont, x_survey, x_time, x_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a75022ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T10:21:42.888020Z",
     "iopub.status.busy": "2025-11-17T10:21:42.887524Z",
     "iopub.status.idle": "2025-11-17T10:21:42.892230Z",
     "shell.execute_reply": "2025-11-17T10:21:42.891539Z"
    },
    "papermill": {
     "duration": 0.009576,
     "end_time": "2025-11-17T10:21:42.893364",
     "exception": false,
     "start_time": "2025-11-17T10:21:42.883788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_weighted_sampler(dataset, labels_df):\n",
    "    \"\"\"\n",
    "    Creates a WeightedRandomSampler to handle class imbalance.\n",
    "    Rare classes are sampled more frequently to balance the batches.\n",
    "    \"\"\"\n",
    "    sample_to_label = labels_df.set_index('sample_index')['label_encoded'].to_dict()\n",
    "    label_counts = labels_df['label_encoded'].value_counts().sort_index()\n",
    "    \n",
    "    # Calculate weights: 1 / (count)\n",
    "    class_weights = 1.0 / label_counts\n",
    "    \n",
    "    weights = []\n",
    "    for idx_tuple in dataset.indices:\n",
    "        s_idx = idx_tuple[0] # Get sample_index from the window tuple\n",
    "        if s_idx in sample_to_label:\n",
    "            l = sample_to_label[s_idx]\n",
    "            weights.append(class_weights[l])\n",
    "        else:\n",
    "            weights.append(0) # Should not happen in training\n",
    "            \n",
    "    return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3408225d",
   "metadata": {
    "papermill": {
     "duration": 0.003139,
     "end_time": "2025-11-17T10:21:42.899566",
     "exception": false,
     "start_time": "2025-11-17T10:21:42.896427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Loss Function\n",
    "\n",
    "We use **Focal Loss** to combat class imbalance. It adds a (1-p_t)^gamma term to the standard Cross-Entropy, forcing the model to focus on \"hard\" or \"confidently wrong\" examples (minority classes) rather than easily classifying the `no_pain` majority. We also use **Label Smoothing** to prevent overconfidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "030d43f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T10:21:42.906661Z",
     "iopub.status.busy": "2025-11-17T10:21:42.906470Z",
     "iopub.status.idle": "2025-11-17T10:21:42.911037Z",
     "shell.execute_reply": "2025-11-17T10:21:42.910552Z"
    },
    "papermill": {
     "duration": 0.00925,
     "end_time": "2025-11-17T10:21:42.911946",
     "exception": false,
     "start_time": "2025-11-17T10:21:42.902696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss with Label Smoothing.\n",
    "    - Focal term (gamma): Focuses learning on hard-to-classify examples.\n",
    "    - Label Smoothing: Prevents over-confidence (e.g., 1.0 probability).\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean', label_smoothing=0.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(\n",
    "            inputs, targets, reduction='none', weight=self.alpha, \n",
    "            label_smoothing=self.label_smoothing\n",
    "        )\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        \n",
    "        return focal_loss.mean() if self.reduction == 'mean' else focal_loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e472fd",
   "metadata": {
    "papermill": {
     "duration": 0.003065,
     "end_time": "2025-11-17T10:21:42.918120",
     "exception": false,
     "start_time": "2025-11-17T10:21:42.915055",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Model Architecture (Hybrid CNN-LSTM)\n",
    "\n",
    "This is the final model that achieved the `0.9425` score.\n",
    "\n",
    "* **Rationale:** A hybrid approach to capture both local patterns and long-term dependencies.\n",
    "* **Embeddings:** All categorical inputs (`surveys`, `time`, `team_name`) are embedded.\n",
    "* **CNN Block:** A `Conv1d` layer (with `BatchNorm1d` for normalization) acts as a learnable feature extractor. It scans the sequence for salient *local patterns* (e.g., spikes, tremors).\n",
    "* **LSTM Block:** A 2-layer `LSTM` processes the *sequence of features* extracted by the CNN, allowing it to model long-term temporal dependencies between these patterns.\n",
    "* **Classifier:** A final `Linear` layer classifies the last hidden state of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f96962f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T10:21:42.925555Z",
     "iopub.status.busy": "2025-11-17T10:21:42.925237Z",
     "iopub.status.idle": "2025-11-17T10:21:42.933256Z",
     "shell.execute_reply": "2025-11-17T10:21:42.932766Z"
    },
    "papermill": {
     "duration": 0.012956,
     "end_time": "2025-11-17T10:21:42.934268",
     "exception": false,
     "start_time": "2025-11-17T10:21:42.921312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PiratePainModel(nn.Module):\n",
    "    def __init__(self, n_continuous, survey_vocab_sizes, time_vocab_size, text_vocab_size, lstm_hidden=128, n_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. EMBEDDING LAYERS (For Categorical Inputs)\n",
    "        # We add +2 to vocab size: +1 for the 0-padding, +1 for potential OOV\n",
    "        self.emb_surveys = nn.ModuleList([nn.Embedding(v+2, 4) for v in survey_vocab_sizes])\n",
    "        self.emb_time = nn.Embedding(time_vocab_size+2, 8)\n",
    "        \n",
    "        self.use_text = (text_vocab_size > 0)\n",
    "        text_dim = 8 if self.use_text else 0\n",
    "        if self.use_text:\n",
    "            self.emb_text = nn.Embedding(text_vocab_size+2, 8)\n",
    "            \n",
    "        # Calculate total input dimension for the CNN\n",
    "        total_survey_dim = len(survey_vocab_sizes) * 4\n",
    "        input_dim = n_continuous + total_survey_dim + 8 + text_dim\n",
    "        \n",
    "        # 2. CNN BLOCK (Local Feature Extraction)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # 3. LSTM BLOCK (Long-Term Memory)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=128, # Input is the output channels from CNN\n",
    "            hidden_size=lstm_hidden, \n",
    "            num_layers=2, \n",
    "            batch_first=True, \n",
    "            dropout=0.3 # Dropout between LSTM layers\n",
    "        )\n",
    "        \n",
    "        # 4. CLASSIFIER\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(lstm_hidden, n_classes)\n",
    "\n",
    "    def forward(self, x_cont, x_survey, x_time, x_text):\n",
    "        batch_size, seq_len, _ = x_cont.shape\n",
    "        \n",
    "        # Process Embeddings\n",
    "        e_surv = [emb(x_survey[:,:,i]) for i, emb in enumerate(self.emb_surveys)]\n",
    "        e_time = self.emb_time(x_time)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        features = [x_cont] + e_surv + [e_time]\n",
    "        if self.use_text:\n",
    "            # Repeat static text embedding across all time steps\n",
    "            e_txt = self.emb_text(x_text).unsqueeze(1).repeat(1, seq_len, 1)\n",
    "            features.append(e_txt)\n",
    "            \n",
    "        full_input = torch.cat(features, dim=2) # Shape: (B, Seq, Feat)\n",
    "        \n",
    "        # CNN Pass\n",
    "        # Conv1d expects (B, Channels, Seq)\n",
    "        x = full_input.permute(0, 2, 1)\n",
    "        x = self.cnn(x)\n",
    "        \n",
    "        # LSTM Pass\n",
    "        # LSTM expects (B, Seq, Channels)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        out, _ = self.lstm(x)\n",
    "        \n",
    "        # Final Classification\n",
    "        # We use the output of the last time step\n",
    "        last_hidden_state = out[:, -1, :]\n",
    "        logits = self.classifier(self.dropout(last_hidden_state))\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ab0ee2",
   "metadata": {
    "papermill": {
     "duration": 0.003167,
     "end_time": "2025-11-17T10:21:42.940615",
     "exception": false,
     "start_time": "2025-11-17T10:21:42.937448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Training & Validation Strategy\n",
    "\n",
    "We employ a robust `StratifiedKFold` (5 splits) cross-validation.\n",
    "\n",
    "* **Per-Fold Scaling:** The `StandardScaler` is fit *only* on the training data for each fold to prevent data leakage.\n",
    "* **Ensemble:** The 5 models (one from each fold) are saved and used as an ensemble during inference.\n",
    "* **OOF Predictions:** We store the validation predictions (Out-of-Fold) from the *best epoch* of each fold.\n",
    "* **Window Aggregation:** Since we predict on windows, we aggregate window-level logits for a single sample by taking their `mean` (Soft Voting).\n",
    "* **Thresholding:** The OOF predictions are used to find the optimal probability thresholds to maximize the F1-score, as `argmax` is suboptimal for imbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11e2e991",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T10:21:42.949552Z",
     "iopub.status.busy": "2025-11-17T10:21:42.949140Z",
     "iopub.status.idle": "2025-11-17T10:21:42.955170Z",
     "shell.execute_reply": "2025-11-17T10:21:42.954431Z"
    },
    "papermill": {
     "duration": 0.012162,
     "end_time": "2025-11-17T10:21:42.956372",
     "exception": false,
     "start_time": "2025-11-17T10:21:42.944210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xc, xs, xt, xtxt, y in loader:\n",
    "        xc, xs, xt, xtxt, y = xc.to(device), xs.to(device), xt.to(device), xtxt.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xc, xs, xt, xtxt)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Clipping (Prevents exploding gradients)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_VALUE)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7e5dcf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T10:21:42.963962Z",
     "iopub.status.busy": "2025-11-17T10:21:42.963763Z",
     "iopub.status.idle": "2025-11-17T11:29:33.699827Z",
     "shell.execute_reply": "2025-11-17T11:29:33.698964Z"
    },
    "papermill": {
     "duration": 4070.745458,
     "end_time": "2025-11-17T11:29:33.705063",
     "exception": false,
     "start_time": "2025-11-17T10:21:42.959605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 7. Starting Stratified K-Fold Training (Ensemble) ---\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Epoch 10/200, Train Loss: 0.0394, Val F1: 0.9424, Patience: 0/20\n",
      "Epoch 20/200, Train Loss: 0.0315, Val F1: 0.9544, Patience: 0/20\n",
      "Epoch 30/200, Train Loss: 0.0284, Val F1: 0.9467, Patience: 4/20\n",
      "Epoch 40/200, Train Loss: 0.0275, Val F1: 0.9513, Patience: 14/20\n",
      "--- Early stopping triggered at epoch 45 ---\n",
      "Fold 1 Best Val F1: 0.9700 at epoch 25\n",
      "Loading best model from: /kaggle/working/model_fold_1_best.pt\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Epoch 10/200, Train Loss: 0.0409, Val F1: 0.9313, Patience: 0/20\n",
      "Epoch 20/200, Train Loss: 0.0332, Val F1: 0.9398, Patience: 1/20\n",
      "Epoch 30/200, Train Loss: 0.0246, Val F1: 0.9255, Patience: 8/20\n",
      "Epoch 40/200, Train Loss: 0.0242, Val F1: 0.9398, Patience: 18/20\n",
      "--- Early stopping triggered at epoch 41 ---\n",
      "Fold 2 Best Val F1: 0.9614 at epoch 21\n",
      "Loading best model from: /kaggle/working/model_fold_2_best.pt\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Epoch 10/200, Train Loss: 0.0391, Val F1: 0.9179, Patience: 0/20\n",
      "Epoch 20/200, Train Loss: 0.0295, Val F1: 0.9177, Patience: 5/20\n",
      "Epoch 30/200, Train Loss: 0.0260, Val F1: 0.9247, Patience: 3/20\n",
      "Epoch 40/200, Train Loss: 0.0256, Val F1: 0.9271, Patience: 13/20\n",
      "--- Early stopping triggered at epoch 46 ---\n",
      "Fold 3 Best Val F1: 0.9411 at epoch 26\n",
      "Loading best model from: /kaggle/working/model_fold_3_best.pt\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Epoch 10/200, Train Loss: 0.0409, Val F1: 0.9080, Patience: 3/20\n",
      "Epoch 20/200, Train Loss: 0.0292, Val F1: 0.9313, Patience: 5/20\n",
      "Epoch 30/200, Train Loss: 0.0222, Val F1: 0.9328, Patience: 15/20\n",
      "--- Early stopping triggered at epoch 34 ---\n",
      "Fold 4 Best Val F1: 0.9550 at epoch 14\n",
      "Loading best model from: /kaggle/working/model_fold_4_best.pt\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Epoch 10/200, Train Loss: 0.0405, Val F1: 0.9128, Patience: 1/20\n",
      "Epoch 20/200, Train Loss: 0.0293, Val F1: 0.9109, Patience: 2/20\n",
      "Epoch 30/200, Train Loss: 0.0228, Val F1: 0.9228, Patience: 7/20\n",
      "Epoch 40/200, Train Loss: 0.0248, Val F1: 0.9163, Patience: 17/20\n",
      "--- Early stopping triggered at epoch 42 ---\n",
      "Fold 5 Best Val F1: 0.9470 at epoch 22\n",
      "Loading best model from: /kaggle/working/model_fold_5_best.pt\n"
     ]
    }
   ],
   "source": [
    "# --- MAIN EXECUTION: K-FOLD ENSEMBLE ---\n",
    "print(\"\\n--- 7. Starting Stratified K-Fold Training (Ensemble) ---\")\n",
    "\n",
    "all_sample_indices = df_labels_raw['sample_index'].unique()\n",
    "all_labels_strat = df_labels_raw.set_index('sample_index').loc[all_sample_indices]['label_encoded'].values\n",
    "\n",
    "# Storage for Out-Of-Fold (OOF) predictions and models\n",
    "oof_probs = np.zeros((len(all_sample_indices), 3))\n",
    "oof_targets = np.zeros(len(all_sample_indices))\n",
    "sample_to_idx = {s: i for i, s in enumerate(all_sample_indices)}\n",
    "models_list = [] \n",
    "\n",
    "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(all_sample_indices, all_labels_strat)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{K_FOLDS} ---\")\n",
    "    \n",
    "    train_samples = all_sample_indices[train_idx]\n",
    "    val_samples = all_sample_indices[val_idx]\n",
    "    \n",
    "    # 1. Standard Scaling (fitted ONLY on this fold's training data)\n",
    "    scaler = StandardScaler()\n",
    "    train_subset = df_features_engineered[df_features_engineered['sample_index'].isin(train_samples)]\n",
    "    scaler.fit(train_subset[CONTINUOUS_COLS])\n",
    "    \n",
    "    # Apply scaler to a copy of the full dataset for this fold\n",
    "    df_fold = df_features_engineered.copy()\n",
    "    df_fold[CONTINUOUS_COLS] = scaler.transform(df_fold[CONTINUOUS_COLS])\n",
    "    \n",
    "    # 2. Datasets & Loaders\n",
    "    train_ds = PiratePainDataset(df_fold, df_labels_raw, train_samples, WINDOW_SIZE, STRIDE, TEXT_COL, augment=True)\n",
    "    val_ds = PiratePainDataset(df_fold, df_labels_raw, val_samples, WINDOW_SIZE, STRIDE, TEXT_COL, augment=False)\n",
    "    \n",
    "    # Weighted Sampler for Training\n",
    "    sampler = get_weighted_sampler(train_ds, df_labels_raw)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, shuffle=False, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # 3. Model Initialization\n",
    "    model = PiratePainModel(\n",
    "        n_continuous=len(CONTINUOUS_COLS), \n",
    "        survey_vocab_sizes=survey_vocab_sizes, \n",
    "        time_vocab_size=time_vocab_size,\n",
    "        text_vocab_size=TEXT_VOCAB_SIZE\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    criterion = FocalLoss(alpha=None, gamma=2.0, label_smoothing=LABEL_SMOOTHING)\n",
    "\n",
    "    # Early-stopping\n",
    "    best_v_f1 = 0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    # Definiamo un path per salvare il modello migliore di questo fold\n",
    "    model_path = os.path.join(WORKING_DIR, f\"model_fold_{fold+1}_best.pt\")\n",
    "    \n",
    "    # 4. Epoch Loop\n",
    "    for ep in range(EPOCHS):\n",
    "        t_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "        \n",
    "        # Validation & OOF Prediction\n",
    "        model.eval()\n",
    "        val_logits_list = []\n",
    "        window_sample_map_val = [x[0] for x in val_ds.indices] # Map window back to sample\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for xc, xs, xt, xtxt, y in val_loader:\n",
    "                xc, xs, xt, xtxt = xc.to(device), xs.to(device), xt.to(device), xtxt.to(device)\n",
    "                logits = model(xc, xs, xt, xtxt)\n",
    "                val_logits_list.extend(logits.cpu().numpy())\n",
    "        \n",
    "        # 5. Window Aggregation (Soft Voting)\n",
    "        df_val_logits = pd.DataFrame(val_logits_list, columns=[0, 1, 2])\n",
    "        df_val_logits['sample_index'] = window_sample_map_val\n",
    "        df_val_agg_logits = df_val_logits.groupby('sample_index').mean()\n",
    "        \n",
    "        # Calculate F1 on aggregated predictions\n",
    "        current_val_probs = torch.softmax(torch.tensor(df_val_agg_logits.values), dim=1).numpy()\n",
    "        current_val_preds = np.argmax(current_val_probs, axis=1)\n",
    "        \n",
    "        current_val_indices = df_val_agg_logits.index\n",
    "        current_val_labels = df_labels_raw.set_index('sample_index').loc[current_val_indices]['label_encoded'].values\n",
    "        \n",
    "        v_f1 = f1_score(current_val_labels, current_val_preds, average='weighted')\n",
    "        \n",
    "        if (ep + 1) % 10 == 0:\n",
    "            print(f\"Epoch {ep+1}/{EPOCHS}, Train Loss: {t_loss:.4f}, Val F1: {v_f1:.4f}, Patience: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
    "            \n",
    "        # 6. Model Checkpoint & Early Stopping (come da Lab)\n",
    "        if v_f1 > best_v_f1:\n",
    "            best_v_f1 = v_f1\n",
    "            best_epoch = ep + 1\n",
    "            patience_counter = 0 # Resetta la pazienza\n",
    "            # Salva il modello migliore su disco\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "            # Store OOF Probabilities for Threshold Optimization\n",
    "            for idx, s_idx in enumerate(current_val_indices):\n",
    "                global_idx = sample_to_idx[s_idx]\n",
    "                oof_probs[global_idx] = current_val_probs[idx]\n",
    "                oof_targets[global_idx] = current_val_labels[idx]\n",
    "        else:\n",
    "            patience_counter += 1 # Incrementa la pazienza\n",
    "            \n",
    "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"--- Early stopping triggered at epoch {ep + 1} ---\")\n",
    "            break\n",
    "\n",
    "    print(f\"Fold {fold+1} Best Val F1: {best_v_f1:.4f} at epoch {best_epoch}\")\n",
    "    \n",
    "    print(f\"Loading best model from: {model_path}\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    models_list.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6520825e",
   "metadata": {
    "papermill": {
     "duration": 0.004055,
     "end_time": "2025-11-17T11:29:33.713396",
     "exception": false,
     "start_time": "2025-11-17T11:29:33.709341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Threshold Optimization\n",
    "\n",
    "We use the OOF (Out-of-Fold) probabilities collected during training to find the optimal decision thresholds. This is crucial for imbalanced classification, as the default 0.5 (or `argmax`) is not optimal for maximizing F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8c9d55d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T11:29:33.722873Z",
     "iopub.status.busy": "2025-11-17T11:29:33.722538Z",
     "iopub.status.idle": "2025-11-17T11:29:34.593244Z",
     "shell.execute_reply": "2025-11-17T11:29:34.592388Z"
    },
    "papermill": {
     "duration": 0.876902,
     "end_time": "2025-11-17T11:29:34.594520",
     "exception": false,
     "start_time": "2025-11-17T11:29:33.717618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 8. Optimizing Decision Thresholds on OOF Data ---\n",
      "Optimal Thresholds Found: Low > 0.43, High > 0.49\n",
      "Best OOF F1 Score: 0.9574\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 8. Optimizing Decision Thresholds on OOF Data ---\")\n",
    "\n",
    "best_thresh = (0.0, 0.0)\n",
    "best_score = 0.0\n",
    "\n",
    "# Search space for thresholds\n",
    "for t_high in np.arange(0.15, 0.50, 0.01):\n",
    "    for t_low in np.arange(0.20, 0.55, 0.01):\n",
    "        if t_low >= t_high: continue # Ensure low < high\n",
    "        \n",
    "        preds = []\n",
    "        for p in oof_probs:\n",
    "            # Apply thresholds\n",
    "            if p[2] > t_high: preds.append(2)     # high_pain\n",
    "            elif p[1] > t_low: preds.append(1)    # low_pain\n",
    "            else: preds.append(0)                 # no_pain\n",
    "            \n",
    "        s = f1_score(oof_targets, preds, average='weighted')\n",
    "        if s > best_score:\n",
    "            best_score = s\n",
    "            best_thresh = (t_low, t_high)\n",
    "\n",
    "print(f\"Optimal Thresholds Found: Low > {best_thresh[0]:.2f}, High > {best_thresh[1]:.2f}\")\n",
    "print(f\"Best OOF F1 Score: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e18f5d",
   "metadata": {
    "papermill": {
     "duration": 0.004257,
     "end_time": "2025-11-17T11:29:34.603432",
     "exception": false,
     "start_time": "2025-11-17T11:29:34.599175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9. Final Inference & Submission\n",
    "\n",
    "We now generate the final `submission.csv` file.\n",
    "\n",
    "1.  A final `StandardScaler` is fit on **all** training data.\n",
    "2.  The test data is processed through the *full* pipeline (FE, Scaling, Windowing).\n",
    "3.  We perform inference with **all 5 models** in our ensemble.\n",
    "4.  The logits from the 5 models are **averaged** (Ensemble Soft Voting).\n",
    "5.  The final averaged probabilities are converted to predictions using our **optimized thresholds**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cb0d08e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T11:29:34.613079Z",
     "iopub.status.busy": "2025-11-17T11:29:34.612870Z",
     "iopub.status.idle": "2025-11-17T11:32:34.343120Z",
     "shell.execute_reply": "2025-11-17T11:32:34.342215Z"
    },
    "papermill": {
     "duration": 179.740668,
     "end_time": "2025-11-17T11:32:34.348313",
     "exception": false,
     "start_time": "2025-11-17T11:29:34.607645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 9. Generating Submission File (Ensemble) ---\n",
      "Running inference with Model 1/5...\n",
      "Running inference with Model 2/5...\n",
      "Running inference with Model 3/5...\n",
      "Running inference with Model 4/5...\n",
      "Running inference with Model 5/5...\n",
      "Submission file created successfully at: /kaggle/working/submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_index    label\n",
       "0             0  no_pain\n",
       "1             1  no_pain\n",
       "2             2  no_pain\n",
       "3             3  no_pain\n",
       "4             4  no_pain"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n--- 9. Generating Submission File (Ensemble) ---\")\n",
    "\n",
    "# 1. Prepare Test Data\n",
    "# Fit the final scaler on ALL training data\n",
    "final_scaler = StandardScaler()\n",
    "final_scaler.fit(df_features_engineered[CONTINUOUS_COLS])\n",
    "\n",
    "df_test_scaled = df_test_engineered.copy()\n",
    "df_test_scaled[CONTINUOUS_COLS] = final_scaler.transform(df_test_scaled[CONTINUOUS_COLS])\n",
    "\n",
    "# Get sample indices from the official sample_submission.csv\n",
    "sub_indices = sample_sub_df['sample_index'].unique()\n",
    "\n",
    "test_ds_final = PiratePainDataset(df_test_scaled, None, sub_indices, WINDOW_SIZE, STRIDE, TEXT_COL, augment=False)\n",
    "test_loader_final = DataLoader(test_ds_final, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "# Map windows back to sample_index\n",
    "window_sample_map_test = [x[0] for x in test_ds_final.indices]\n",
    "\n",
    "# 2. Ensemble Inference\n",
    "ensemble_logits_sum = None\n",
    "\n",
    "for i, model in enumerate(models_list):\n",
    "    model.eval()\n",
    "    fold_logits = []\n",
    "    print(f\"Running inference with Model {i+1}/{K_FOLDS}...\")\n",
    "    with torch.no_grad():\n",
    "        for xc, xs, xt, xtxt, _ in test_loader_final:\n",
    "            xc, xs, xt, xtxt = xc.to(device), xs.to(device), xt.to(device), xtxt.to(device)\n",
    "            logits = model(xc, xs, xt, xtxt)\n",
    "            fold_logits.extend(logits.cpu().numpy())\n",
    "    \n",
    "    # Aggregate windows for this fold\n",
    "    df_tmp = pd.DataFrame(fold_logits, columns=[0, 1, 2])\n",
    "    df_tmp['sample_index'] = window_sample_map_test\n",
    "    df_avg = df_tmp.groupby('sample_index').mean()\n",
    "    \n",
    "    # Add to the ensemble sum\n",
    "    if ensemble_logits_sum is None:\n",
    "        ensemble_logits_sum = df_avg\n",
    "    else:\n",
    "        ensemble_logits_sum = ensemble_logits_sum.add(df_avg, fill_value=0)\n",
    "\n",
    "# 3. Average Ensemble & Apply Thresholds\n",
    "ensemble_logits_avg = ensemble_logits_sum / K_FOLDS\n",
    "final_probs = torch.softmax(torch.tensor(ensemble_logits_avg.values), dim=1).numpy()\n",
    "\n",
    "final_preds_list = []\n",
    "thr_l, thr_h = best_thresh\n",
    "\n",
    "for p in final_probs:\n",
    "    if p[2] > thr_h: final_preds_list.append(2)    # high_pain\n",
    "    elif p[1] > thr_l: final_preds_list.append(1)  # low_pain\n",
    "    else: final_preds_list.append(0)               # no_pain\n",
    "\n",
    "final_series = pd.Series(final_preds_list, index=ensemble_logits_avg.index)\n",
    "\n",
    "# 4. Format & Save Submission\n",
    "inv_map = {v: k for k, v in label_mapping.items()}\n",
    "submission = final_series.map(inv_map).reset_index()\n",
    "submission.columns = ['sample_index', 'label']\n",
    "\n",
    "# Re-order to match sample_submission.csv\n",
    "submission_final = submission.set_index('sample_index').reindex(sample_sub_df['sample_index']).reset_index()\n",
    "\n",
    "submission_path = os.path.join(WORKING_DIR, 'submission.csv')\n",
    "submission_final.to_csv(submission_path, index=False)\n",
    "print(f\"Submission file created successfully at: {submission_path}\")\n",
    "display(submission_final.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8758305,
     "sourceId": 13762689,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4268.260663,
   "end_time": "2025-11-17T11:32:36.173640",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-17T10:21:27.912977",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31831fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "import re\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "# Ignore not critical warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Hardware configuration ---\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Used device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba087b8",
   "metadata": {},
   "source": [
    "## PART 1: DATA ANALYSIS, see ML_data_analysis file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6792f5e8",
   "metadata": {},
   "source": [
    "#### INSIGHT 1: CLASS IMBALANCE\n",
    "Analysis of the target distribution reveals a severe imbalance:\n",
    "No Pain (~77%) >> Low Pain (~14%) >> High Pain (~8%).<br>\n",
    "IMPLICATION: We cannot use standard CrossEntropy. We must implement:\n",
    "1. WeightedRandomSampler (to balance batches).\n",
    "2. Focal Loss (to penalize hard/rare examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd40c5a",
   "metadata": {},
   "source": [
    "#### INSIGHT 2: STATIC FEATURES RELIABILITY\n",
    "EDA showed that static features like 'n_legs' (peg_leg) have near-perfect correlation \n",
    "with 'no_pain' in the training set, but are virtually non-existent (<1%) in the test set.<br>\n",
    "IMPLICATION: These are spurious correlations (\"Cheat Codes\") that lead to overfitting.<br>\n",
    "DECISION: We explicitly DROP 'n_legs', 'n_hands', 'n_eyes' from the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c17929c",
   "metadata": {},
   "source": [
    "#### INSIGHT 3: TEMPORAL DYNAMICS & WINDOWING\n",
    "Autocorrelation plots (ACF) on 'joint' features showed a slow decay of signal memory \n",
    "lasting approximately 30-40 time steps.<br>\n",
    "IMPLICATION: The model needs a context window large enough to capture this memory.<br>\n",
    "DECISION: Window Size set to 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddf7cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global parameters ---\n",
    "# 1. Columns definition\n",
    "JOINT_COLS = [f'joint_{i:02d}' for i in range(30)]\n",
    "SURVEY_COLS = ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
    "STATIC_COLS = []  # empty for now, see Insight 2\n",
    "TIME_COL = 'time'\n",
    "\n",
    "# 2. Hiperparameters for NN architecture and training\n",
    "WINDOW_SIZE = 40\n",
    "STRIDE = 10\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 50\n",
    "GRADIENT_CLIP_VALUE = 1.0\n",
    "K_FOLDS = 5\n",
    "LABEL_SMOOTHING = 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a82a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Data reading ---\n",
    "try:\n",
    "    df_features_raw = pd.read_csv('/kaggle/input/pirate/pirate_pain_train.csv')\n",
    "    df_labels_raw = pd.read_csv('/kaggle/input/pirate/pirate_pain_train_labels.csv')\n",
    "    df_test_raw = pd.read_csv('/kaggle/input/pirate/pirate_pain_test.csv')\n",
    "except:\n",
    "    # Fallback per path locale\n",
    "    df_features_raw = pd.read_csv('pirate_pain_train.csv')\n",
    "    df_labels_raw = pd.read_csv('pirate_pain_train_labels.csv')\n",
    "    df_test_raw = pd.read_csv('pirate_pain_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be236b7e",
   "metadata": {},
   "source": [
    "## PART 2: FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918eaa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Transforms raw data into model-ready features based on stationarity analysis.\n",
    "    \"\"\"\n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    # 1. DELTA FEATURES (Velocity)\n",
    "    # Raw position data is often non-stationary. We compute the first difference (velocity).\n",
    "    # This helps the CNN extract motion patterns rather than absolute positions.\n",
    "    grouped = df_eng.groupby('sample_index')\n",
    "    for col in JOINT_COLS:\n",
    "        df_eng[f'd_{col}'] = grouped[col].diff().fillna(0)\n",
    "        \n",
    "    # 2. CYCLIC TIME ENCODING\n",
    "    # The 'time' column represents a sequence. To help the model understand phases \n",
    "    # (start vs end of movement) without linear bias, we encode it cyclically.\n",
    "    max_time_val = df_eng[TIME_COL].max() + 1 \n",
    "    df_eng['sin_time'] = np.sin(2 * np.pi * df_eng[TIME_COL] / max_time_val)\n",
    "    df_eng['cos_time'] = np.cos(2 * np.pi * df_eng[TIME_COL] / max_time_val)\n",
    "\n",
    "    # 3. DROP NOISE\n",
    "    # 'joint_30' was found to be a constant value (0.5) across the dataset.\n",
    "    if 'joint_30' in df_eng.columns:\n",
    "        df_eng = df_eng.drop(columns=['joint_30'])\n",
    "        \n",
    "    return df_eng\n",
    "\n",
    "print(\"Applying Feature Engineering...\")\n",
    "df_features_engineered = engineer_features(df_features_raw)\n",
    "df_test_engineered = engineer_features(df_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad0f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define final feature sets\n",
    "DELTA_JOINT_COLS = [f'd_{col}' for col in JOINT_COLS]\n",
    "# We combine raw joints, delta joints, and cyclic time into the continuous input vector\n",
    "CONTINUOUS_COLS = JOINT_COLS + DELTA_JOINT_COLS + ['sin_time', 'cos_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9713fb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Categorical Vocabularies for Embeddings\n",
    "# Pain surveys are categorical (0,1,2), not continuous scalars.\n",
    "survey_vocab_sizes = [int(df_features_engineered[c].max() + 1) for c in SURVEY_COLS]\n",
    "time_vocab_size = int(df_features_engineered[TIME_COL].max() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b10f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Text Feature (Team Name)\n",
    "# We treat the Team Name as a categorical entity via Label Encoding\n",
    "exclude_cols = ['label', 'sample_index']\n",
    "string_cols = df_features_raw.select_dtypes(include=['object']).columns.tolist()\n",
    "string_cols = [c for c in string_cols if c not in exclude_cols]\n",
    "\n",
    "TEXT_COL = None\n",
    "TEXT_VOCAB_SIZE = 0\n",
    "\n",
    "if len(string_cols) > 0:\n",
    "    TEXT_COL = string_cols[0] \n",
    "    print(f\"Trovata colonna 'Team Name': {TEXT_COL}\")\n",
    "    \n",
    "    def clean_team_name(text):\n",
    "        if pd.isna(text): return \"unknown\"\n",
    "        return re.sub(r'[^a-z0-9]', '', str(text).lower())\n",
    "\n",
    "    # Apply cleaning to the engineered dataframes\n",
    "    df_features_engineered[TEXT_COL] = df_features_engineered[TEXT_COL].apply(clean_team_name)\n",
    "    df_test_engineered[TEXT_COL] = df_test_engineered[TEXT_COL].apply(clean_team_name)\n",
    "    \n",
    "    le_text = LabelEncoder()\n",
    "    # Fit on the combined text from the *engineered* dataframes\n",
    "    all_text = pd.concat([df_features_engineered[TEXT_COL], df_test_engineered[TEXT_COL]], axis=0)\n",
    "    le_text.fit(all_text)\n",
    "    \n",
    "    # Transform the *engineered* dataframes, which are used later\n",
    "    df_features_engineered[TEXT_COL] = le_text.transform(df_features_engineered[TEXT_COL])\n",
    "    df_test_engineered[TEXT_COL] = le_text.transform(df_test_engineered[TEXT_COL])\n",
    "    \n",
    "    TEXT_VOCAB_SIZE = len(le_text.classes_)\n",
    "else:\n",
    "    print(\"Nessuna colonna 'Team Name' trovata.\")\n",
    "\n",
    "# Map Targets\n",
    "label_mapping = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "df_labels_raw['label_encoded'] = df_labels_raw['label'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ee2612",
   "metadata": {},
   "source": [
    "## PART 3: DATASET & SAMPLING STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d52d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiratePainDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Handles windowing of time-series data.\n",
    "    Separates Continuous inputs (for Scaler) from Categorical inputs (for Embeddings).\n",
    "    \"\"\"\n",
    "    def __init__(self, features_df, labels_df, sample_indices, window_size, stride, text_col=None, augment=False):\n",
    "        self.features_df = features_df\n",
    "        self.labels_df = labels_df.set_index('sample_index') if labels_df is not None else None\n",
    "        self.sample_indices = sample_indices\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.text_col = text_col\n",
    "        self.augment = augment \n",
    "        \n",
    "        # Grouping for O(1) access\n",
    "        self.grouped_features = dict(tuple(features_df.groupby('sample_index')))\n",
    "        self.indices = self._create_indices()\n",
    "\n",
    "    def _create_indices(self):\n",
    "        # Creates a list of valid (sample_idx, start, end) tuples\n",
    "        indices = []\n",
    "        for sample_idx in self.sample_indices:\n",
    "            if sample_idx not in self.grouped_features: continue\n",
    "            data = self.grouped_features[sample_idx]\n",
    "            n_timesteps = len(data)\n",
    "            for start in range(0, n_timesteps - self.window_size + 1, self.stride):\n",
    "                indices.append((sample_idx, start, start + self.window_size))\n",
    "        return indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_idx, start, end = self.indices[idx]\n",
    "        window_data = self.grouped_features[sample_idx].iloc[start:end]\n",
    "\n",
    "        # 1. Continuous Data + Gaussian Noise Jittering (Augmentation)\n",
    "        vals = window_data[CONTINUOUS_COLS].values\n",
    "        if self.augment:\n",
    "            noise = np.random.normal(0, 0.02, vals.shape) \n",
    "            vals = vals + noise\n",
    "        x_cont = torch.tensor(vals, dtype=torch.float)\n",
    "        \n",
    "        # 2. Categorical Data (Surveys + Time)\n",
    "        x_survey = torch.tensor((window_data[SURVEY_COLS].values + 1), dtype=torch.long)\n",
    "        x_time = torch.tensor((window_data[TIME_COL].values + 1), dtype=torch.long)\n",
    "        \n",
    "        # 3. Text Data\n",
    "        x_text = torch.tensor(0, dtype=torch.long)\n",
    "        if self.text_col:\n",
    "            val = window_data[self.text_col].iloc[0]\n",
    "            x_text = torch.tensor(val, dtype=torch.long)\n",
    "\n",
    "        # 4. Target\n",
    "        label = torch.tensor(-1, dtype=torch.long)\n",
    "        if self.labels_df is not None:\n",
    "            label = torch.tensor(self.labels_df.loc[sample_idx, 'label_encoded'], dtype=torch.long)\n",
    "\n",
    "        return x_cont, x_survey, x_time, x_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734c35a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_sampler(dataset, labels_df):\n",
    "    \"\"\"\n",
    "    Creates a WeightedRandomSampler to handle class imbalance.\n",
    "    Rare classes are sampled more frequently to balance the batches.\n",
    "    \"\"\"\n",
    "    sample_to_label = labels_df.set_index('sample_index')['label_encoded'].to_dict()\n",
    "    label_counts = labels_df['label_encoded'].value_counts().sort_index()\n",
    "    class_weights = 1.0 / label_counts\n",
    "    \n",
    "    weights = []\n",
    "    for idx_tuple in dataset.indices:\n",
    "        s_idx = idx_tuple[0]\n",
    "        if s_idx in sample_to_label:\n",
    "            l = sample_to_label[s_idx]\n",
    "            weights.append(class_weights[l])\n",
    "        else:\n",
    "            weights.append(0)\n",
    "    return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5313b54",
   "metadata": {},
   "source": [
    "## PART 4: ADVANCED TRAINING UTILITIES (LOSS)\n",
    "// for the training I'm working on now, I've added also a MixUp Augmentation technique, let's hope it's gonna be useful to improve our score :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd09e3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss with Label Smoothing.\n",
    "    - Focal term: Focuses learning on hard-to-classify examples.\n",
    "    - Label Smoothing: Prevents the model from becoming over-confident (1.0 probability),\n",
    "      which is a sign of overfitting on small data.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean', label_smoothing=0.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(\n",
    "            inputs, targets, reduction='none', weight=self.alpha, \n",
    "            label_smoothing=self.label_smoothing\n",
    "        )\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean() if self.reduction == 'mean' else focal_loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a12ab1",
   "metadata": {},
   "source": [
    "## PART 5: MODEL ARCHITECTURE (CNN-LSTM)\n",
    "// for the training I'm working on now, I've added also an \"Attention\" block, let's hope it's gonna be useful to improve our score :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae050dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiratePainModel(nn.Module):\n",
    "    def __init__(self, n_continuous, survey_vocab_sizes, time_vocab_size, text_vocab_size, lstm_hidden=128, n_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. EMBEDDING LAYERS (For Categorical Inputs)\n",
    "        self.emb_surveys = nn.ModuleList([nn.Embedding(v+2, 4) for v in survey_vocab_sizes])\n",
    "        self.emb_time = nn.Embedding(time_vocab_size+2, 8)\n",
    "        \n",
    "        self.use_text = (text_vocab_size > 0)\n",
    "        text_dim = 8 if self.use_text else 0\n",
    "        if self.use_text:\n",
    "            self.emb_text = nn.Embedding(text_vocab_size+2, 8)\n",
    "            \n",
    "        # Calculate total input dimension\n",
    "        total_survey_dim = len(survey_vocab_sizes) * 4\n",
    "        input_dim = n_continuous + total_survey_dim + 8 + text_dim\n",
    "        \n",
    "        # 2. CNN BLOCK (Local Feature Extraction)\n",
    "        # Extracts local shapes/patterns before temporal processing.\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # 3. LSTM BLOCK (Long-Term Memory)\n",
    "        # Processes the sequence of features extracted by the CNN.\n",
    "        self.lstm = nn.LSTM(\n",
    "            128, \n",
    "            lstm_hidden, \n",
    "            num_layers=2, \n",
    "            batch_first=True, \n",
    "            dropout=0.3, \n",
    "            bidirectional=True  # Aggiunto\n",
    "        )\n",
    "\n",
    "        self.attention_w = nn.Linear(lstm_hidden * 2, lstm_hidden * 2)\n",
    "        self.attention_v = nn.Linear(lstm_hidden * 2, 1, bias=False)\n",
    "        \n",
    "        # 5. CLASSIFIER\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(lstm_hidden * 2, n_classes)\n",
    "\n",
    "    def forward(self, x_cont, x_survey, x_time, x_text):\n",
    "        batch_size, seq_len, _ = x_cont.shape\n",
    "        \n",
    "        # Process Embeddings\n",
    "        e_surv = [emb(x_survey[:,:,i]) for i, emb in enumerate(self.emb_surveys)]\n",
    "        e_time = self.emb_time(x_time)\n",
    "        \n",
    "        features = [x_cont] + e_surv + [e_time]\n",
    "        if self.use_text:\n",
    "            e_txt = self.emb_text(x_text).unsqueeze(1).repeat(1, seq_len, 1)\n",
    "            features.append(e_txt)\n",
    "            \n",
    "        full_input = torch.cat(features, dim=2) # (B, Seq, Feat)\n",
    "        \n",
    "        # CNN Pass (Permute required for Conv1d: B, Channels, Seq)\n",
    "        x = full_input.permute(0, 2, 1)\n",
    "        x = self.cnn(x)\n",
    "        \n",
    "        # LSTM Pass (Permute back: B, Seq, Channels)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        out, _ = self.lstm(x)\n",
    "        \n",
    "        # Final Classification\n",
    "        # 1. Calcola l'energia (quanto Ã¨ \"importante\" ogni step)\n",
    "        #    (B, Seq, H*2) -> (B, Seq, H*2)\n",
    "        e = torch.tanh(self.attention_w(out))\n",
    "        \n",
    "        # 2. Calcola i punteggi (scores)\n",
    "        #    (B, Seq, H*2) -> (B, Seq, 1)\n",
    "        scores = self.attention_v(e)\n",
    "        \n",
    "        # 3. Normalizza i punteggi in pesi (alpha) con Softmax\n",
    "        #    (B, Seq, 1)\n",
    "        alpha = F.softmax(scores, dim=1)\n",
    "        \n",
    "        # 4. Calcola il vettore di contesto (weighted sum)\n",
    "        #    Moltiplica i pesi (alpha) per gli output (out)\n",
    "        #    (B, 1, Seq) @ (B, Seq, H*2) -> (B, 1, H*2)\n",
    "        context_vector = torch.bmm(alpha.permute(0, 2, 1), out).squeeze(1) # Rimuove la dim 1\n",
    "        \n",
    "        # 5. Classifica usando il vettore di contesto\n",
    "        #    (B, H*2) -> (B, n_classes)\n",
    "        logits = self.classifier(self.dropout(context_vector))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8055caaa",
   "metadata": {},
   "source": [
    "## PART 6: TRAINING LOOP & ENSEMBLE EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32bb890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xc, xs, xt, xtxt, y in loader:\n",
    "        xc, xs, xt, xtxt, y = xc.to(device), xs.to(device), xt.to(device), xtxt.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xc, xs, xt, xtxt)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient Clipping (Stabilization)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_VALUE)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# --- MAIN EXECUTION: K-FOLD ENSEMBLE ---\n",
    "print(\"\\n--- Starting Stratified K-Fold Training (Ensemble) ---\")\n",
    "\n",
    "all_sample_indices = df_labels_raw['sample_index'].unique()\n",
    "all_labels_strat = df_labels_raw.set_index('sample_index').loc[all_sample_indices]['label_encoded'].values\n",
    "\n",
    "# Storage for Out-Of-Fold (OOF) predictions\n",
    "oof_probs = np.zeros((len(all_sample_indices), 3))\n",
    "oof_targets = np.zeros(len(all_sample_indices))\n",
    "sample_to_idx = {s: i for i, s in enumerate(all_sample_indices)}\n",
    "models_list = []  \n",
    "\n",
    "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(all_sample_indices, all_labels_strat)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{K_FOLDS} ---\")\n",
    "    \n",
    "    train_samples = all_sample_indices[train_idx]\n",
    "    val_samples = all_sample_indices[val_idx]\n",
    "    \n",
    "    # Standard Scaling (fitted on fold training data)\n",
    "    scaler = StandardScaler()\n",
    "    train_subset = df_features_engineered[df_features_engineered['sample_index'].isin(train_samples)]\n",
    "    scaler.fit(train_subset[CONTINUOUS_COLS])\n",
    "    \n",
    "    df_fold = df_features_engineered.copy()\n",
    "    df_fold[CONTINUOUS_COLS] = scaler.transform(df_fold[CONTINUOUS_COLS])\n",
    "    \n",
    "    # Datasets & Loaders\n",
    "    train_ds = PiratePainDataset(df_fold, df_labels_raw, train_samples, WINDOW_SIZE, STRIDE, TEXT_COL, augment=True)\n",
    "    val_ds = PiratePainDataset(df_fold, df_labels_raw, val_samples, WINDOW_SIZE, STRIDE, TEXT_COL, augment=False)\n",
    "    \n",
    "    # Weighted Sampler for Training\n",
    "    sampler = get_weighted_sampler(train_ds, df_labels_raw)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, shuffle=False, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Model Initialization\n",
    "    model = PiratePainModel(\n",
    "        n_continuous=len(CONTINUOUS_COLS), \n",
    "        survey_vocab_sizes=survey_vocab_sizes, \n",
    "        time_vocab_size=time_vocab_size,\n",
    "        text_vocab_size=TEXT_VOCAB_SIZE\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    criterion = FocalLoss(alpha=None, gamma=2.0, label_smoothing=LABEL_SMOOTHING)\n",
    "    \n",
    "    best_v_f1 = 0\n",
    "    best_model_wts = None\n",
    "    \n",
    "    # Epoch Loop\n",
    "    for ep in range(EPOCHS):\n",
    "        t_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "        \n",
    "        # Validation: Soft Voting per Sample\n",
    "        model.eval()\n",
    "        val_logits_list, val_sample_indices_list = [], []\n",
    "        window_sample_map_val = [x[0] for x in val_ds.indices]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for xc, xs, xt, xtxt, y in val_loader:\n",
    "                xc, xs, xt, xtxt = xc.to(device), xs.to(device), xt.to(device), xtxt.to(device)\n",
    "                logits = model(xc, xs, xt, xtxt)\n",
    "                val_logits_list.extend(logits.cpu().numpy())\n",
    "        \n",
    "        # Aggregate window predictions to sample predictions\n",
    "        df_val_logits = pd.DataFrame(val_logits_list, columns=[0, 1, 2])\n",
    "        df_val_logits['sample_index'] = window_sample_map_val\n",
    "        df_val_probs = df_val_logits.groupby('sample_index').mean()\n",
    "        \n",
    "        # Calculate Metrics using temporary argmax\n",
    "        current_val_probs = torch.softmax(torch.tensor(df_val_probs.values), dim=1).numpy()\n",
    "        current_val_preds = np.argmax(current_val_probs, axis=1)\n",
    "        \n",
    "        current_val_indices = df_val_probs.index\n",
    "        current_val_labels = df_labels_raw.set_index('sample_index').loc[current_val_indices]['label_encoded'].values\n",
    "        \n",
    "        v_f1 = f1_score(current_val_labels, current_val_preds, average='weighted')\n",
    "        \n",
    "        if v_f1 > best_v_f1:\n",
    "            best_v_f1 = v_f1\n",
    "            best_model_wts = model.state_dict()\n",
    "            # Store OOF Probabilities for Threshold Optimization\n",
    "            for idx, s_idx in enumerate(current_val_indices):\n",
    "                global_idx = sample_to_idx[s_idx]\n",
    "                oof_probs[global_idx] = current_val_probs[idx]\n",
    "                oof_targets[global_idx] = current_val_labels[idx]\n",
    "\n",
    "    print(f\"Fold {fold+1} Best Val F1: {best_v_f1:.4f}\")\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    models_list.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968a9911",
   "metadata": {},
   "source": [
    "## PART 7: THRESHOLD OPTIMIZATION & INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Optimizing Decision Thresholds on OOF Data ---\")\n",
    "\n",
    "# Standard Argmax favors the majority class. We find custom thresholds to \n",
    "# maximize F1 Score for rare classes.\n",
    "\n",
    "best_thresh = (0.0, 0.0)\n",
    "best_score = 0.0\n",
    "\n",
    "for t_high in np.arange(0.15, 0.50, 0.01):\n",
    "    for t_low in np.arange(0.20, 0.55, 0.01):\n",
    "        if t_low >= t_high: continue\n",
    "        \n",
    "        preds = []\n",
    "        for p in oof_probs:\n",
    "            if p[2] > t_high: preds.append(2)\n",
    "            elif p[1] > t_low: preds.append(1)\n",
    "            else: preds.append(0)\n",
    "            \n",
    "        s = f1_score(oof_targets, preds, average='weighted')\n",
    "        if s > best_score:\n",
    "            best_score = s\n",
    "            best_thresh = (t_low, t_high)\n",
    "\n",
    "print(f\"Optimal Thresholds Found: Low > {best_thresh[0]:.2f}, High > {best_thresh[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35477fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FINAL INFERENCE ---\n",
    "print(\"\\n--- Submission generation (Ensemble) ---\")\n",
    "\n",
    "# Prepare Test Data\n",
    "final_scaler = StandardScaler()\n",
    "final_scaler.fit(df_features_engineered[CONTINUOUS_COLS])\n",
    "df_test_scaled = df_test_engineered.copy()\n",
    "df_test_scaled[CONTINUOUS_COLS] = final_scaler.transform(df_test_scaled[CONTINUOUS_COLS])\n",
    "\n",
    "sub_indices = pd.read_csv('/kaggle/input/pirate/sample_submission.csv')['sample_index'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1fc78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We predict N times with augmentation enabled and average the results.\n",
    "\n",
    "sub_indices = pd.read_csv('/kaggle/input/pirate/sample_submission.csv')['sample_index'].unique()\n",
    "test_ds_final = PiratePainDataset(df_test_scaled, None, sub_indices, WINDOW_SIZE, STRIDE, TEXT_COL, augment=False)\n",
    "test_loader_final = DataLoader(test_ds_final, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "window_sample_map_test = [x[0] for x in test_ds_final.indices]\n",
    "\n",
    "ensemble_logits = None\n",
    "\n",
    "# Loop over each model in the ensemble\n",
    "for i, model in enumerate(models_list):\n",
    "    model.eval()\n",
    "    fold_logits = []\n",
    "    with torch.no_grad():\n",
    "        for xc, xs, xt, xtxt, _ in test_loader_final:\n",
    "            xc, xs, xt, xtxt = xc.to(device), xs.to(device), xt.to(device), xtxt.to(device)\n",
    "            logits = model(xc, xs, xt, xtxt)\n",
    "            fold_logits.extend(logits.cpu().numpy())\n",
    "        \n",
    "        # Aggregate windows\n",
    "        df_tmp = pd.DataFrame(fold_logits, columns=[0, 1, 2])\n",
    "        df_tmp['sample_index'] = window_sample_map_test\n",
    "        df_avg = df_tmp.groupby('sample_index').mean()\n",
    "        \n",
    "        if ensemble_logits is None:\n",
    "            ensemble_logits = df_avg\n",
    "        else:\n",
    "            ensemble_logits = ensemble_logits.add(df_avg, fill_value=0)\n",
    "\n",
    "ensemble_logits /= K_FOLDS\n",
    "final_probs = torch.softmax(torch.tensor(ensemble_logits.values), dim=1).numpy()\n",
    "\n",
    "# Apply Optimized Thresholds\n",
    "final_preds_list = []\n",
    "thr_l, thr_h = best_thresh\n",
    "\n",
    "for p in final_probs:\n",
    "    if p[2] > thr_h: final_preds_list.append(2)\n",
    "    elif p[1] > thr_l: final_preds_list.append(1)\n",
    "    else: final_preds_list.append(0)\n",
    "\n",
    "final_series = pd.Series(final_preds_list, index=ensemble_logits.index)\n",
    "\n",
    "# Export\n",
    "inv_map = {v: k for k, v in label_mapping.items()}\n",
    "submission = final_series.map(inv_map).reset_index()\n",
    "submission.columns = ['sample_index', 'label']\n",
    "\n",
    "sample_sub = pd.read_csv('/kaggle/input/pirate/sample_submission.csv')\n",
    "submission = submission.set_index('sample_index').reindex(sample_sub['sample_index']).reset_index()\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29b18ac5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T08:46:52.732986Z",
     "iopub.status.busy": "2025-11-13T08:46:52.732728Z",
     "iopub.status.idle": "2025-11-13T08:46:58.817783Z",
     "shell.execute_reply": "2025-11-13T08:46:58.816887Z"
    },
    "papermill": {
     "duration": 6.08992,
     "end_time": "2025-11-13T08:46:58.819039",
     "exception": false,
     "start_time": "2025-11-13T08:46:52.729119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in uso: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Ignora avvisi non critici\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Parametri Globali ---\n",
    "JOINT_COLS = [f'joint_{i:02d}' for i in range(30)]\n",
    "SURVEY_COLS = ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
    "STATIC_COLS = ['n_legs', 'n_hands', 'n_eyes'] \n",
    "TIME_COL = 'time'\n",
    "\n",
    "WINDOW_SIZE = 40\n",
    "STRIDE = 10\n",
    "\n",
    "# --- Parametri Training ---\n",
    "SEED = 42\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 50 \n",
    "GRADIENT_CLIP_VALUE = 1.0 \n",
    "K_FOLDS = 5 \n",
    "\n",
    "# Setup Riproducibilità\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device in uso: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a96889b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T08:46:58.824185Z",
     "iopub.status.busy": "2025-11-13T08:46:58.823844Z",
     "iopub.status.idle": "2025-11-13T08:47:04.899522Z",
     "shell.execute_reply": "2025-11-13T08:47:04.898702Z"
    },
    "papermill": {
     "duration": 6.079607,
     "end_time": "2025-11-13T08:47:04.900813",
     "exception": false,
     "start_time": "2025-11-13T08:46:58.821206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Caricamento e Pulizia Iniziale ---\n",
      "Nessuna colonna 'Team Name' trovata (a parte le feature numeriche).\n",
      "Calcolo Delta Features...\n",
      "Preprocessing Completato.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Caricamento e Pulizia Iniziale ---\")\n",
    "\n",
    "df_features_raw = pd.read_csv('/kaggle/input/pirate/pirate_pain_train.csv')\n",
    "df_labels_raw = pd.read_csv('/kaggle/input/pirate/pirate_pain_train_labels.csv')\n",
    "df_test_raw = pd.read_csv('/kaggle/input/pirate/pirate_pain_test.csv')\n",
    "\n",
    "# 1. FIX: Forziamo le colonne statiche a essere Interi (per evitare che Pandas le legga come stringhe)\n",
    "for col in STATIC_COLS:\n",
    "    df_features_raw[col] = pd.to_numeric(df_features_raw[col], errors='coerce').fillna(0).astype(int)\n",
    "    df_test_raw[col] = pd.to_numeric(df_test_raw[col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# 2. COMMUNICATION HINT: Cerca una VERA colonna di testo (Team Name)\n",
    "exclude_cols = ['label', 'sample_index']\n",
    "string_cols = df_features_raw.select_dtypes(include=['object']).columns.tolist()\n",
    "string_cols = [c for c in string_cols if c not in exclude_cols]\n",
    "\n",
    "TEXT_COL = None\n",
    "TEXT_VOCAB_SIZE = 0\n",
    "\n",
    "if len(string_cols) > 0:\n",
    "    TEXT_COL = string_cols[0] \n",
    "    print(f\"Trovata colonna 'Team Name': {TEXT_COL}\")\n",
    "    \n",
    "    def clean_team_name(text):\n",
    "        if pd.isna(text): return \"unknown\"\n",
    "        return re.sub(r'[^a-z0-9]', '', str(text).lower())\n",
    "\n",
    "    df_features_raw[TEXT_COL] = df_features_raw[TEXT_COL].apply(clean_team_name)\n",
    "    df_test_raw[TEXT_COL] = df_test_raw[TEXT_COL].apply(clean_team_name)\n",
    "    \n",
    "    le_text = LabelEncoder()\n",
    "    all_text = pd.concat([df_features_raw[TEXT_COL], df_test_raw[TEXT_COL]], axis=0)\n",
    "    le_text.fit(all_text)\n",
    "    \n",
    "    df_features_raw[TEXT_COL] = le_text.transform(df_features_raw[TEXT_COL])\n",
    "    df_test_raw[TEXT_COL] = le_text.transform(df_test_raw[TEXT_COL])\n",
    "    \n",
    "    TEXT_VOCAB_SIZE = len(le_text.classes_)\n",
    "    print(f\"Vocabolario Team Name: {TEXT_VOCAB_SIZE} squadre uniche.\")\n",
    "else:\n",
    "    print(\"Nessuna colonna 'Team Name' trovata (a parte le feature numeriche).\")\n",
    "\n",
    "# 3. Feature Engineering Delta\n",
    "def engineer_features(df):\n",
    "    df_eng = df.copy()\n",
    "    grouped = df_eng.groupby('sample_index')\n",
    "    for col in JOINT_COLS:\n",
    "        df_eng[f'd_{col}'] = grouped[col].diff().fillna(0)\n",
    "    \n",
    "    # --- MODIFICA QUI (Advice 12/11) ---\n",
    "    # Aggiungiamo Seno e Coseno per catturare la ciclicità del tempo\n",
    "    # Assumiamo un ciclo massimo di circa 160 step (lunghezza tipica sequenza)\n",
    "    max_time_val = df_eng[TIME_COL].max() + 1 \n",
    "    df_eng['sin_time'] = np.sin(2 * np.pi * df_eng[TIME_COL] / max_time_val)\n",
    "    df_eng['cos_time'] = np.cos(2 * np.pi * df_eng[TIME_COL] / max_time_val)\n",
    "    # ------------------------------------\n",
    "\n",
    "    if 'joint_30' in df_eng.columns:\n",
    "        df_eng = df_eng.drop(columns=['joint_30'])\n",
    "    return df_eng\n",
    "\n",
    "print(\"Calcolo Delta Features...\")\n",
    "df_features_engineered = engineer_features(df_features_raw)\n",
    "df_test_engineered = engineer_features(df_test_raw)\n",
    "\n",
    "DELTA_JOINT_COLS = [f'd_{col}' for col in JOINT_COLS]\n",
    "CONTINUOUS_COLS = JOINT_COLS + DELTA_JOINT_COLS + ['sin_time', 'cos_time']\n",
    "\n",
    "# --- Preparazione Vocabolari per Embedding (CORREZIONE QUI) ---\n",
    "# Usiamo int(...) invece di .astype(int)\n",
    "survey_vocab_sizes = [int(df_features_engineered[c].max() + 1) for c in SURVEY_COLS]\n",
    "time_vocab_size = int(df_features_engineered[TIME_COL].max() + 1)\n",
    "static_vocab_sizes = [int(df_features_engineered[c].max() + 1) for c in STATIC_COLS]\n",
    "\n",
    "print(\"Preprocessing Completato.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7415da1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T08:47:04.906697Z",
     "iopub.status.busy": "2025-11-13T08:47:04.906430Z",
     "iopub.status.idle": "2025-11-13T08:47:04.917894Z",
     "shell.execute_reply": "2025-11-13T08:47:04.917356Z"
    },
    "papermill": {
     "duration": 0.015707,
     "end_time": "2025-11-13T08:47:04.918877",
     "exception": false,
     "start_time": "2025-11-13T08:47:04.903170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mappatura Label\n",
    "label_mapping = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "df_labels_raw['label_encoded'] = df_labels_raw['label'].map(label_mapping)\n",
    "\n",
    "class PiratePainDataset(Dataset):\n",
    "    def __init__(self, features_df, labels_df, sample_indices, window_size, stride, text_col=None, augment=False):\n",
    "        self.features_df = features_df\n",
    "        # Se labels_df è None, siamo in fase di test\n",
    "        self.labels_df = labels_df.set_index('sample_index') if labels_df is not None else None\n",
    "        self.sample_indices = sample_indices\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.text_col = text_col\n",
    "        \n",
    "        # Raggruppamento per accesso veloce\n",
    "        self.grouped_features = dict(tuple(features_df.groupby('sample_index')))\n",
    "        self.indices = self._create_indices()\n",
    "\n",
    "        self.augment = augment # Salva il parametro\n",
    "\n",
    "    def _create_indices(self):\n",
    "        indices = []\n",
    "        for sample_idx in self.sample_indices:\n",
    "            if sample_idx not in self.grouped_features: continue\n",
    "            data = self.grouped_features[sample_idx]\n",
    "            n_timesteps = len(data)\n",
    "            for start in range(0, n_timesteps - self.window_size + 1, self.stride):\n",
    "                indices.append((sample_idx, start, start + self.window_size))\n",
    "        return indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_idx, start, end = self.indices[idx]\n",
    "        window_data = self.grouped_features[sample_idx].iloc[start:end]\n",
    "\n",
    "        # 1. Continui (Modifica qui!)\n",
    "        vals = window_data[CONTINUOUS_COLS].values\n",
    "        \n",
    "        # --- INIZIO AUGMENTATION ---\n",
    "        if self.augment:\n",
    "            # Aggiunge rumore casuale (Gaussian Noise)\n",
    "            noise = np.random.normal(0, 0.02, vals.shape) \n",
    "            vals = vals + noise\n",
    "        # --- FINE AUGMENTATION ---\n",
    "\n",
    "        # 1. Continui\n",
    "        x_cont = torch.tensor(window_data[CONTINUOUS_COLS].values, dtype=torch.float)\n",
    "        # 2. Survey (+1 sicurezza)\n",
    "        x_survey = torch.tensor((window_data[SURVEY_COLS].values + 1), dtype=torch.long)\n",
    "        # 3. Time (+1 sicurezza)\n",
    "        x_time = torch.tensor((window_data[TIME_COL].values + 1), dtype=torch.long)\n",
    "        # 4. Static (Legs, Hands, Eyes) - Prendi la prima riga (sono costanti)\n",
    "        x_static = torch.tensor((window_data[STATIC_COLS].iloc[0].values + 1), dtype=torch.long)\n",
    "        \n",
    "        # 5. Text (Opzionale)\n",
    "        x_text = torch.tensor(0, dtype=torch.long)\n",
    "        if self.text_col:\n",
    "            val = window_data[self.text_col].iloc[0]\n",
    "            x_text = torch.tensor(val, dtype=torch.long)\n",
    "\n",
    "        label = torch.tensor(-1, dtype=torch.long)\n",
    "        if self.labels_df is not None:\n",
    "            label = torch.tensor(self.labels_df.loc[sample_idx, 'label_encoded'], dtype=torch.long)\n",
    "\n",
    "        return x_cont, x_survey, x_time, x_static, x_text, label\n",
    "\n",
    "# --- Weighted Sampler (Per Advice 08/11 Advanced) ---\n",
    "def get_weighted_sampler(dataset, labels_df):\n",
    "    # Mappa sample -> label\n",
    "    sample_to_label = labels_df.set_index('sample_index')['label_encoded'].to_dict()\n",
    "    # Calcola frequenza inversa classi\n",
    "    label_counts = labels_df['label_encoded'].value_counts().sort_index()\n",
    "    class_weights = 1.0 / label_counts\n",
    "    \n",
    "    # Assegna peso a ogni finestra nel dataset\n",
    "    weights = []\n",
    "    for idx_tuple in dataset.indices:\n",
    "        s_idx = idx_tuple[0]\n",
    "        if s_idx in sample_to_label:\n",
    "            l = sample_to_label[s_idx]\n",
    "            weights.append(class_weights[l])\n",
    "        else:\n",
    "            weights.append(0) # Non dovrebbe accadere in train\n",
    "            \n",
    "    return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "763e4a45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T08:47:04.924034Z",
     "iopub.status.busy": "2025-11-13T08:47:04.923660Z",
     "iopub.status.idle": "2025-11-13T08:47:04.937450Z",
     "shell.execute_reply": "2025-11-13T08:47:04.936740Z"
    },
    "papermill": {
     "duration": 0.017656,
     "end_time": "2025-11-13T08:47:04.938516",
     "exception": false,
     "start_time": "2025-11-13T08:47:04.920860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Focal Loss Custom ---\n",
    "class FocalLoss(nn.Module):\n",
    "    # Aggiungiamo il parametro label_smoothing (default 0.0, consigliato 0.1)\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean', label_smoothing=0.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.label_smoothing = label_smoothing # Salva il valore\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # QUI applichiamo l'Advice 09/11: label_smoothing=self.label_smoothing\n",
    "        ce_loss = F.cross_entropy(\n",
    "            inputs, \n",
    "            targets, \n",
    "            reduction='none', \n",
    "            weight=self.alpha, \n",
    "            label_smoothing=self.label_smoothing  # <--- ECCOLO!\n",
    "        )\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean() if self.reduction == 'mean' else focal_loss.sum()\n",
    "\n",
    "# --- Modello Completo ---\n",
    "import math\n",
    "\n",
    "# --- Modulo Positional Encoding (Necessario per i Transformer) ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=500):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Seq_len, Emebedding_dim]\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# --- IL MODELLO \"CIOTTO\": Transformer Encoder ---\n",
    "class PirateTransformerModel(nn.Module):\n",
    "    def __init__(self, n_continuous, survey_vocab_sizes, time_vocab_size, \n",
    "                 static_vocab_sizes, text_vocab_size, \n",
    "                 d_model=128, nhead=4, num_layers=3, n_classes=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- 1. EMBEDDINGS (Uguale a prima) ---\n",
    "        self.emb_surveys = nn.ModuleList([nn.Embedding(v+2, 4) for v in survey_vocab_sizes])\n",
    "        self.emb_time = nn.Embedding(time_vocab_size+2, 8)\n",
    "        self.emb_static = nn.ModuleList([nn.Embedding(v+2, 4) for v in static_vocab_sizes])\n",
    "        \n",
    "        self.use_text = (text_vocab_size > 0)\n",
    "        text_dim = 8 if self.use_text else 0\n",
    "        if self.use_text:\n",
    "            self.emb_text = nn.Embedding(text_vocab_size+2, text_dim)\n",
    "            \n",
    "        # Calcolo dimensioni input\n",
    "        total_survey_dim = len(survey_vocab_sizes) * 4\n",
    "        total_static_dim = len(static_vocab_sizes) * 4\n",
    "        input_dim = n_continuous + total_survey_dim + 8 + total_static_dim + text_dim\n",
    "        \n",
    "        # Proiezione lineare per portare tutto a d_model (es. 128)\n",
    "        # Questo serve perché il Transformer vuole una dimensione fissa per le \"teste\"\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # --- 2. POSITIONAL ENCODING ---\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # --- 3. TRANSFORMER ENCODER BLOCK ---\n",
    "        # \"batch_first=True\" è cruciale per come abbiamo i dati\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, \n",
    "                                                    dim_feedforward=d_model*4, \n",
    "                                                    dropout=dropout, \n",
    "                                                    batch_first=True,\n",
    "                                                    activation='gelu') # GELU è meglio di RELU\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        \n",
    "        # --- 4. CLASSIFICATION HEAD ---\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cont, x_survey, x_time, x_static, x_text):\n",
    "        # (Stessa logica di concatenazione della LSTM)\n",
    "        batch_size, seq_len, _ = x_cont.shape\n",
    "        \n",
    "        e_surv = [emb(x_survey[:,:,i]) for i, emb in enumerate(self.emb_surveys)]\n",
    "        e_time = self.emb_time(x_time)\n",
    "        \n",
    "        e_stat = [emb(x_static[:,i]) for i, emb in enumerate(self.emb_static)]\n",
    "        e_stat_cat = torch.cat(e_stat, dim=1)\n",
    "        if self.use_text:\n",
    "            e_txt = self.emb_text(x_text)\n",
    "            e_stat_cat = torch.cat([e_stat_cat, e_txt], dim=1)\n",
    "        e_stat_seq = e_stat_cat.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        \n",
    "        # Concatena tutto\n",
    "        full_input = torch.cat([x_cont] + e_surv + [e_time, e_stat_seq], dim=2)\n",
    "        \n",
    "        # --- DIFFERENZA KEY: Proiezione + Transformer ---\n",
    "        \n",
    "        # 1. Proietta a d_model (es. 68 features -> 128 dimensioni)\n",
    "        x = self.input_projection(full_input) \n",
    "        \n",
    "        # 2. Aggiungi info posizionale (il Transformer non sa l'ordine senza questo)\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # 3. Passa nel Transformer (Self-Attention)\n",
    "        # Output: [Batch, Seq, d_model]\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # 4. Pooling Strategy (Attention Pooling o Mean Pooling)\n",
    "        # Qui usiamo Mean Pooling sull'asse temporale (media di tutti gli step)\n",
    "        # Alternativa: Prendere solo l'ultimo step x[:, -1, :]\n",
    "        x_pooled = x.mean(dim=1) \n",
    "        \n",
    "        # 5. Classifica\n",
    "        logits = self.classifier(x_pooled)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "182f5547",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T08:47:04.943466Z",
     "iopub.status.busy": "2025-11-13T08:47:04.943016Z",
     "iopub.status.idle": "2025-11-13T08:47:04.949419Z",
     "shell.execute_reply": "2025-11-13T08:47:04.948842Z"
    },
    "papermill": {
     "duration": 0.009988,
     "end_time": "2025-11-13T08:47:04.950478",
     "exception": false,
     "start_time": "2025-11-13T08:47:04.940490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xc, xs, xt, xst, xtxt, y in loader:\n",
    "        xc, xs, xt, xst, xtxt, y = xc.to(device), xs.to(device), xt.to(device), xst.to(device), xtxt.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xc, xs, xt, xst, xtxt)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_VALUE)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xc, xs, xt, xst, xtxt, y in loader:\n",
    "            xc, xs, xt, xst, xtxt, y = xc.to(device), xs.to(device), xt.to(device), xst.to(device), xtxt.to(device), y.to(device)\n",
    "            \n",
    "            logits = model(xc, xs, xt, xst, xtxt)\n",
    "            loss = criterion(logits, y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "            labels.extend(y.cpu().numpy())\n",
    "            \n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    return total_loss / len(loader), f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5cecd6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T08:47:04.955539Z",
     "iopub.status.busy": "2025-11-13T08:47:04.955100Z",
     "iopub.status.idle": "2025-11-13T11:05:30.147807Z",
     "shell.execute_reply": "2025-11-13T11:05:30.146946Z"
    },
    "papermill": {
     "duration": 8305.198108,
     "end_time": "2025-11-13T11:05:30.150491",
     "exception": false,
     "start_time": "2025-11-13T08:47:04.952383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Avvio K-Fold con Ensemble Strategy ---\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Fold 1 Best Val F1: 0.9507\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Fold 2 Best Val F1: 0.9543\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Fold 3 Best Val F1: 0.9404\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Fold 4 Best Val F1: 0.9393\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Fold 5 Best Val F1: 0.9455\n",
      "\n",
      "--- Ricerca Soglie Ottimali su OOF ---\n",
      "Soglie Trovate: Low>0.43, High>0.46 -> OOF F1: 0.9488\n",
      "\n",
      "--- Generazione Submission (Ensemble 5 Models) ---\n",
      "Fatto! Ensemble Submission creata.\n"
     ]
    }
   ],
   "source": [
    "# --- FIX: Ripristina le definizioni mancanti ---\n",
    "all_sample_indices = df_labels_raw['sample_index'].unique()\n",
    "all_labels_strat = df_labels_raw.set_index('sample_index').loc[all_sample_indices]['label_encoded'].values\n",
    "\n",
    "# --- SOSTITUISCI TUTTO DA QUI IN POI ---\n",
    "\n",
    "print(\"--- Avvio K-Fold con Ensemble Strategy ---\")\n",
    "\n",
    "# Setup per OOF e Ensemble\n",
    "oof_probs = np.zeros((len(all_sample_indices), 3)) # Matrice (N_Samples, 3_Classi)\n",
    "oof_targets = np.zeros(len(all_sample_indices))\n",
    "models_list = [] # Qui salveremo i 5 modelli addestrati\n",
    "\n",
    "# Mappa SampleIndex -> Posizione nell'array (0, 1, 2...)\n",
    "sample_to_idx = {s: i for i, s in enumerate(all_sample_indices)}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(all_sample_indices, all_labels_strat)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{K_FOLDS} ---\")\n",
    "    \n",
    "    train_samples = all_sample_indices[train_idx]\n",
    "    val_samples = all_sample_indices[val_idx]\n",
    "    \n",
    "    # 1. Scaling (Standard su Train, applicato a entrambi)\n",
    "    scaler = StandardScaler()\n",
    "    train_subset = df_features_engineered[df_features_engineered['sample_index'].isin(train_samples)]\n",
    "    scaler.fit(train_subset[CONTINUOUS_COLS])\n",
    "    \n",
    "    df_fold = df_features_engineered.copy()\n",
    "    df_fold[CONTINUOUS_COLS] = scaler.transform(df_fold[CONTINUOUS_COLS])\n",
    "    \n",
    "    # 2. Dataset & Loader\n",
    "    train_ds = PiratePainDataset(df_fold, df_labels_raw, train_samples, WINDOW_SIZE, STRIDE, TEXT_COL, augment=True)\n",
    "    val_ds = PiratePainDataset(df_fold, df_labels_raw, val_samples, WINDOW_SIZE, STRIDE, TEXT_COL, augment=False)\n",
    "    \n",
    "    sampler = get_weighted_sampler(train_ds, df_labels_raw)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, shuffle=False, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # 3. Modello e Ottimizzatore\n",
    "    # Modello Transformer (Più parametri, più lento, più potente)\n",
    "    model = PirateTransformerModel(\n",
    "        n_continuous=len(CONTINUOUS_COLS), \n",
    "        survey_vocab_sizes=survey_vocab_sizes, \n",
    "        time_vocab_size=time_vocab_size,\n",
    "        static_vocab_sizes=static_vocab_sizes, \n",
    "        text_vocab_size=TEXT_VOCAB_SIZE, \n",
    "        d_model=128,   # Dimensione interna\n",
    "        nhead=4,       # Numero di teste di attenzione (deve essere divisore di d_model)\n",
    "        num_layers=3,  # Numero di blocchi transformer (più sono, più è \"ciotto\")\n",
    "        dropout=0.3    # Alto dropout per evitare overfitting sui pochi dati\n",
    "    ).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    criterion = FocalLoss(alpha=None, gamma=2.0, label_smoothing=0.1)\n",
    "    \n",
    "    # 4. Training Loop con Salvataggio Best Model\n",
    "    best_v_f1 = 0\n",
    "    best_model_wts = None\n",
    "    \n",
    "    for ep in range(EPOCHS):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for xc, xs, xt, xst, xtxt, y in train_loader:\n",
    "            xc, xs, xt, xst, xtxt, y = xc.to(device), xs.to(device), xt.to(device), xst.to(device), xtxt.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xc, xs, xt, xst, xtxt)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_VALUE)\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Validation (Soft Voting per Sample)\n",
    "        model.eval()\n",
    "        val_logits_list = []\n",
    "        val_sample_indices_list = []\n",
    "        val_labels_list = [] # Solo per controllo\n",
    "        \n",
    "        # Mappa inversa per sapere quale finestra appartiene a quale sample\n",
    "        # Nota: PiratePainDataset non restituisce sample_idx nel __getitem__, dobbiamo ricostruirlo o fidarci dell'ordine\n",
    "        # TRUCCO: Usiamo l'ordine del dataset.indices che è deterministico nel val_loader (shuffle=False)\n",
    "        window_sample_map_val = [x[0] for x in val_ds.indices]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_start = 0\n",
    "            for xc, xs, xt, xst, xtxt, y in val_loader:\n",
    "                xc, xs, xt, xst, xtxt = xc.to(device), xs.to(device), xt.to(device), xst.to(device), xtxt.to(device)\n",
    "                logits = model(xc, xs, xt, xst, xtxt)\n",
    "                val_logits_list.extend(logits.cpu().numpy())\n",
    "                \n",
    "        # Aggregazione per Sample (Soft Voting)\n",
    "        df_val_logits = pd.DataFrame(val_logits_list, columns=[0, 1, 2])\n",
    "        df_val_logits['sample_index'] = window_sample_map_val\n",
    "        df_val_probs = df_val_logits.groupby('sample_index').mean() # Media Logits\n",
    "        \n",
    "        # Calcolo F1 corrente (usando Argmax provvisorio)\n",
    "        # Dobbiamo allineare le predizioni con le label vere\n",
    "        current_val_probs = torch.softmax(torch.tensor(df_val_probs.values), dim=1).numpy()\n",
    "        current_val_preds = np.argmax(current_val_probs, axis=1)\n",
    "        \n",
    "        # Recuperiamo le label vere ordinate come nel df_val_probs\n",
    "        current_val_indices = df_val_probs.index\n",
    "        current_val_labels = df_labels_raw.set_index('sample_index').loc[current_val_indices]['label_encoded'].values\n",
    "        \n",
    "        v_f1 = f1_score(current_val_labels, current_val_preds, average='weighted')\n",
    "        \n",
    "        if v_f1 > best_v_f1:\n",
    "            best_v_f1 = v_f1\n",
    "            best_model_wts = model.state_dict()\n",
    "            # Salviamo le probabilità vincenti nell'array OOF globale\n",
    "            for idx, s_idx in enumerate(current_val_indices):\n",
    "                global_idx = sample_to_idx[s_idx]\n",
    "                oof_probs[global_idx] = current_val_probs[idx]\n",
    "                oof_targets[global_idx] = current_val_labels[idx]\n",
    "\n",
    "    print(f\"Fold {fold+1} Best Val F1: {best_v_f1:.4f}\")\n",
    "    \n",
    "    # Ricarica il miglior modello e salvalo nella lista ensemble\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    models_list.append(model)\n",
    "\n",
    "# --- OTTIMIZZAZIONE SOGLIE (Threshold Optimization) ---\n",
    "print(\"\\n--- Ricerca Soglie Ottimali su OOF ---\")\n",
    "best_thresh = (0.0, 0.0)\n",
    "best_score = 0.0\n",
    "\n",
    "# Grid Search fine\n",
    "for t_high in np.arange(0.15, 0.50, 0.01):\n",
    "    for t_low in np.arange(0.20, 0.55, 0.01):\n",
    "        if t_low >= t_high: continue # Logica: High ha priorità\n",
    "        \n",
    "        preds = []\n",
    "        for p in oof_probs:\n",
    "            if p[2] > t_high: preds.append(2)\n",
    "            elif p[1] > t_low: preds.append(1)\n",
    "            else: preds.append(0)\n",
    "            \n",
    "        s = f1_score(oof_targets, preds, average='weighted')\n",
    "        if s > best_score:\n",
    "            best_score = s\n",
    "            best_thresh = (t_low, t_high)\n",
    "\n",
    "print(f\"Soglie Trovate: Low>{best_thresh[0]:.2f}, High>{best_thresh[1]:.2f} -> OOF F1: {best_score:.4f}\")\n",
    "\n",
    "\n",
    "# --- INFERENZA ENSEMBLE (Test Set) ---\n",
    "print(\"\\n--- Generazione Submission (Ensemble 5 Models) ---\")\n",
    "\n",
    "# Preparazione Test Set (Scaling) - Nota: Usiamo scaler fittato sull'ultimo fold o rifittiamo su tutto?\n",
    "# Meglio: Per ogni modello del fold, dovremmo usare il suo scaler. \n",
    "# Ma per semplicità (e dato che StandardScaler è robusto), usiamo uno scaler fittato su tutto il train.\n",
    "final_scaler = StandardScaler()\n",
    "final_scaler.fit(df_features_engineered[CONTINUOUS_COLS])\n",
    "df_test_scaled = df_test_engineered.copy()\n",
    "df_test_scaled[CONTINUOUS_COLS] = final_scaler.transform(df_test_scaled[CONTINUOUS_COLS])\n",
    "\n",
    "# Dataset Test\n",
    "sub_indices = pd.read_csv('/kaggle/input/pirate/sample_submission.csv')['sample_index'].unique()\n",
    "test_ds_final = PiratePainDataset(df_test_scaled, None, sub_indices, WINDOW_SIZE, STRIDE, TEXT_COL, augment=False)\n",
    "test_loader_final = DataLoader(test_ds_final, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "window_sample_map_test = [x[0] for x in test_ds_final.indices]\n",
    "\n",
    "# Accumulatore Probabilità\n",
    "ensemble_logits = None\n",
    "\n",
    "for i, model in enumerate(models_list):\n",
    "    model.eval()\n",
    "    fold_logits = []\n",
    "    with torch.no_grad():\n",
    "        for xc, xs, xt, xst, xtxt, _ in test_loader_final:\n",
    "            xc, xs, xt, xst, xtxt = xc.to(device), xs.to(device), xt.to(device), xst.to(device), xtxt.to(device)\n",
    "            logits = model(xc, xs, xt, xst, xtxt)\n",
    "            fold_logits.extend(logits.cpu().numpy())\n",
    "    \n",
    "    # Aggrega per Sample (Media Logits del modello i)\n",
    "    df_tmp = pd.DataFrame(fold_logits, columns=[0, 1, 2])\n",
    "    df_tmp['sample_index'] = window_sample_map_test\n",
    "    df_avg = df_tmp.groupby('sample_index').mean() # (N_Test_Samples, 3)\n",
    "    \n",
    "    # Somma ai logits totali (allineando per indice)\n",
    "    if ensemble_logits is None:\n",
    "        ensemble_logits = df_avg\n",
    "    else:\n",
    "        ensemble_logits = ensemble_logits.add(df_avg, fill_value=0)\n",
    "\n",
    "# Media finale dei 5 modelli\n",
    "ensemble_logits /= K_FOLDS\n",
    "\n",
    "# Converti in Probabilità\n",
    "final_probs = torch.softmax(torch.tensor(ensemble_logits.values), dim=1).numpy()\n",
    "\n",
    "# Applica Soglie Ottimizzate\n",
    "final_preds_list = []\n",
    "thr_l, thr_h = best_thresh\n",
    "\n",
    "for p in final_probs:\n",
    "    if p[2] > thr_h: final_preds_list.append(2)\n",
    "    elif p[1] > thr_l: final_preds_list.append(1)\n",
    "    else: final_preds_list.append(0)\n",
    "\n",
    "final_series = pd.Series(final_preds_list, index=ensemble_logits.index)\n",
    "\n",
    "# Salvataggio\n",
    "inv_map = {v: k for k, v in label_mapping.items()}\n",
    "submission = final_series.map(inv_map).reset_index()\n",
    "submission.columns = ['sample_index', 'label']\n",
    "\n",
    "sample_sub = pd.read_csv('/kaggle/input/pirate/sample_submission.csv')\n",
    "submission = submission.set_index('sample_index').reindex(sample_sub['sample_index']).reset_index()\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Fatto! Ensemble Submission creata.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8720958,
     "sourceId": 13708868,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8322.811277,
   "end_time": "2025-11-13T11:05:31.974399",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-13T08:46:49.163122",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

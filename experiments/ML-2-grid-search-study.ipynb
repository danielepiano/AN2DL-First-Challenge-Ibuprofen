{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9ii1W1gb8LV",
        "outputId": "a90bfee1-a3fc-449c-8f30-e51a25b08bab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dati di Training (X_train_raw) ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 105760 entries, 0 to 105759\n",
            "Data columns (total 40 columns):\n",
            " #   Column         Non-Null Count   Dtype  \n",
            "---  ------         --------------   -----  \n",
            " 0   sample_index   105760 non-null  int64  \n",
            " 1   time           105760 non-null  int64  \n",
            " 2   pain_survey_1  105760 non-null  int64  \n",
            " 3   pain_survey_2  105760 non-null  int64  \n",
            " 4   pain_survey_3  105760 non-null  int64  \n",
            " 5   pain_survey_4  105760 non-null  int64  \n",
            " 6   n_legs         105760 non-null  object \n",
            " 7   n_hands        105760 non-null  object \n",
            " 8   n_eyes         105760 non-null  object \n",
            " 9   joint_00       105760 non-null  float64\n",
            " 10  joint_01       105760 non-null  float64\n",
            " 11  joint_02       105760 non-null  float64\n",
            " 12  joint_03       105760 non-null  float64\n",
            " 13  joint_04       105760 non-null  float64\n",
            " 14  joint_05       105760 non-null  float64\n",
            " 15  joint_06       105760 non-null  float64\n",
            " 16  joint_07       105760 non-null  float64\n",
            " 17  joint_08       105760 non-null  float64\n",
            " 18  joint_09       105760 non-null  float64\n",
            " 19  joint_10       105760 non-null  float64\n",
            " 20  joint_11       105760 non-null  float64\n",
            " 21  joint_12       105760 non-null  float64\n",
            " 22  joint_13       105760 non-null  float64\n",
            " 23  joint_14       105760 non-null  float64\n",
            " 24  joint_15       105760 non-null  float64\n",
            " 25  joint_16       105760 non-null  float64\n",
            " 26  joint_17       105760 non-null  float64\n",
            " 27  joint_18       105760 non-null  float64\n",
            " 28  joint_19       105760 non-null  float64\n",
            " 29  joint_20       105760 non-null  float64\n",
            " 30  joint_21       105760 non-null  float64\n",
            " 31  joint_22       105760 non-null  float64\n",
            " 32  joint_23       105760 non-null  float64\n",
            " 33  joint_24       105760 non-null  float64\n",
            " 34  joint_25       105760 non-null  float64\n",
            " 35  joint_26       105760 non-null  float64\n",
            " 36  joint_27       105760 non-null  float64\n",
            " 37  joint_28       105760 non-null  float64\n",
            " 38  joint_29       105760 non-null  float64\n",
            " 39  joint_30       105760 non-null  float64\n",
            "dtypes: float64(31), int64(6), object(3)\n",
            "memory usage: 32.3+ MB\n",
            "\n",
            "--- Esempio di dati di Training ---\n",
            "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
            "0             0     0              2              0              2   \n",
            "1             0     1              2              2              2   \n",
            "2             0     2              2              0              2   \n",
            "3             0     3              2              2              2   \n",
            "4             0     4              2              2              2   \n",
            "\n",
            "   pain_survey_4 n_legs n_hands n_eyes  joint_00  ...      joint_21  \\\n",
            "0              1    two     two    two  1.094705  ...  3.499558e-06   \n",
            "1              2    two     two    two  1.135183  ...  3.976952e-07   \n",
            "2              2    two     two    two  1.080745  ...  1.533820e-07   \n",
            "3              2    two     two    two  0.938017  ...  1.006865e-05   \n",
            "4              2    two     two    two  1.090185  ...  4.437266e-06   \n",
            "\n",
            "       joint_22  joint_23      joint_24  joint_25  joint_26  joint_27  \\\n",
            "0  1.945042e-06  0.000004  1.153299e-05  0.000004  0.017592  0.013508   \n",
            "1  6.765107e-07  0.000006  4.643774e-08  0.000000  0.013352  0.000000   \n",
            "2  1.698525e-07  0.000001  2.424536e-06  0.000003  0.016225  0.008110   \n",
            "3  5.511079e-07  0.000002  5.432416e-08  0.000000  0.011832  0.007450   \n",
            "4  1.735459e-07  0.000002  5.825366e-08  0.000007  0.005360  0.002532   \n",
            "\n",
            "   joint_28  joint_29  joint_30  \n",
            "0  0.026798  0.027815       0.5  \n",
            "1  0.013377  0.013716       0.5  \n",
            "2  0.024097  0.023105       0.5  \n",
            "3  0.028613  0.024648       0.5  \n",
            "4  0.033026  0.025328       0.5  \n",
            "\n",
            "[5 rows x 40 columns]\n",
            "\n",
            "--- Dati di Test (X_test_raw) ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 211840 entries, 0 to 211839\n",
            "Data columns (total 40 columns):\n",
            " #   Column         Non-Null Count   Dtype  \n",
            "---  ------         --------------   -----  \n",
            " 0   sample_index   211840 non-null  int64  \n",
            " 1   time           211840 non-null  int64  \n",
            " 2   pain_survey_1  211840 non-null  int64  \n",
            " 3   pain_survey_2  211840 non-null  int64  \n",
            " 4   pain_survey_3  211840 non-null  int64  \n",
            " 5   pain_survey_4  211840 non-null  int64  \n",
            " 6   n_legs         211840 non-null  object \n",
            " 7   n_hands        211840 non-null  object \n",
            " 8   n_eyes         211840 non-null  object \n",
            " 9   joint_00       211840 non-null  float64\n",
            " 10  joint_01       211840 non-null  float64\n",
            " 11  joint_02       211840 non-null  float64\n",
            " 12  joint_03       211840 non-null  float64\n",
            " 13  joint_04       211840 non-null  float64\n",
            " 14  joint_05       211840 non-null  float64\n",
            " 15  joint_06       211840 non-null  float64\n",
            " 16  joint_07       211840 non-null  float64\n",
            " 17  joint_08       211840 non-null  float64\n",
            " 18  joint_09       211840 non-null  float64\n",
            " 19  joint_10       211840 non-null  float64\n",
            " 20  joint_11       211840 non-null  float64\n",
            " 21  joint_12       211840 non-null  float64\n",
            " 22  joint_13       211840 non-null  float64\n",
            " 23  joint_14       211840 non-null  float64\n",
            " 24  joint_15       211840 non-null  float64\n",
            " 25  joint_16       211840 non-null  float64\n",
            " 26  joint_17       211840 non-null  float64\n",
            " 27  joint_18       211840 non-null  float64\n",
            " 28  joint_19       211840 non-null  float64\n",
            " 29  joint_20       211840 non-null  float64\n",
            " 30  joint_21       211840 non-null  float64\n",
            " 31  joint_22       211840 non-null  float64\n",
            " 32  joint_23       211840 non-null  float64\n",
            " 33  joint_24       211840 non-null  float64\n",
            " 34  joint_25       211840 non-null  float64\n",
            " 35  joint_26       211840 non-null  float64\n",
            " 36  joint_27       211840 non-null  float64\n",
            " 37  joint_28       211840 non-null  float64\n",
            " 38  joint_29       211840 non-null  float64\n",
            " 39  joint_30       211840 non-null  float64\n",
            "dtypes: float64(31), int64(6), object(3)\n",
            "memory usage: 64.6+ MB\n",
            "\n",
            "--- Etichette (y_train_raw) ---\n",
            "   sample_index     label\n",
            "0             0   no_pain\n",
            "1             1   no_pain\n",
            "2             2  low_pain\n",
            "3             3   no_pain\n",
            "4             4   no_pain\n",
            "\n",
            "Classi uniche: ['no_pain' 'low_pain' 'high_pain']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler # Useremo uno di questi\n",
        "from sklearn.metrics import f1_score # La nostra metrica di gara!\n",
        "\n",
        "# --- Carichiamo i dati ---\n",
        "\n",
        "# Assicurati che i file siano nella stessa cartella del notebook\n",
        "# o aggiorna il percorso.\n",
        "try:\n",
        "    X_train_raw = pd.read_csv('/content/pirate_pain_train.csv')\n",
        "    y_train_raw = pd.read_csv('/content/pirate_pain_train_labels.csv')\n",
        "    X_test_raw = pd.read_csv('/content/pirate_pain_test.csv')\n",
        "\n",
        "    print(\"--- Dati di Training (X_train_raw) ---\")\n",
        "    X_train_raw.info()\n",
        "\n",
        "    print(\"\\n--- Esempio di dati di Training ---\")\n",
        "    print(X_train_raw.head())\n",
        "\n",
        "    print(\"\\n--- Dati di Test (X_test_raw) ---\")\n",
        "    X_test_raw.info()\n",
        "\n",
        "    print(\"\\n--- Etichette (y_train_raw) ---\")\n",
        "    print(y_train_raw.head())\n",
        "    print(f\"\\nClassi uniche: {y_train_raw['label'].unique()}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Errore: File .csv non trovati.\")\n",
        "    print(\"Assicurati che 'pirate_pain_train.csv', 'pirate_pain_train_labels.csv' e 'pirate_pain_test.csv' siano nella cartella corretta.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Verifica Integrità Dati ---\n",
        "\n",
        "# 1. Controlliamo se il numero di campioni in X e y corrisponde\n",
        "num_samples_x = X_train_raw['sample_index'].nunique()\n",
        "num_samples_y = len(y_train_raw)\n",
        "\n",
        "print(f\"Campioni unici in X_train_raw: {num_samples_x}\")\n",
        "print(f\"Campioni totali in y_train_raw: {num_samples_y}\")\n",
        "if num_samples_x == num_samples_y:\n",
        "    print(\"✅ OK: Corrispondenza campioni X e y.\")\n",
        "else:\n",
        "    print(\"⚠️ ATTENZIONE: Il numero di campioni in X e y NON corrisponde.\")\n",
        "\n",
        "# 2. Controlliamo la lunghezza delle sequenze\n",
        "sequence_lengths = X_train_raw.groupby('sample_index').size()\n",
        "print(\"\\n--- Statistiche Lunghezza Sequenze ---\")\n",
        "print(sequence_lengths.describe())\n",
        "\n",
        "# 3. Controlliamo se TUTTE le sequenze sono lunghe 180\n",
        "if (sequence_lengths == 180).all():\n",
        "    print(\"\\n✅ OK: Tutte le sequenze hanno esattamente 180 time step.\")\n",
        "else:\n",
        "    print(\"\\n⚠️ ATTENZIONE: NON tutte le sequenze hanno 180 time step.\")\n",
        "    # Stampiamo quelle che non lo sono, se sono poche\n",
        "    if len(sequence_lengths[sequence_lengths != 180]) < 20:\n",
        "        print(f\"Sequenze con lunghezza diversa da 180:\\n{sequence_lengths[sequence_lengths != 180]}\")\n",
        "    else:\n",
        "        print(\"Molte sequenze hanno una lunghezza diversa da 180.\")\n",
        "\n",
        "\n",
        "# 4. Controlliamo i valori unici delle features statiche (categoriche)\n",
        "print(\"\\n--- Valori Unici Features Statiche ---\")\n",
        "print(f\"n_legs: {X_train_raw['n_legs'].unique()}\")\n",
        "print(f\"n_hands: {X_train_raw['n_hands'].unique()}\")\n",
        "print(f\"n_eyes: {X_train_raw['n_eyes'].unique()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp-c3aLscjLi",
        "outputId": "b28728ac-f887-4e3b-c046-7b4147ece7cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Campioni unici in X_train_raw: 661\n",
            "Campioni totali in y_train_raw: 661\n",
            "✅ OK: Corrispondenza campioni X e y.\n",
            "\n",
            "--- Statistiche Lunghezza Sequenze ---\n",
            "count    661.0\n",
            "mean     160.0\n",
            "std        0.0\n",
            "min      160.0\n",
            "25%      160.0\n",
            "50%      160.0\n",
            "75%      160.0\n",
            "max      160.0\n",
            "dtype: float64\n",
            "\n",
            "⚠️ ATTENZIONE: NON tutte le sequenze hanno 180 time step.\n",
            "Molte sequenze hanno una lunghezza diversa da 180.\n",
            "\n",
            "--- Valori Unici Features Statiche ---\n",
            "n_legs: ['two' 'one+peg_leg']\n",
            "n_hands: ['two' 'one+hook_hand']\n",
            "n_eyes: ['two' 'one+eye_patch']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "from pandas.errors import SettingWithCopyWarning\n",
        "\n",
        "# --- Setup Iniziale ---\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# --- 1. Caricamento Dati ---\n",
        "try:\n",
        "    X_train_raw = pd.read_csv('/content/pirate_pain_train.csv')\n",
        "    y_train_raw = pd.read_csv('/content/pirate_pain_train_labels.csv')\n",
        "    X_test_raw = pd.read_csv('/content/pirate_pain_test.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Errore: File .csv non trovati.\")\n",
        "    # Stop execution or handle error\n",
        "\n",
        "# --- 2. Definizione Features e Label Encoding ---\n",
        "\n",
        "# Liste delle colonne per tipo\n",
        "TEMPORAL_FEATURES = [\n",
        "    'pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4',\n",
        "    'joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05',\n",
        "    'joint_06', 'joint_07', 'joint_08', 'joint_09', 'joint_10', 'joint_11',\n",
        "    'joint_12', 'joint_13', 'joint_14', 'joint_15', 'joint_16', 'joint_17',\n",
        "    'joint_18', 'joint_19', 'joint_20', 'joint_21', 'joint_22', 'joint_23',\n",
        "    'joint_24', 'joint_25', 'joint_26', 'joint_27', 'joint_28', 'joint_29',\n",
        "    'joint_30'\n",
        "] # Totale: 35 features\n",
        "\n",
        "STATIC_FEATURES = ['n_legs', 'n_hands', 'n_eyes'] # Totale: 3 features\n",
        "\n",
        "# Lunghezza delle sequenze (dalla nostra analisi)\n",
        "SEQUENCE_LENGTH = 160\n",
        "\n",
        "# --- Label Encoding ---\n",
        "label_map = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
        "y_train = y_train_raw['label'].map(label_map).values\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "print(f\"Features temporali identificate: {len(TEMPORAL_FEATURES)}\")\n",
        "print(f\"Features statiche identificate: {len(STATIC_FEATURES)}\")\n",
        "print(f\"Shape delle etichette (y_train_tensor): {y_train_tensor.shape}\")\n",
        "print(f\"Prime 5 etichette: {y_train_tensor[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO7dFUo7cmoG",
        "outputId": "2607e81f-54fb-4194-dd0c-07a65458b8ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Features temporali identificate: 35\n",
            "Features statiche identificate: 3\n",
            "Shape delle etichette (y_train_tensor): torch.Size([661])\n",
            "Prime 5 etichette: tensor([0, 0, 1, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignoriamo i warning di pandas durante lo scaling\n",
        "warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n",
        "\n",
        "# --- 2. Scaler e Encoder (FONDAMENTALE: Fittati solo su X_train_raw) ---\n",
        "\n",
        "# 2a. One-Hot Encoder per le features statiche\n",
        "# Prendiamo i valori unici da X_train_raw per fittare l'encoder\n",
        "static_data_train = X_train_raw[STATIC_FEATURES].drop_duplicates()\n",
        "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "ohe.fit(static_data_train)\n",
        "\n",
        "# 2b. Scaler per le features temporali\n",
        "# Fittiamo lo scaler SOLO sui dati di training temporali\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train_raw[TEMPORAL_FEATURES])\n",
        "\n",
        "\n",
        "# --- 3. Funzione di creazione sequenze ---\n",
        "\n",
        "def create_sequences(x_df, y_series_or_none, seq_len, fitted_ohe, fitted_scaler):\n",
        "    \"\"\"\n",
        "    Trasforma il dataframe \"long\" in un tensore 3D [campioni, timesteps, features]\n",
        "    e applica OHE e Scaling.\n",
        "    \"\"\"\n",
        "\n",
        "    # Lista per i dati 3D finali\n",
        "    X_processed = []\n",
        "\n",
        "    # ID unici (661 per il train, 1177 per il test)\n",
        "    sample_indices = x_df['sample_index'].unique()\n",
        "\n",
        "    # Raggruppa per 'sample_index'\n",
        "    # Questo è più robusto di un reshape se i dati non fossero ordinati\n",
        "    grouped = x_df.groupby('sample_index')\n",
        "\n",
        "    for sample_idx in sample_indices:\n",
        "        group = grouped.get_group(sample_idx)\n",
        "\n",
        "        # --- 3a. Processa features temporali ---\n",
        "        # Ordina per tempo (sicurezza) e prendi le features\n",
        "        temporal_data = group.sort_values('time')[TEMPORAL_FEATURES]\n",
        "        # Applica lo scaler fittato\n",
        "        temporal_data_scaled = fitted_scaler.transform(temporal_data)\n",
        "\n",
        "        # --- 3b. Processa features statiche ---\n",
        "        # Prendi solo la prima riga (sono tutte uguali)\n",
        "        static_data = group[STATIC_FEATURES].iloc[0:1]\n",
        "        # Applica l'OHE fittato\n",
        "        static_data_encoded = fitted_ohe.transform(static_data) # Shape (1, 6)\n",
        "\n",
        "        # --- 3c. Combina ---\n",
        "        # Ripeti (\"tile\") le features statiche per ogni time step\n",
        "        # (1, 6) -> (160, 6)\n",
        "        static_data_tiled = np.tile(static_data_encoded, (seq_len, 1))\n",
        "\n",
        "        # Concatena temporali (160, 35) e statiche (160, 6)\n",
        "        # Risultato -> (160, 41)\n",
        "        combined_features = np.concatenate([temporal_data_scaled, static_data_tiled], axis=1)\n",
        "\n",
        "        X_processed.append(combined_features)\n",
        "\n",
        "    # Converte la lista di array (661) in un unico array 3D\n",
        "    X_tensor = torch.tensor(np.array(X_processed), dtype=torch.float32)\n",
        "\n",
        "    # Se stiamo processando il test set, non abbiamo y\n",
        "    if y_series_or_none is None:\n",
        "        return X_tensor\n",
        "\n",
        "    # Altrimenti, processiamo anche y (già fatto fuori, ma per coerenza)\n",
        "    y_tensor = torch.tensor(y_series_or_none, dtype=torch.long)\n",
        "    return X_tensor, y_tensor\n",
        "\n",
        "\n",
        "# --- 4. Applica la funzione ---\n",
        "\n",
        "print(\"Inizio preprocessing X_train...\")\n",
        "# Processiamo i dati di training (passando le etichette già encodate)\n",
        "X_train_tensor, y_train_tensor = create_sequences(X_train_raw, y_train, SEQUENCE_LENGTH, ohe, scaler)\n",
        "print(f\"Shape X_train_tensor: {X_train_tensor.shape}\") # Atteso: (661, 160, 41)\n",
        "print(f\"Shape y_train_tensor: {y_train_tensor.shape}\") # Atteso: (661)\n",
        "\n",
        "print(\"\\nInizio preprocessing X_test...\")\n",
        "# Processiamo i dati di test (passando None per le y)\n",
        "X_test_tensor = create_sequences(X_test_raw, None, SEQUENCE_LENGTH, ohe, scaler)\n",
        "print(f\"Shape X_test_tensor: {X_test_tensor.shape}\") # Atteso: (1177, 160, 41)\n",
        "\n",
        "# Salviamo il numero totale di features per il modello\n",
        "total_features = X_train_tensor.shape[2]\n",
        "print(f\"\\nNumero totale di features (temporali + statiche OHE): {total_features}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQbJNHAlcoVP",
        "outputId": "5f8f7386-d982-4463-85f3-dadc4ea60583"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inizio preprocessing X_train...\n",
            "Shape X_train_tensor: torch.Size([661, 160, 41])\n",
            "Shape y_train_tensor: torch.Size([661])\n",
            "\n",
            "Inizio preprocessing X_test...\n",
            "Shape X_test_tensor: torch.Size([1324, 160, 41])\n",
            "\n",
            "Numero totale di features (temporali + statiche OHE): 41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Creazione Validation Split ---\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    X_train_tensor,\n",
        "    y_train_tensor,\n",
        "    test_size=0.2, # 20% per la validazione\n",
        "    random_state=SEED,\n",
        "    stratify=y_train_tensor # FONDAMENTALE!\n",
        ")\n",
        "\n",
        "print(f\"Shape X_train_split: {X_train_split.shape}\")\n",
        "print(f\"Shape X_val_split:   {X_val_split.shape}\")\n",
        "\n",
        "# --- 6. Creazione DataLoaders (con FIX per CPU) ---\n",
        "\n",
        "def make_loader(ds, batch_size, shuffle, drop_last):\n",
        "\n",
        "    # *** FIX: Abilita pin_memory solo se siamo su CUDA ***\n",
        "    pin_memory_enabled = (device.type == 'cuda')\n",
        "\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=2,\n",
        "        pin_memory=pin_memory_enabled, # <-- FIX\n",
        "        pin_memory_device=str(device) if pin_memory_enabled else \"\", # <-- FIX\n",
        "        prefetch_factor=4 if pin_memory_enabled else 2,\n",
        "    )\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_ds = TensorDataset(X_train_split, y_train_split)\n",
        "val_ds = TensorDataset(X_val_split, y_val_split)\n",
        "\n",
        "train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "val_loader = make_loader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "print(\"\\nDataLoaders creati.\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKUHe2ZXcqP9",
        "outputId": "99c6bb11-831f-4f46-fb7d-89c2833179ef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape X_train_split: torch.Size([528, 160, 41])\n",
            "Shape X_val_split:   torch.Size([133, 160, 41])\n",
            "\n",
            "DataLoaders creati.\n",
            "Batch size: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import os\n",
        "import copy\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# --- 1. Definizione del Modello (da Lab 4) ---\n",
        "# Adattato per la tua task\n",
        "\n",
        "class RecurrentClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Classifier RNN generico (RNN, LSTM, GRU).\n",
        "    Usa l'ultimo hidden state per la classificazione (Many-to-One).\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size,       # Nostro: 41\n",
        "            hidden_size,      # Es. 128\n",
        "            num_layers,       # Es. 2\n",
        "            num_classes,      # Nostro: 3\n",
        "            rnn_type='GRU',   # 'RNN', 'LSTM', o 'GRU'\n",
        "            bidirectional=False,\n",
        "            dropout_rate=0.0\n",
        "            ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        rnn_map = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}\n",
        "        if rnn_type not in rnn_map:\n",
        "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
        "\n",
        "        rnn_module = rnn_map[rnn_type]\n",
        "\n",
        "        # Dropout solo tra i layer (se num_layers > 1)\n",
        "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
        "\n",
        "        self.rnn = rnn_module(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,       # Input shape: (batch, seq_len, features)\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout_val\n",
        "        )\n",
        "\n",
        "        # Calcola l'input per il classificatore\n",
        "        if self.bidirectional:\n",
        "            classifier_input_size = hidden_size * 2 # Concat fwd + bwd\n",
        "        else:\n",
        "            classifier_input_size = hidden_size\n",
        "\n",
        "        # Layer di classificazione finale\n",
        "        # Aggiungiamo il Dropout qui, come da Lab 2\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(classifier_input_size, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_length, input_size)\n",
        "\n",
        "        # rnn_out shape: (batch_size, seq_len, hidden_size * num_directions)\n",
        "        # hidden shape: (num_layers * num_directions, batch_size, hidden_size)\n",
        "        rnn_out, hidden = self.rnn(x)\n",
        "\n",
        "        # LSTM ritorna (h_n, c_n), a noi serve solo h_n\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            hidden = hidden[0]\n",
        "\n",
        "        # Estrai l'ultimo hidden state dall'ultimo layer\n",
        "        if self.bidirectional:\n",
        "            # Reshape a (num_layers, 2, batch_size, hidden_size)\n",
        "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
        "            # Concatena l'ultimo fwd (hidden[-1, 0, ...]) e bwd (hidden[-1, 1, ...])\n",
        "            # Shape finale: (batch_size, hidden_size * 2)\n",
        "            hidden_to_classify = torch.cat([hidden[-1, 0, :, :], hidden[-1, 1, :, :]], dim=1)\n",
        "        else:\n",
        "            # Prendi l'hidden state dell'ultimo layer\n",
        "            # Shape finale: (batch_size, hidden_size)\n",
        "            hidden_to_classify = hidden[-1]\n",
        "\n",
        "        # Ottieni i logits\n",
        "        logits = self.classifier(hidden_to_classify)\n",
        "        return logits\n",
        "\n",
        "# --- 2. Funzioni di Training e Validazione (da Lab 2) ---\n",
        "\n",
        "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "            # Aggiungiamo L1/L2 (come da Lab 2)\n",
        "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
        "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        predictions = logits.argmax(dim=1)\n",
        "        all_predictions.append(predictions.cpu().numpy())\n",
        "        all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_f1 = f1_score(\n",
        "        np.concatenate(all_targets),\n",
        "        np.concatenate(all_predictions),\n",
        "        average='weighted'\n",
        "    )\n",
        "    return epoch_loss, epoch_f1\n",
        "\n",
        "def validate_one_epoch(model, val_loader, criterion, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, targets)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            predictions = logits.argmax(dim=1)\n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "            all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(val_loader.dataset)\n",
        "    epoch_f1 = f1_score(\n",
        "        np.concatenate(all_targets),\n",
        "        np.concatenate(all_predictions),\n",
        "        average='weighted'\n",
        "    )\n",
        "    return epoch_loss, epoch_f1\n",
        "\n",
        "def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
        "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
        "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
        "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
        "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
        "    # Puoi aggiungere l'histogramma dei pesi se vuoi (come nel Lab 2)\n",
        "\n",
        "# --- 3. Funzione FIT (da Lab 2) ---\n",
        "\n",
        "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
        "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
        "        restore_best_weights=True, writer=None, verbose=1, experiment_name=\"\"):\n",
        "\n",
        "    training_history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_f1': [], 'val_f1': []\n",
        "    }\n",
        "\n",
        "    # Inizializziamo best_metric qui\n",
        "    best_metric = float('-inf') if mode == 'max' else float('inf')\n",
        "\n",
        "    if patience > 0:\n",
        "        patience_counter = 0\n",
        "        best_epoch = 0\n",
        "        # Assicurati che la cartella 'models' esista\n",
        "        os.makedirs(\"models\", exist_ok=True)\n",
        "        model_path = f\"models/{experiment_name}_best_model.pt\"\n",
        "\n",
        "    print(f\"Inizio training per {epochs} epochs...\")\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss, train_f1 = train_one_epoch(\n",
        "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
        "        )\n",
        "        val_loss, val_f1 = validate_one_epoch(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "\n",
        "        training_history['train_loss'].append(train_loss)\n",
        "        training_history['val_loss'].append(val_loss)\n",
        "        training_history['train_f1'].append(train_f1)\n",
        "        training_history['val_f1'].append(val_f1)\n",
        "\n",
        "        if writer is not None:\n",
        "            log_metrics_to_tensorboard(\n",
        "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
        "            )\n",
        "\n",
        "        if epoch % verbose == 0 or epoch == 1:\n",
        "            print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
        "                  f\"Train: Loss={train_loss:.4f}, F1={train_f1:.4f} | \"\n",
        "                  f\"Val: Loss={val_loss:.4f}, F1={val_f1:.4f}\")\n",
        "\n",
        "        if patience > 0:\n",
        "            current_metric = training_history[evaluation_metric][-1]\n",
        "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
        "\n",
        "            if is_improvement:\n",
        "                best_metric = current_metric\n",
        "                best_epoch = epoch\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping triggerato dopo {epoch} epochs.\")\n",
        "                    break\n",
        "\n",
        "    if restore_best_weights and patience > 0:\n",
        "        print(f\"Carico il modello migliore dall'epoch {best_epoch} con {evaluation_metric} {best_metric:.4f}\")\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "\n",
        "    # *** FIX: Restituisci anche best_metric ***\n",
        "    return model, training_history, best_metric\n",
        "\n",
        "print(\"Definizioni del modello e delle funzioni di training caricate (FIX 2).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCUhLOd_cr9m",
        "outputId": "0bd71500-f80b-4bbb-cf6d-f7460d8a6691"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Definizioni del modello e delle funzioni di training caricate (FIX 2).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Configurazione Esperimento Baseline ---\n",
        "\n",
        "# Hyperparameters\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 200\n",
        "PATIENCE = 20\n",
        "L2_LAMBDA = 1e-4\n",
        "DROPOUT_RATE = 0.3\n",
        "HIDDEN_SIZE = 128\n",
        "NUM_LAYERS = 2\n",
        "RNN_TYPE = 'GRU'\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# --- Inizializza Modello, Loss, Optimizer ---\n",
        "print(\"Inizializzo il modello baseline...\")\n",
        "\n",
        "baseline_model = RecurrentClassifier(\n",
        "    input_size=total_features, # 41\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    num_classes=3,             # no_pain, low_pain, high_pain\n",
        "    rnn_type=RNN_TYPE,\n",
        "    bidirectional=BIDIRECTIONAL,\n",
        "    dropout_rate=DROPOUT_RATE\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(baseline_model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
        "\n",
        "scaler = torch.amp.GradScaler(device.type, enabled=(device.type == 'cuda'))\n",
        "\n",
        "# Setup TensorBoard\n",
        "experiment_name = f\"{RNN_TYPE}_bidir={BIDIRECTIONAL}_hs={HIDDEN_SIZE}_nl={NUM_LAYERS}_dr={DROPOUT_RATE}\"\n",
        "writer = SummaryWriter(f\"runs/{experiment_name}\")\n",
        "\n",
        "# --- 6. Avvia Training ---\n",
        "print(f\"Avvio training per: {experiment_name}\")\n",
        "\n",
        "# *** FIX: Cattura best_metric qui ***\n",
        "baseline_model, history, best_metric = fit(\n",
        "    model=baseline_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    epochs=EPOCHS,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scaler=scaler,\n",
        "    device=device,\n",
        "    patience=PATIENCE,\n",
        "    evaluation_metric=\"val_f1\",\n",
        "    mode='max',\n",
        "    restore_best_weights=True,\n",
        "    writer=writer,\n",
        "    verbose=1,\n",
        "    experiment_name=experiment_name\n",
        ")\n",
        "\n",
        "print(\"Training completato.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoRvaTV9ctc-",
        "outputId": "61455845-2347-4ed1-d95f-6a8f1b6a852c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inizializzo il modello baseline...\n",
            "Avvio training per: GRU_bidir=True_hs=128_nl=2_dr=0.3\n",
            "Inizio training per 200 epochs...\n",
            "Epoch   1/200 | Train: Loss=0.8021, F1=0.6471 | Val: Loss=0.7028, F1=0.6760\n",
            "Epoch   2/200 | Train: Loss=0.6625, F1=0.6720 | Val: Loss=0.6898, F1=0.6760\n",
            "Epoch   3/200 | Train: Loss=0.6624, F1=0.6773 | Val: Loss=0.6732, F1=0.6760\n",
            "Epoch   4/200 | Train: Loss=0.6383, F1=0.6826 | Val: Loss=0.6602, F1=0.6760\n",
            "Epoch   5/200 | Train: Loss=0.6361, F1=0.6746 | Val: Loss=0.6519, F1=0.6760\n",
            "Epoch   6/200 | Train: Loss=0.6258, F1=0.6720 | Val: Loss=0.6431, F1=0.6760\n",
            "Epoch   7/200 | Train: Loss=0.6052, F1=0.6800 | Val: Loss=0.6265, F1=0.6954\n",
            "Epoch   8/200 | Train: Loss=0.6037, F1=0.7088 | Val: Loss=0.7109, F1=0.7331\n",
            "Epoch   9/200 | Train: Loss=0.6166, F1=0.7150 | Val: Loss=0.6211, F1=0.6954\n",
            "Epoch  10/200 | Train: Loss=0.5960, F1=0.7056 | Val: Loss=0.6188, F1=0.7107\n",
            "Epoch  11/200 | Train: Loss=0.5845, F1=0.7280 | Val: Loss=0.6055, F1=0.7249\n",
            "Epoch  12/200 | Train: Loss=0.5599, F1=0.7335 | Val: Loss=0.6008, F1=0.7228\n",
            "Epoch  13/200 | Train: Loss=0.5635, F1=0.7729 | Val: Loss=0.6300, F1=0.7249\n",
            "Epoch  14/200 | Train: Loss=0.5497, F1=0.7442 | Val: Loss=0.6081, F1=0.7380\n",
            "Epoch  15/200 | Train: Loss=0.5426, F1=0.7651 | Val: Loss=0.6028, F1=0.7270\n",
            "Epoch  16/200 | Train: Loss=0.5173, F1=0.7738 | Val: Loss=0.6077, F1=0.7417\n",
            "Epoch  17/200 | Train: Loss=0.5136, F1=0.7788 | Val: Loss=0.6186, F1=0.7407\n",
            "Epoch  18/200 | Train: Loss=0.5102, F1=0.7969 | Val: Loss=0.6223, F1=0.7380\n",
            "Epoch  19/200 | Train: Loss=0.5129, F1=0.7901 | Val: Loss=0.5825, F1=0.7384\n",
            "Epoch  20/200 | Train: Loss=0.5085, F1=0.7759 | Val: Loss=0.6410, F1=0.7161\n",
            "Epoch  21/200 | Train: Loss=0.5140, F1=0.7859 | Val: Loss=0.5777, F1=0.7550\n",
            "Epoch  22/200 | Train: Loss=0.4742, F1=0.7940 | Val: Loss=0.5694, F1=0.7459\n",
            "Epoch  23/200 | Train: Loss=0.4473, F1=0.8005 | Val: Loss=0.5946, F1=0.7577\n",
            "Epoch  24/200 | Train: Loss=0.4514, F1=0.8009 | Val: Loss=0.5834, F1=0.7447\n",
            "Epoch  25/200 | Train: Loss=0.4001, F1=0.8200 | Val: Loss=0.5313, F1=0.7858\n",
            "Epoch  26/200 | Train: Loss=0.4015, F1=0.8426 | Val: Loss=0.5785, F1=0.7869\n",
            "Epoch  27/200 | Train: Loss=0.4382, F1=0.7868 | Val: Loss=0.5655, F1=0.7763\n",
            "Epoch  28/200 | Train: Loss=0.3937, F1=0.8302 | Val: Loss=0.5629, F1=0.7925\n",
            "Epoch  29/200 | Train: Loss=0.4171, F1=0.8316 | Val: Loss=0.6396, F1=0.7700\n",
            "Epoch  30/200 | Train: Loss=0.4427, F1=0.8168 | Val: Loss=0.5409, F1=0.8297\n",
            "Epoch  31/200 | Train: Loss=0.3844, F1=0.8489 | Val: Loss=0.5029, F1=0.8061\n",
            "Epoch  32/200 | Train: Loss=0.3295, F1=0.8739 | Val: Loss=0.5542, F1=0.7992\n",
            "Epoch  33/200 | Train: Loss=0.3239, F1=0.8654 | Val: Loss=0.5403, F1=0.7904\n",
            "Epoch  34/200 | Train: Loss=0.3070, F1=0.8840 | Val: Loss=0.5825, F1=0.7946\n",
            "Epoch  35/200 | Train: Loss=0.3173, F1=0.8786 | Val: Loss=0.5483, F1=0.7994\n",
            "Epoch  36/200 | Train: Loss=0.3037, F1=0.8858 | Val: Loss=0.5064, F1=0.7958\n",
            "Epoch  37/200 | Train: Loss=0.3405, F1=0.8797 | Val: Loss=0.5704, F1=0.8135\n",
            "Epoch  38/200 | Train: Loss=0.2886, F1=0.8961 | Val: Loss=0.4718, F1=0.8239\n",
            "Epoch  39/200 | Train: Loss=0.2837, F1=0.8935 | Val: Loss=0.5882, F1=0.7720\n",
            "Epoch  40/200 | Train: Loss=0.2781, F1=0.8908 | Val: Loss=0.5695, F1=0.8187\n",
            "Epoch  41/200 | Train: Loss=0.2946, F1=0.8885 | Val: Loss=0.5995, F1=0.8094\n",
            "Epoch  42/200 | Train: Loss=0.2912, F1=0.9040 | Val: Loss=0.4699, F1=0.8343\n",
            "Epoch  43/200 | Train: Loss=0.2555, F1=0.9036 | Val: Loss=0.5236, F1=0.7991\n",
            "Epoch  44/200 | Train: Loss=0.2863, F1=0.8905 | Val: Loss=0.5159, F1=0.8018\n",
            "Epoch  45/200 | Train: Loss=0.2504, F1=0.9187 | Val: Loss=0.5727, F1=0.7871\n",
            "Epoch  46/200 | Train: Loss=0.2289, F1=0.9250 | Val: Loss=0.4389, F1=0.8289\n",
            "Epoch  47/200 | Train: Loss=0.2366, F1=0.9182 | Val: Loss=0.6292, F1=0.8061\n",
            "Epoch  48/200 | Train: Loss=0.2051, F1=0.9207 | Val: Loss=0.4794, F1=0.8363\n",
            "Epoch  49/200 | Train: Loss=0.2161, F1=0.9257 | Val: Loss=0.5317, F1=0.8185\n",
            "Epoch  50/200 | Train: Loss=0.2181, F1=0.9205 | Val: Loss=0.5415, F1=0.8204\n",
            "Epoch  51/200 | Train: Loss=0.2158, F1=0.9227 | Val: Loss=0.4799, F1=0.8402\n",
            "Epoch  52/200 | Train: Loss=0.2094, F1=0.9292 | Val: Loss=0.5489, F1=0.8087\n",
            "Epoch  53/200 | Train: Loss=0.2057, F1=0.9351 | Val: Loss=0.5707, F1=0.8304\n",
            "Epoch  54/200 | Train: Loss=0.2399, F1=0.9121 | Val: Loss=0.6152, F1=0.7964\n",
            "Epoch  55/200 | Train: Loss=0.2139, F1=0.9140 | Val: Loss=0.5018, F1=0.8257\n",
            "Epoch  56/200 | Train: Loss=0.2518, F1=0.8916 | Val: Loss=0.4829, F1=0.8274\n",
            "Epoch  57/200 | Train: Loss=0.2186, F1=0.9242 | Val: Loss=0.5323, F1=0.8163\n",
            "Epoch  58/200 | Train: Loss=0.1997, F1=0.9243 | Val: Loss=0.5046, F1=0.8574\n",
            "Epoch  59/200 | Train: Loss=0.1956, F1=0.9294 | Val: Loss=0.5029, F1=0.8354\n",
            "Epoch  60/200 | Train: Loss=0.1834, F1=0.9374 | Val: Loss=0.5376, F1=0.8434\n",
            "Epoch  61/200 | Train: Loss=0.1539, F1=0.9533 | Val: Loss=0.5605, F1=0.8372\n",
            "Epoch  62/200 | Train: Loss=0.1502, F1=0.9600 | Val: Loss=0.5120, F1=0.8581\n",
            "Epoch  63/200 | Train: Loss=0.1583, F1=0.9475 | Val: Loss=0.5882, F1=0.8547\n",
            "Epoch  64/200 | Train: Loss=0.1767, F1=0.9318 | Val: Loss=0.5597, F1=0.8036\n",
            "Epoch  65/200 | Train: Loss=0.2079, F1=0.9251 | Val: Loss=0.4839, F1=0.8493\n",
            "Epoch  66/200 | Train: Loss=0.1697, F1=0.9496 | Val: Loss=0.7324, F1=0.7764\n",
            "Epoch  67/200 | Train: Loss=0.2205, F1=0.9210 | Val: Loss=0.5125, F1=0.8328\n",
            "Epoch  68/200 | Train: Loss=0.1968, F1=0.9278 | Val: Loss=0.5137, F1=0.8092\n",
            "Epoch  69/200 | Train: Loss=0.1761, F1=0.9360 | Val: Loss=0.5254, F1=0.8375\n",
            "Epoch  70/200 | Train: Loss=0.1694, F1=0.9441 | Val: Loss=0.4812, F1=0.8457\n",
            "Epoch  71/200 | Train: Loss=0.1414, F1=0.9617 | Val: Loss=0.5955, F1=0.8445\n",
            "Epoch  72/200 | Train: Loss=0.1284, F1=0.9597 | Val: Loss=0.5643, F1=0.8667\n",
            "Epoch  73/200 | Train: Loss=0.1386, F1=0.9436 | Val: Loss=0.6924, F1=0.7923\n",
            "Epoch  74/200 | Train: Loss=0.1575, F1=0.9387 | Val: Loss=0.5037, F1=0.8724\n",
            "Epoch  75/200 | Train: Loss=0.1208, F1=0.9703 | Val: Loss=0.6296, F1=0.8422\n",
            "Epoch  76/200 | Train: Loss=0.1243, F1=0.9580 | Val: Loss=0.4870, F1=0.8561\n",
            "Epoch  77/200 | Train: Loss=0.1305, F1=0.9523 | Val: Loss=0.6246, F1=0.8418\n",
            "Epoch  78/200 | Train: Loss=0.1467, F1=0.9538 | Val: Loss=0.5414, F1=0.8509\n",
            "Epoch  79/200 | Train: Loss=0.1425, F1=0.9559 | Val: Loss=0.6672, F1=0.8081\n",
            "Epoch  80/200 | Train: Loss=0.3047, F1=0.9059 | Val: Loss=0.5465, F1=0.7937\n",
            "Epoch  81/200 | Train: Loss=0.2663, F1=0.8902 | Val: Loss=0.5614, F1=0.8217\n",
            "Epoch  82/200 | Train: Loss=0.1870, F1=0.9316 | Val: Loss=0.7168, F1=0.7653\n",
            "Epoch  83/200 | Train: Loss=0.1937, F1=0.9290 | Val: Loss=0.5152, F1=0.8359\n",
            "Epoch  84/200 | Train: Loss=0.1873, F1=0.9344 | Val: Loss=0.6695, F1=0.7940\n",
            "Epoch  85/200 | Train: Loss=0.2642, F1=0.9144 | Val: Loss=0.4550, F1=0.8204\n",
            "Epoch  86/200 | Train: Loss=0.2173, F1=0.9230 | Val: Loss=0.5682, F1=0.8074\n",
            "Epoch  87/200 | Train: Loss=0.2152, F1=0.9265 | Val: Loss=0.4839, F1=0.8392\n",
            "Epoch  88/200 | Train: Loss=0.1645, F1=0.9389 | Val: Loss=0.5524, F1=0.8319\n",
            "Epoch  89/200 | Train: Loss=0.1434, F1=0.9499 | Val: Loss=0.5001, F1=0.8335\n",
            "Epoch  90/200 | Train: Loss=0.1397, F1=0.9444 | Val: Loss=0.5333, F1=0.8346\n",
            "Epoch  91/200 | Train: Loss=0.1242, F1=0.9618 | Val: Loss=0.5703, F1=0.8315\n",
            "Epoch  92/200 | Train: Loss=0.1279, F1=0.9619 | Val: Loss=0.5135, F1=0.8466\n",
            "Epoch  93/200 | Train: Loss=0.1181, F1=0.9604 | Val: Loss=0.5116, F1=0.8571\n",
            "Epoch  94/200 | Train: Loss=0.1019, F1=0.9701 | Val: Loss=0.5409, F1=0.8466\n",
            "Early stopping triggerato dopo 94 epochs.\n",
            "Carico il modello migliore dall'epoch 74 con val_f1 0.8724\n",
            "Training completato.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
        "\n",
        "# --- 7. Valutazione sul Validation Set ---\n",
        "print(\"Valutazione del modello migliore sul Validation Set...\")\n",
        "\n",
        "baseline_model.eval()  # Mettiamo il modello in modalità valutazione\n",
        "val_preds = []\n",
        "val_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in val_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        logits = baseline_model(inputs)\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        val_preds.append(preds.cpu().numpy())\n",
        "        val_targets.append(targets.cpu().numpy())\n",
        "\n",
        "val_preds = np.concatenate(val_preds)\n",
        "val_targets = np.concatenate(val_targets)\n",
        "\n",
        "# Calcola metriche\n",
        "val_f1 = f1_score(val_targets, val_preds, average='weighted')\n",
        "val_acc = accuracy_score(val_targets, val_preds)\n",
        "\n",
        "print(f\"\\n--- Metriche sul Validation Set ---\")\n",
        "print(f\"F1 Score (Weighted): {val_f1:.4f} (Il nostro best_metric era {best_metric:.4f})\")\n",
        "print(f\"Accuracy:              {val_acc:.4f}\")\n",
        "\n",
        "# --- Confusion Matrix ---\n",
        "cm = confusion_matrix(val_targets, val_preds)\n",
        "\n",
        "# Mappa inversa per le etichette (da 0 -> 'no_pain')\n",
        "inv_label_map = {v: k for k, v in label_map.items()}\n",
        "class_names = [inv_label_map[i] for i in range(len(inv_label_map))]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix (Validation Set)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "msyt8cXHfNUf",
        "outputId": "001c95ac-0c64-4e52-c8ad-25b850afe755"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valutazione del modello migliore sul Validation Set...\n",
            "\n",
            "--- Metriche sul Validation Set ---\n",
            "F1 Score (Weighted): 0.8724 (Il nostro best_metric era 0.8724)\n",
            "Accuracy:              0.8797\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8. Predizione sul Test Set e Creazione Submission ---\n",
        "\n",
        "print(\"Inizio predizione sul Test Set...\")\n",
        "\n",
        "# 1. Creiamo il Test Dataset e DataLoader\n",
        "# (X_test_tensor è stato creato nella cella 2)\n",
        "# Usiamo solo X_test_tensor, non ci sono etichette\n",
        "test_ds = TensorDataset(X_test_tensor)\n",
        "\n",
        "test_loader = make_loader(\n",
        "    test_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,  # MAI shuffle sul test set\n",
        "    drop_last=False # MAI droppare dati sul test set\n",
        ")\n",
        "\n",
        "# 2. Eseguiamo la predizione\n",
        "baseline_model.eval() # Modalità valutazione (disattiva dropout, ecc.)\n",
        "all_test_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for (inputs,) in test_loader: # Nota: (inputs,) perché non c'è la 'y'\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        logits = baseline_model(inputs)\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        all_test_preds.append(preds.cpu().numpy())\n",
        "\n",
        "# Concatena tutte le predizioni dei batch\n",
        "test_predictions_int = np.concatenate(all_test_preds)\n",
        "\n",
        "# 3. Riconverti i numeri (0, 1, 2) in etichette stringa\n",
        "# (inv_label_map è stato definito nella cella precedente)\n",
        "test_predictions_labels = [inv_label_map[pred] for pred in test_predictions_int]\n",
        "\n",
        "# 4. Crea il DataFrame per la submission\n",
        "# Dobbiamo prendere gli 'sample_index' unici da X_test_raw\n",
        "submission_df = pd.DataFrame({\n",
        "    'sample_index': X_test_raw['sample_index'].unique(),\n",
        "    'label': test_predictions_labels\n",
        "})\n",
        "\n",
        "# 5. Salva il file .csv\n",
        "submission_filename = 'submission_baseline.csv'\n",
        "submission_df.to_csv(submission_filename, index=False)\n",
        "\n",
        "print(f\"\\nFile di submission creato: {submission_filename}\")\n",
        "print(f\"Totale predizioni: {len(submission_df)}\")\n",
        "print(\"\\nPrime 5 righe della submission:\")\n",
        "print(submission_df.head())"
      ],
      "metadata": {
        "id": "arRl25VVgFiy",
        "outputId": "992692b8-b726-4a49-f857-098a921f7828",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inizio predizione sul Test Set...\n",
            "\n",
            "File di submission creato: submission_baseline.csv\n",
            "Totale predizioni: 1324\n",
            "\n",
            "Prime 5 righe della submission:\n",
            "   sample_index    label\n",
            "0             0  no_pain\n",
            "1             1  no_pain\n",
            "2             2  no_pain\n",
            "3             3  no_pain\n",
            "4             4  no_pain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 🎯 Fase 2: Ottimizzazione Iperparametri (Grid Search)\n",
        "\n",
        "Abbiamo una baseline di 0.891. Ora, come nel Lab 3, proviamo a migliorare il punteggio\n",
        "cercando una combinazione migliore di iperparametri.\n",
        "\n",
        "Useremo un \"Grid Search\" manuale per testare diverse configurazioni, allenandole\n",
        "sul `train_loader` e valutandole sul `val_loader` (gli stessi che abbiamo già creato)."
      ],
      "metadata": {
        "id": "CXVP2V6VPC0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# --- 1. Definizione della Griglia di Ricerca (Grid Search) ---\n",
        "\n",
        "# Definiamo gli iperparametri da testare\n",
        "param_grid = {\n",
        "    'HIDDEN_SIZE': [128, 256],      # Testiamo il nostro baseline (128) vs uno più grande (256)\n",
        "    'NUM_LAYERS': [2, 3],          # Testiamo 2 layer (baseline) vs 3\n",
        "    'DROPOUT_RATE': [0.3, 0.5],    # Testiamo il 30% (baseline) vs 50%\n",
        "    'LEARNING_RATE': [1e-3]        # Per ora teniamo fisso il learning rate per velocità\n",
        "}\n",
        "\n",
        "# Parametri fissi del nostro esperimento\n",
        "FIXED_PARAMS = {\n",
        "    'RNN_TYPE': 'GRU',\n",
        "    'BIDIRECTIONAL': True,\n",
        "    'L2_LAMBDA': 1e-4,\n",
        "    'EPOCHS': 200,\n",
        "    'PATIENCE': 20,\n",
        "    'CRITERION': nn.CrossEntropyLoss()\n",
        "}\n",
        "\n",
        "# Lista per salvare i risultati\n",
        "grid_search_results = []\n",
        "best_overall_f1 = 0.0\n",
        "best_overall_config = {}\n",
        "\n",
        "# Genera tutte le combinazioni\n",
        "all_combinations = list(itertools.product(\n",
        "    param_grid['HIDDEN_SIZE'],\n",
        "    param_grid['NUM_LAYERS'],\n",
        "    param_grid['DROPOUT_RATE'],\n",
        "    param_grid['LEARNING_RATE']\n",
        "))\n",
        "\n",
        "print(f\"Grid Search: {len(all_combinations)} combinazioni totali da testare.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fiF1u3ZPHmc",
        "outputId": "4828e490-c5d2-42bd-ecf5-a8a4a90f1239"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid Search: 8 combinazioni totali da testare.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# --- 2. Esecuzione del Grid Search (CORRETTA) ---\n",
        "\n",
        "for i, (hs, nl, dr, lr) in enumerate(all_combinations):\n",
        "\n",
        "    current_config = {\n",
        "        'HIDDEN_SIZE': hs,\n",
        "        'NUM_LAYERS': nl,\n",
        "        'DROPOUT_RATE': dr,\n",
        "        'LEARNING_RATE': lr\n",
        "    }\n",
        "\n",
        "    experiment_name = f\"{FIXED_PARAMS['RNN_TYPE']}_bidir={FIXED_PARAMS['BIDIRECTIONAL']}_hs={hs}_nl={nl}_dr={dr}_lr={lr}\"\n",
        "    print(f\"\\n--- Inizio Test {i+1}/{len(all_combinations)}: {experiment_name} ---\")\n",
        "\n",
        "    # 1. Inizializza Modello\n",
        "    model = RecurrentClassifier(\n",
        "        input_size=total_features,\n",
        "        hidden_size=hs,\n",
        "        num_layers=nl,\n",
        "        num_classes=3,\n",
        "        rnn_type=FIXED_PARAMS['RNN_TYPE'],\n",
        "        bidirectional=FIXED_PARAMS['BIDIRECTIONAL'],\n",
        "        dropout_rate=dr\n",
        "    ).to(device)\n",
        "\n",
        "    # 2. Inizializza Optimizer e Scaler\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=FIXED_PARAMS['L2_LAMBDA'])\n",
        "    scaler = torch.amp.GradScaler(device.type, enabled=(device.type == 'cuda'))\n",
        "\n",
        "    # 3. Inizializza Writer di TensorBoard\n",
        "    writer = SummaryWriter(f\"runs/{experiment_name}\")\n",
        "\n",
        "    # 4. Avvia il training (usando la nostra funzione 'fit')\n",
        "    model, history, best_f1_metric = fit(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        epochs=FIXED_PARAMS['EPOCHS'],\n",
        "        criterion=FIXED_PARAMS['CRITERION'],\n",
        "        optimizer=optimizer,\n",
        "        scaler=scaler,\n",
        "        device=device,\n",
        "        patience=FIXED_PARAMS['PATIENCE'],\n",
        "        evaluation_metric=\"val_f1\",\n",
        "        mode='max',\n",
        "        restore_best_weights=True,\n",
        "        writer=writer,\n",
        "        verbose=100, # <-- FIX: Cambiato da 0 a 100\n",
        "        experiment_name=experiment_name\n",
        "    )\n",
        "\n",
        "    print(f\"--- Risultato Test {i+1}: Val F1 = {best_f1_metric:.4f} ---\")\n",
        "\n",
        "    # Salva i risultati\n",
        "    result = current_config.copy()\n",
        "    result['val_f1'] = best_f1_metric\n",
        "    grid_search_results.append(result)\n",
        "\n",
        "    # Tieni traccia del migliore\n",
        "    if best_f1_metric > best_overall_f1:\n",
        "        best_overall_f1 = best_f1_metric\n",
        "        best_overall_config = current_config.copy()\n",
        "        print(f\"🎉 NUOVO MIGLIOR PUNTEGGIO TROVATO! 🎉\")\n",
        "\n",
        "print(\"\\n--- Grid Search Completato ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Hp49fvMPIM1",
        "outputId": "3cbbc6b9-c558-48e8-e055-302892f88d7e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Inizio Test 1/8: GRU_bidir=True_hs=128_nl=2_dr=0.3_lr=0.001 ---\n",
            "Inizio training per 200 epochs...\n",
            "Epoch   1/200 | Train: Loss=0.7988, F1=0.6378 | Val: Loss=0.6914, F1=0.6760\n",
            "Early stopping triggerato dopo 60 epochs.\n",
            "Carico il modello migliore dall'epoch 40 con val_f1 0.8446\n",
            "--- Risultato Test 1: Val F1 = 0.8446 ---\n",
            "🎉 NUOVO MIGLIOR PUNTEGGIO TROVATO! 🎉\n",
            "\n",
            "--- Inizio Test 2/8: GRU_bidir=True_hs=128_nl=2_dr=0.5_lr=0.001 ---\n",
            "Inizio training per 200 epochs...\n",
            "Epoch   1/200 | Train: Loss=0.7870, F1=0.6534 | Val: Loss=0.7053, F1=0.6760\n",
            "Epoch 100/200 | Train: Loss=0.1248, F1=0.9583 | Val: Loss=0.5357, F1=0.8244\n",
            "Early stopping triggerato dopo 104 epochs.\n",
            "Carico il modello migliore dall'epoch 84 con val_f1 0.8599\n",
            "--- Risultato Test 2: Val F1 = 0.8599 ---\n",
            "🎉 NUOVO MIGLIOR PUNTEGGIO TROVATO! 🎉\n",
            "\n",
            "--- Inizio Test 3/8: GRU_bidir=True_hs=128_nl=3_dr=0.3_lr=0.001 ---\n",
            "Inizio training per 200 epochs...\n",
            "Epoch   1/200 | Train: Loss=0.7585, F1=0.6474 | Val: Loss=0.6924, F1=0.6760\n",
            "Epoch 100/200 | Train: Loss=0.0396, F1=0.9863 | Val: Loss=0.5685, F1=0.8739\n",
            "Early stopping triggerato dopo 155 epochs.\n",
            "Carico il modello migliore dall'epoch 135 con val_f1 0.9062\n",
            "--- Risultato Test 3: Val F1 = 0.9062 ---\n",
            "🎉 NUOVO MIGLIOR PUNTEGGIO TROVATO! 🎉\n",
            "\n",
            "--- Inizio Test 4/8: GRU_bidir=True_hs=128_nl=3_dr=0.5_lr=0.001 ---\n",
            "Inizio training per 200 epochs...\n",
            "Epoch   1/200 | Train: Loss=0.7712, F1=0.6625 | Val: Loss=0.7033, F1=0.6760\n",
            "Epoch 100/200 | Train: Loss=0.0734, F1=0.9740 | Val: Loss=0.5457, F1=0.8724\n",
            "Early stopping triggerato dopo 100 epochs.\n",
            "Carico il modello migliore dall'epoch 80 con val_f1 0.8780\n",
            "--- Risultato Test 4: Val F1 = 0.8780 ---\n",
            "\n",
            "--- Inizio Test 5/8: GRU_bidir=True_hs=256_nl=2_dr=0.3_lr=0.001 ---\n",
            "Inizio training per 200 epochs...\n",
            "Epoch   1/200 | Train: Loss=0.7314, F1=0.6766 | Val: Loss=0.7069, F1=0.6760\n",
            "Early stopping triggerato dopo 89 epochs.\n",
            "Carico il modello migliore dall'epoch 69 con val_f1 0.8626\n",
            "--- Risultato Test 5: Val F1 = 0.8626 ---\n",
            "\n",
            "--- Inizio Test 6/8: GRU_bidir=True_hs=256_nl=2_dr=0.5_lr=0.001 ---\n",
            "Inizio training per 200 epochs...\n",
            "Epoch   1/200 | Train: Loss=0.7609, F1=0.6622 | Val: Loss=0.6988, F1=0.6760\n",
            "Early stopping triggerato dopo 97 epochs.\n",
            "Carico il modello migliore dall'epoch 77 con val_f1 0.8717\n",
            "--- Risultato Test 6: Val F1 = 0.8717 ---\n",
            "\n",
            "--- Inizio Test 7/8: GRU_bidir=True_hs=256_nl=3_dr=0.3_lr=0.001 ---\n",
            "Inizio training per 200 epochs...\n",
            "Epoch   1/200 | Train: Loss=0.7432, F1=0.6568 | Val: Loss=0.7119, F1=0.6760\n",
            "Early stopping triggerato dopo 47 epochs.\n",
            "Carico il modello migliore dall'epoch 27 con val_f1 0.8590\n",
            "--- Risultato Test 7: Val F1 = 0.8590 ---\n",
            "\n",
            "--- Inizio Test 8/8: GRU_bidir=True_hs=256_nl=3_dr=0.5_lr=0.001 ---\n",
            "Inizio training per 200 epochs...\n",
            "Epoch   1/200 | Train: Loss=0.7108, F1=0.6647 | Val: Loss=0.7403, F1=0.6760\n",
            "Early stopping triggerato dopo 76 epochs.\n",
            "Carico il modello migliore dall'epoch 56 con val_f1 0.8631\n",
            "--- Risultato Test 8: Val F1 = 0.8631 ---\n",
            "\n",
            "--- Grid Search Completato ---\n",
            "CPU times: user 2min 44s, sys: 1min 17s, total: 4min 2s\n",
            "Wall time: 5min 28s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Visualizzazione Risultati Grid Search ---\n",
        "\n",
        "print(\"--- Risultati del Grid Search ---\")\n",
        "\n",
        "# Converti i risultati in un DataFrame per una facile visualizzazione\n",
        "results_df = pd.DataFrame(grid_search_results)\n",
        "results_df = results_df.sort_values(by='val_f1', ascending=False)\n",
        "\n",
        "print(results_df.to_markdown(index=False))\n",
        "\n",
        "print(\"\\n--- Configurazione Migliore ---\")\n",
        "print(f\"Punteggio F1: {best_overall_f1:.4f}\")\n",
        "print(f\"Iperparametri: {best_overall_config}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UytczcuXPKoE",
        "outputId": "7e0ea45b-0c57-4ed6-cd1e-0967a765ed25"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Risultati del Grid Search ---\n",
            "|   HIDDEN_SIZE |   NUM_LAYERS |   DROPOUT_RATE |   LEARNING_RATE |   val_f1 |\n",
            "|--------------:|-------------:|---------------:|----------------:|---------:|\n",
            "|           128 |            3 |            0.3 |           0.001 | 0.906243 |\n",
            "|           128 |            3 |            0.5 |           0.001 | 0.878044 |\n",
            "|           256 |            2 |            0.5 |           0.001 | 0.871668 |\n",
            "|           256 |            3 |            0.5 |           0.001 | 0.863137 |\n",
            "|           256 |            2 |            0.3 |           0.001 | 0.862601 |\n",
            "|           128 |            2 |            0.5 |           0.001 | 0.859864 |\n",
            "|           256 |            3 |            0.3 |           0.001 | 0.858983 |\n",
            "|           128 |            2 |            0.3 |           0.001 | 0.844573 |\n",
            "\n",
            "--- Configurazione Migliore ---\n",
            "Punteggio F1: 0.9062\n",
            "Iperparametri: {'HIDDEN_SIZE': 128, 'NUM_LAYERS': 3, 'DROPOUT_RATE': 0.3, 'LEARNING_RATE': 0.001}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Definizione e Caricamento Modello Vincente ---\n",
        "\n",
        "# Definiamo la configurazione vincente del Grid Search\n",
        "BEST_CONFIG = {\n",
        "    'HIDDEN_SIZE': 128,\n",
        "    'NUM_LAYERS': 3,\n",
        "    'DROPOUT_RATE': 0.3,\n",
        "    'RNN_TYPE': 'GRU',\n",
        "    'BIDIRECTIONAL': True\n",
        "}\n",
        "\n",
        "# Inizializza il modello con questa architettura\n",
        "new_best_model = RecurrentClassifier(\n",
        "    input_size=total_features,\n",
        "    hidden_size=BEST_CONFIG['HIDDEN_SIZE'],\n",
        "    num_layers=BEST_CONFIG['NUM_LAYERS'],\n",
        "    num_classes=3,\n",
        "    rnn_type=BEST_CONFIG['RNN_TYPE'],\n",
        "    bidirectional=BEST_CONFIG['BIDIRECTIONAL'],\n",
        "    dropout_rate=BEST_CONFIG['DROPOUT_RATE']\n",
        ").to(device)\n",
        "\n",
        "# Definisci il path del modello salvato\n",
        "# (corrisponde al nome dell'esperimento vincente)\n",
        "model_path = \"models/GRU_bidir=True_hs=128_nl=3_dr=0.3_lr=0.001_best_model.pt\"\n",
        "\n",
        "# Carica i pesi\n",
        "new_best_model.load_state_dict(torch.load(model_path))\n",
        "new_best_model.eval() # Metti in modalità valutazione\n",
        "\n",
        "print(f\"Modello vincente caricato da: {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Jcn_hS4R_Sy",
        "outputId": "2722e7ce-e895-4e2b-ecea-33a2b6741406"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello vincente caricato da: models/GRU_bidir=True_hs=128_nl=3_dr=0.3_lr=0.001_best_model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Predizione sul Test Set e Creazione Submission v2 ---\n",
        "\n",
        "print(\"Inizio predizione sul Test Set con il modello ottimizzato...\")\n",
        "\n",
        "# (test_loader è già definito dalla cella 8)\n",
        "# (BATCH_SIZE è già definito)\n",
        "\n",
        "# Eseguiamo la predizione\n",
        "all_test_preds = []\n",
        "with torch.no_grad():\n",
        "    for (inputs,) in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        logits = new_best_model(inputs)\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        all_test_preds.append(preds.cpu().numpy())\n",
        "\n",
        "test_predictions_int = np.concatenate(all_test_preds)\n",
        "\n",
        "# Mappa inversa per le etichette (definita nella cella 7)\n",
        "inv_label_map = {0: 'no_pain', 1: 'low_pain', 2: 'high_pain'}\n",
        "test_predictions_labels = [inv_label_map[pred] for pred in test_predictions_int]\n",
        "\n",
        "# Crea il DataFrame\n",
        "submission_df_v2 = pd.DataFrame({\n",
        "    'sample_index': X_test_raw['sample_index'].unique(),\n",
        "    'label': test_predictions_labels\n",
        "})\n",
        "\n",
        "# Salva il nuovo file .csv\n",
        "submission_filename = 'submission_gridsearch_v1.csv'\n",
        "submission_df_v2.to_csv(submission_filename, index=False)\n",
        "\n",
        "print(f\"\\nNuovo file di submission creato: {submission_filename}\")\n",
        "print(submission_df_v2.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XER-uyBwSBaZ",
        "outputId": "8a07c76b-8c36-4ebc-85ab-c17fcd95427e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inizio predizione sul Test Set con il modello ottimizzato...\n",
            "\n",
            "Nuovo file di submission creato: submission_gridsearch_v1.csv\n",
            "   sample_index    label\n",
            "0             0  no_pain\n",
            "1             1  no_pain\n",
            "2             2  no_pain\n",
            "3             3  no_pain\n",
            "4             4  no_pain\n"
          ]
        }
      ]
    }
  ]
}
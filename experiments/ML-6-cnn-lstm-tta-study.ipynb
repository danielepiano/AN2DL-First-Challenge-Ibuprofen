{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fac4df72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T13:30:36.198696Z",
     "iopub.status.busy": "2025-11-17T13:30:36.198419Z",
     "iopub.status.idle": "2025-11-17T15:05:51.474271Z",
     "shell.execute_reply": "2025-11-17T15:05:51.473418Z"
    },
    "papermill": {
     "duration": 5715.288475,
     "end_time": "2025-11-17T15:05:51.483563",
     "exception": false,
     "start_time": "2025-11-17T13:30:36.195088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in uso: cuda\n",
      "--- Esecuzione Run con Configurazione (V-FINALE: 0.9425 + TTA + NaN/Time Fix) ---\n",
      "LSTM_HIDDEN: 128\n",
      "LSTM_LAYERS: 2\n",
      "CNN_DROPOUT: 0.2\n",
      "LSTM_DROPOUT: 0.3\n",
      "CLASSIFIER_DROPOUT: 0.3\n",
      "WINDOW_SIZE: 40\n",
      "STRIDE: 10\n",
      "LEARNING_RATE: 0.001\n",
      "WEIGHT_DECAY: 0.0001\n",
      "EPOCHS: 50\n",
      "BATCH_SIZE: 64\n",
      "GRADIENT_CLIP_VALUE: 1.0\n",
      "EARLY_STOPPING_PATIENCE: 15\n",
      "LABEL_SMOOTHING: 0.1\n",
      "TTA_STEPS: 10\n",
      "TTA_NOISE_LEVEL: 0.02\n",
      "K_FOLDS: 5\n",
      "-----------------------------------------\n",
      "--- 1. Caricamento e Pulizia Iniziale ---\n",
      "Pre-fillna (Continui): NaN nel test set: 0\n",
      "Post-fillna (Continui): NaN nel test set: 0\n",
      "Trovata colonna 'Team Name': n_legs\n",
      "Vocabolario Text Size: 2\n",
      "Calcolo Feature Engineering (Train)...\n",
      "  Calcolato max_time_val (Train): 160\n",
      "Calcolo Feature Engineering (Test)...\n",
      "  Calcolato max_time_val (Test): 160\n",
      "Numero colonne continue: 62\n",
      "\n",
      "--- Avvio K-Fold (V-FINALE: 0.9425 + TTA + NaN/Time Fix) ---\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Epoch 1/50, Train Loss: 0.2523, Val F1: 0.7898, LR: 1.0e-03\n",
      "Validation F1 increased (0.789821 --> 0.789821).  Saving model to model_fold_1.pth ...\n",
      "Saving OOF probabilities for 133 samples...\n",
      "Validation F1 increased (0.900708 --> 0.900708).  Saving model to model_fold_1.pth ...\n",
      "Saving OOF probabilities for 133 samples...\n",
      "Validation F1 increased (0.925322 --> 0.925322).  Saving model to model_fold_1.pth ...\n",
      "Saving OOF probabilities for 133 samples...\n",
      "EarlyStopping counter: 1 out of 15\n",
      "Epoch 5/50, Train Loss: 0.0711, Val F1: 0.9266, LR: 1.0e-03\n",
      "Validation F1 increased (0.926568 --> 0.926568).  Saving model to model_fold_1.pth ...\n",
      "Saving OOF probabilities for 133 samples...\n",
      "EarlyStopping counter: 1 out of 15\n",
      "Validation F1 increased (0.928011 --> 0.928011).  Saving model to model_fold_1.pth ...\n",
      "Saving OOF probabilities for 133 samples...\n",
      "Validation F1 increased (0.940611 --> 0.940611).  Saving model to model_fold_1.pth ...\n",
      "Saving OOF probabilities for 133 samples...\n",
      "Validation F1 increased (0.954897 --> 0.954897).  Saving model to model_fold_1.pth ...\n",
      "Saving OOF probabilities for 133 samples...\n",
      "Epoch 10/50, Train Loss: 0.0407, Val F1: 0.9491, LR: 1.0e-03\n",
      "EarlyStopping counter: 1 out of 15\n",
      "EarlyStopping counter: 2 out of 15\n",
      "Validation F1 increased (0.968714 --> 0.968714).  Saving model to model_fold_1.pth ...\n",
      "Saving OOF probabilities for 133 samples...\n",
      "EarlyStopping counter: 1 out of 15\n",
      "EarlyStopping counter: 2 out of 15\n",
      "Epoch 15/50, Train Loss: 0.0324, Val F1: 0.9530, LR: 1.0e-03\n",
      "EarlyStopping counter: 3 out of 15\n",
      "EarlyStopping counter: 4 out of 15\n",
      "EarlyStopping counter: 5 out of 15\n",
      "EarlyStopping counter: 6 out of 15\n",
      "Validation F1 increased (0.969971 --> 0.969971).  Saving model to model_fold_1.pth ...\n",
      "Saving OOF probabilities for 133 samples...\n",
      "Epoch 20/50, Train Loss: 0.0308, Val F1: 0.9350, LR: 1.0e-03\n",
      "EarlyStopping counter: 1 out of 15\n",
      "EarlyStopping counter: 2 out of 15\n",
      "EarlyStopping counter: 3 out of 15\n",
      "EarlyStopping counter: 4 out of 15\n",
      "EarlyStopping counter: 5 out of 15\n",
      "Epoch 25/50, Train Loss: 0.0266, Val F1: 0.9348, LR: 1.0e-03\n",
      "EarlyStopping counter: 6 out of 15\n",
      "EarlyStopping counter: 7 out of 15\n",
      "EarlyStopping counter: 8 out of 15\n",
      "EarlyStopping counter: 9 out of 15\n",
      "EarlyStopping counter: 10 out of 15\n",
      "Epoch 30/50, Train Loss: 0.0249, Val F1: 0.9475, LR: 1.0e-03\n",
      "EarlyStopping counter: 11 out of 15\n",
      "EarlyStopping counter: 12 out of 15\n",
      "EarlyStopping counter: 13 out of 15\n",
      "EarlyStopping counter: 14 out of 15\n",
      "Validation F1 increased (0.969971 --> 0.969971).  Saving model to model_fold_1.pth ...\n",
      "Saving OOF probabilities for 133 samples...\n",
      "Epoch 35/50, Train Loss: 0.0239, Val F1: 0.9619, LR: 1.0e-03\n",
      "EarlyStopping counter: 1 out of 15\n",
      "Validation F1 increased (0.984399 --> 0.984399).  Saving model to model_fold_1.pth ...\n",
      "Saving OOF probabilities for 133 samples...\n",
      "EarlyStopping counter: 1 out of 15\n",
      "EarlyStopping counter: 2 out of 15\n",
      "EarlyStopping counter: 3 out of 15\n",
      "Epoch 40/50, Train Loss: 0.0254, Val F1: 0.9760, LR: 1.0e-03\n",
      "EarlyStopping counter: 4 out of 15\n",
      "EarlyStopping counter: 5 out of 15\n",
      "EarlyStopping counter: 6 out of 15\n",
      "EarlyStopping counter: 7 out of 15\n",
      "EarlyStopping counter: 8 out of 15\n",
      "Epoch 45/50, Train Loss: 0.0244, Val F1: 0.9399, LR: 1.0e-03\n",
      "EarlyStopping counter: 9 out of 15\n",
      "EarlyStopping counter: 10 out of 15\n",
      "EarlyStopping counter: 11 out of 15\n",
      "EarlyStopping counter: 12 out of 15\n",
      "EarlyStopping counter: 13 out of 15\n",
      "Epoch 50/50, Train Loss: 0.0243, Val F1: 0.9699, LR: 1.0e-03\n",
      "EarlyStopping counter: 14 out of 15\n",
      "Fold 1 Best Val F1: 0.9844 (Tempo: 985.4s)\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Epoch 1/50, Train Loss: 0.2719, Val F1: 0.8658, LR: 1.0e-03\n",
      "Validation F1 increased (0.865850 --> 0.865850).  Saving model to model_fold_2.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "Validation F1 increased (0.893290 --> 0.893290).  Saving model to model_fold_2.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "EarlyStopping counter: 1 out of 15\n",
      "Validation F1 increased (0.898862 --> 0.898862).  Saving model to model_fold_2.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "Epoch 5/50, Train Loss: 0.0550, Val F1: 0.9073, LR: 1.0e-03\n",
      "Validation F1 increased (0.907281 --> 0.907281).  Saving model to model_fold_2.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "Validation F1 increased (0.912346 --> 0.912346).  Saving model to model_fold_2.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "Validation F1 increased (0.938414 --> 0.938414).  Saving model to model_fold_2.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "EarlyStopping counter: 1 out of 15\n",
      "EarlyStopping counter: 2 out of 15\n",
      "Epoch 10/50, Train Loss: 0.0471, Val F1: 0.9265, LR: 1.0e-03\n",
      "EarlyStopping counter: 3 out of 15\n",
      "EarlyStopping counter: 4 out of 15\n",
      "EarlyStopping counter: 5 out of 15\n",
      "EarlyStopping counter: 6 out of 15\n",
      "EarlyStopping counter: 7 out of 15\n",
      "Epoch 15/50, Train Loss: 0.0345, Val F1: 0.9161, LR: 1.0e-03\n",
      "EarlyStopping counter: 8 out of 15\n",
      "EarlyStopping counter: 9 out of 15\n",
      "EarlyStopping counter: 10 out of 15\n",
      "Validation F1 increased (0.945455 --> 0.945455).  Saving model to model_fold_2.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "EarlyStopping counter: 1 out of 15\n",
      "Epoch 20/50, Train Loss: 0.0288, Val F1: 0.9072, LR: 1.0e-03\n",
      "EarlyStopping counter: 2 out of 15\n",
      "EarlyStopping counter: 3 out of 15\n",
      "EarlyStopping counter: 4 out of 15\n",
      "EarlyStopping counter: 5 out of 15\n",
      "EarlyStopping counter: 6 out of 15\n",
      "Epoch 25/50, Train Loss: 0.0256, Val F1: 0.9167, LR: 1.0e-03\n",
      "EarlyStopping counter: 7 out of 15\n",
      "EarlyStopping counter: 8 out of 15\n",
      "Validation F1 increased (0.960953 --> 0.960953).  Saving model to model_fold_2.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "EarlyStopping counter: 1 out of 15\n",
      "EarlyStopping counter: 2 out of 15\n",
      "Epoch 30/50, Train Loss: 0.0279, Val F1: 0.9353, LR: 1.0e-03\n",
      "EarlyStopping counter: 3 out of 15\n",
      "EarlyStopping counter: 4 out of 15\n",
      "EarlyStopping counter: 5 out of 15\n",
      "EarlyStopping counter: 6 out of 15\n",
      "EarlyStopping counter: 7 out of 15\n",
      "Epoch 35/50, Train Loss: 0.0247, Val F1: 0.9314, LR: 1.0e-03\n",
      "EarlyStopping counter: 8 out of 15\n",
      "EarlyStopping counter: 9 out of 15\n",
      "EarlyStopping counter: 10 out of 15\n",
      "EarlyStopping counter: 11 out of 15\n",
      "EarlyStopping counter: 12 out of 15\n",
      "Epoch 40/50, Train Loss: 0.0211, Val F1: 0.9396, LR: 1.0e-03\n",
      "EarlyStopping counter: 13 out of 15\n",
      "EarlyStopping counter: 14 out of 15\n",
      "EarlyStopping counter: 15 out of 15\n",
      "Early stopping\n",
      "Fold 2 Best Val F1: 0.9610 (Tempo: 831.0s)\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Epoch 1/50, Train Loss: 0.2795, Val F1: 0.8774, LR: 1.0e-03\n",
      "Validation F1 increased (0.877375 --> 0.877375).  Saving model to model_fold_3.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "EarlyStopping counter: 1 out of 15\n",
      "Validation F1 increased (0.891636 --> 0.891636).  Saving model to model_fold_3.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "Validation F1 increased (0.901126 --> 0.901126).  Saving model to model_fold_3.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "Epoch 5/50, Train Loss: 0.0579, Val F1: 0.8892, LR: 1.0e-03\n",
      "EarlyStopping counter: 1 out of 15\n",
      "Validation F1 increased (0.910685 --> 0.910685).  Saving model to model_fold_3.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "EarlyStopping counter: 1 out of 15\n",
      "EarlyStopping counter: 2 out of 15\n",
      "EarlyStopping counter: 3 out of 15\n",
      "Epoch 10/50, Train Loss: 0.0363, Val F1: 0.9107, LR: 1.0e-03\n",
      "Validation F1 increased (0.910685 --> 0.910685).  Saving model to model_fold_3.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "Validation F1 increased (0.924242 --> 0.924242).  Saving model to model_fold_3.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "EarlyStopping counter: 1 out of 15\n",
      "EarlyStopping counter: 2 out of 15\n",
      "EarlyStopping counter: 3 out of 15\n",
      "Epoch 15/50, Train Loss: 0.0358, Val F1: 0.9238, LR: 1.0e-03\n",
      "EarlyStopping counter: 4 out of 15\n",
      "EarlyStopping counter: 5 out of 15\n",
      "EarlyStopping counter: 6 out of 15\n",
      "Validation F1 increased (0.930318 --> 0.930318).  Saving model to model_fold_3.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "EarlyStopping counter: 1 out of 15\n",
      "Epoch 20/50, Train Loss: 0.0258, Val F1: 0.9312, LR: 1.0e-03\n",
      "Validation F1 increased (0.931235 --> 0.931235).  Saving model to model_fold_3.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "EarlyStopping counter: 1 out of 15\n",
      "EarlyStopping counter: 2 out of 15\n",
      "EarlyStopping counter: 3 out of 15\n",
      "EarlyStopping counter: 4 out of 15\n",
      "Epoch 25/50, Train Loss: 0.0286, Val F1: 0.9247, LR: 1.0e-03\n",
      "EarlyStopping counter: 5 out of 15\n",
      "Validation F1 increased (0.932902 --> 0.932902).  Saving model to model_fold_3.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "EarlyStopping counter: 1 out of 15\n",
      "EarlyStopping counter: 2 out of 15\n",
      "EarlyStopping counter: 3 out of 15\n",
      "Epoch 30/50, Train Loss: 0.0263, Val F1: 0.9099, LR: 1.0e-03\n",
      "EarlyStopping counter: 4 out of 15\n",
      "EarlyStopping counter: 5 out of 15\n",
      "EarlyStopping counter: 6 out of 15\n",
      "EarlyStopping counter: 7 out of 15\n",
      "EarlyStopping counter: 8 out of 15\n",
      "Epoch 35/50, Train Loss: 0.0257, Val F1: 0.9107, LR: 1.0e-03\n",
      "EarlyStopping counter: 9 out of 15\n",
      "EarlyStopping counter: 10 out of 15\n",
      "EarlyStopping counter: 11 out of 15\n",
      "Validation F1 increased (0.938584 --> 0.938584).  Saving model to model_fold_3.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "EarlyStopping counter: 1 out of 15\n",
      "Epoch 40/50, Train Loss: 0.0250, Val F1: 0.9081, LR: 1.0e-03\n",
      "EarlyStopping counter: 2 out of 15\n",
      "EarlyStopping counter: 3 out of 15\n",
      "EarlyStopping counter: 4 out of 15\n",
      "EarlyStopping counter: 5 out of 15\n",
      "EarlyStopping counter: 6 out of 15\n",
      "Epoch 45/50, Train Loss: 0.0209, Val F1: 0.9169, LR: 1.0e-03\n",
      "EarlyStopping counter: 7 out of 15\n",
      "EarlyStopping counter: 8 out of 15\n",
      "EarlyStopping counter: 9 out of 15\n",
      "EarlyStopping counter: 10 out of 15\n",
      "EarlyStopping counter: 11 out of 15\n",
      "Epoch 50/50, Train Loss: 0.0241, Val F1: 0.9255, LR: 1.0e-03\n",
      "EarlyStopping counter: 12 out of 15\n",
      "Fold 3 Best Val F1: 0.9386 (Tempo: 1000.6s)\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Epoch 1/50, Train Loss: 0.2708, Val F1: 0.9055, LR: 1.0e-03\n",
      "Validation F1 increased (0.905528 --> 0.905528).  Saving model to model_fold_4.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "EarlyStopping counter: 1 out of 15\n",
      "EarlyStopping counter: 2 out of 15\n",
      "Validation F1 increased (0.909062 --> 0.909062).  Saving model to model_fold_4.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "Epoch 5/50, Train Loss: 0.0623, Val F1: 0.9026, LR: 1.0e-03\n",
      "EarlyStopping counter: 1 out of 15\n",
      "Validation F1 increased (0.929626 --> 0.929626).  Saving model to model_fold_4.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "EarlyStopping counter: 1 out of 15\n",
      "EarlyStopping counter: 2 out of 15\n",
      "EarlyStopping counter: 3 out of 15\n",
      "Epoch 10/50, Train Loss: 0.0408, Val F1: 0.9219, LR: 1.0e-03\n",
      "EarlyStopping counter: 4 out of 15\n",
      "Validation F1 increased (0.940988 --> 0.940988).  Saving model to model_fold_4.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "EarlyStopping counter: 1 out of 15\n",
      "Validation F1 increased (0.955267 --> 0.955267).  Saving model to model_fold_4.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "EarlyStopping counter: 1 out of 15\n",
      "Epoch 15/50, Train Loss: 0.0270, Val F1: 0.9475, LR: 1.0e-03\n",
      "EarlyStopping counter: 2 out of 15\n",
      "EarlyStopping counter: 3 out of 15\n",
      "EarlyStopping counter: 4 out of 15\n",
      "EarlyStopping counter: 5 out of 15\n",
      "EarlyStopping counter: 6 out of 15\n",
      "Epoch 20/50, Train Loss: 0.0280, Val F1: 0.9416, LR: 1.0e-03\n",
      "EarlyStopping counter: 7 out of 15\n",
      "EarlyStopping counter: 8 out of 15\n",
      "EarlyStopping counter: 9 out of 15\n",
      "EarlyStopping counter: 10 out of 15\n",
      "EarlyStopping counter: 11 out of 15\n",
      "Epoch 25/50, Train Loss: 0.0254, Val F1: 0.9409, LR: 1.0e-03\n",
      "EarlyStopping counter: 12 out of 15\n",
      "EarlyStopping counter: 13 out of 15\n",
      "EarlyStopping counter: 14 out of 15\n",
      "EarlyStopping counter: 15 out of 15\n",
      "Early stopping\n",
      "Fold 4 Best Val F1: 0.9553 (Tempo: 566.2s)\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Epoch 1/50, Train Loss: 0.2399, Val F1: 0.8436, LR: 1.0e-03\n",
      "Validation F1 increased (0.843564 --> 0.843564).  Saving model to model_fold_5.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "Validation F1 increased (0.864976 --> 0.864976).  Saving model to model_fold_5.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "EarlyStopping counter: 1 out of 15\n",
      "EarlyStopping counter: 2 out of 15\n",
      "Epoch 5/50, Train Loss: 0.0600, Val F1: 0.8938, LR: 1.0e-03\n",
      "Validation F1 increased (0.893796 --> 0.893796).  Saving model to model_fold_5.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "Validation F1 increased (0.903136 --> 0.903136).  Saving model to model_fold_5.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "EarlyStopping counter: 1 out of 15\n",
      "Validation F1 increased (0.946349 --> 0.946349).  Saving model to model_fold_5.pth ...\n",
      "Saving OOF probabilities for 132 samples...\n",
      "EarlyStopping counter: 1 out of 15\n",
      "Epoch 10/50, Train Loss: 0.0474, Val F1: 0.9166, LR: 1.0e-03\n",
      "EarlyStopping counter: 2 out of 15\n",
      "EarlyStopping counter: 3 out of 15\n",
      "EarlyStopping counter: 4 out of 15\n",
      "EarlyStopping counter: 5 out of 15\n",
      "EarlyStopping counter: 6 out of 15\n",
      "Epoch 15/50, Train Loss: 0.0318, Val F1: 0.9388, LR: 1.0e-03\n",
      "EarlyStopping counter: 7 out of 15\n",
      "EarlyStopping counter: 8 out of 15\n",
      "EarlyStopping counter: 9 out of 15\n",
      "EarlyStopping counter: 10 out of 15\n",
      "EarlyStopping counter: 11 out of 15\n",
      "Epoch 20/50, Train Loss: 0.0305, Val F1: 0.9382, LR: 1.0e-03\n",
      "EarlyStopping counter: 12 out of 15\n",
      "EarlyStopping counter: 13 out of 15\n",
      "EarlyStopping counter: 14 out of 15\n",
      "EarlyStopping counter: 15 out of 15\n",
      "Early stopping\n",
      "Fold 5 Best Val F1: 0.9463 (Tempo: 461.1s)\n",
      "\n",
      "--- CV Completato in 3844.3 secondi ---\n",
      "\n",
      "--- Ricerca Soglie Ottimali su OOF ---\n",
      "OOF F1 (base, argmax): 0.9571\n",
      "Soglie Trovate: Low>0.50, High>0.50 -> OOF F1 (ottimizzato): 0.9571\n",
      "\n",
      "--- Generazione Submission (Ensemble + TTA) ---\n",
      "Eseguo TTA per Fold 1/5...\n",
      "Eseguo TTA per Fold 2/5...\n",
      "Eseguo TTA per Fold 3/5...\n",
      "Eseguo TTA per Fold 4/5...\n",
      "Eseguo TTA per Fold 5/5...\n",
      "Fatto! Submission creata.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "import re\n",
    "import warnings\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import os # Aggiunto\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- CONFIGURAZIONE HARDWARE ---\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device in uso: {device}\")\n",
    "\n",
    "# =============================================================================\n",
    "# === SEZIONE CONFIGURAZIONE (V-FINALE: 0.9425 + TTA + Fix) ===\n",
    "# =============================================================================\n",
    "CONFIG = {\n",
    "    # --- Architettura (L'ORIGINALE 0.9425) ---\n",
    "    \"LSTM_HIDDEN\": 128,\n",
    "    \"LSTM_LAYERS\": 2,\n",
    "    \"CNN_DROPOUT\": 0.2,\n",
    "    \"LSTM_DROPOUT\": 0.3,\n",
    "    \"CLASSIFIER_DROPOUT\": 0.3,\n",
    "\n",
    "    # --- Feature & Windowing (L'ORIGINALE) ---\n",
    "    \"WINDOW_SIZE\": 40,\n",
    "    \"STRIDE\": 10,\n",
    "\n",
    "    # --- Training (L'ORIGINALE) ---\n",
    "    \"LEARNING_RATE\": 1e-3, \n",
    "    \"WEIGHT_DECAY\": 1e-4, \n",
    "    \"EPOCHS\": 50,                # Originale\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"GRADIENT_CLIP_VALUE\": 1.0, \n",
    "    \"EARLY_STOPPING_PATIENCE\": 15, # Un po' piÃ¹ di pazienza non guasta\n",
    "    \"LABEL_SMOOTHING\": 0.1,\n",
    "    \n",
    "    # --- Advice 16: TTA ---\n",
    "    \"TTA_STEPS\": 10,\n",
    "    \"TTA_NOISE_LEVEL\": 0.02,\n",
    "\n",
    "    # --- Fissi ---\n",
    "    \"K_FOLDS\": 5,\n",
    "}\n",
    "\n",
    "print(\"--- Esecuzione Run con Configurazione (V-FINALE: 0.9425 + TTA + NaN/Time Fix) ---\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# --- 1. CARICAMENTO E PULIZIA ---\n",
    "print(\"--- 1. Caricamento e Pulizia Iniziale ---\")\n",
    "INPUT_DIR = \"/kaggle/input/pirate\"\n",
    "if not os.path.exists(INPUT_DIR):\n",
    "    print(\"Percorso Kaggle non trovato, uso i file locali...\")\n",
    "    INPUT_DIR = \".\" # Fallback locale\n",
    "\n",
    "try:\n",
    "    df_features_raw = pd.read_csv(os.path.join(INPUT_DIR, 'pirate_pain_train.csv'))\n",
    "    df_labels_raw = pd.read_csv(os.path.join(INPUT_DIR, 'pirate_pain_train_labels.csv'))\n",
    "    df_test_raw = pd.read_csv(os.path.join(INPUT_DIR, 'pirate_pain_test.csv'))\n",
    "    sample_sub_df = pd.read_csv(os.path.join(INPUT_DIR, 'sample_submission.csv'))\n",
    "except Exception as e:\n",
    "    print(f\"Errore caricamento file: {e}\")\n",
    "    df_features_raw = pd.read_csv('pirate_pain_train.csv')\n",
    "    df_labels_raw = pd.read_csv('pirate_pain_train_labels.csv')\n",
    "    df_test_raw = pd.read_csv('pirate_pain_test.csv')\n",
    "    sample_sub_df = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "\n",
    "# --- FIX PER IL CRASH 0.6494 (Completo) ---\n",
    "# 1. Colonne continue\n",
    "cols_to_fill_numeric = [f'joint_{i:02d}' for i in range(31)] + ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
    "print(f\"Pre-fillna (Continui): NaN nel test set: {df_test_raw[cols_to_fill_numeric].isnull().sum().sum()}\")\n",
    "for col in cols_to_fill_numeric:\n",
    "    if col in df_test_raw.columns:\n",
    "        df_test_raw[col] = df_test_raw.groupby('sample_index')[col].ffill()\n",
    "df_test_raw = df_test_raw.fillna(0) # Riempi i NaN iniziali\n",
    "print(f\"Post-fillna (Continui): NaN nel test set: {df_test_raw[cols_to_fill_numeric].isnull().sum().sum()}\")\n",
    "# --- FINE FIX 1 ---\n",
    "\n",
    "# --- LOGICA FEATURE TESTO (DAL 0.9425) ---\n",
    "exclude_cols = ['label', 'sample_index']\n",
    "string_cols = df_features_raw.select_dtypes(include=['object']).columns.tolist()\n",
    "string_cols = [c for c in string_cols if c not in exclude_cols]\n",
    "TEXT_COL = None\n",
    "TEXT_VOCAB_SIZE = 0\n",
    "if len(string_cols) > 0:\n",
    "    TEXT_COL = string_cols[0] \n",
    "    print(f\"Trovata colonna 'Team Name': {TEXT_COL}\")\n",
    "    def clean_team_name(text):\n",
    "        if pd.isna(text): return \"unknown\" # Gestisce i NaN\n",
    "        return re.sub(r'[^a-z0-9]', '', str(text).lower())\n",
    "    \n",
    "    df_features_raw[TEXT_COL] = df_features_raw[TEXT_COL].apply(clean_team_name)\n",
    "    df_test_raw[TEXT_COL] = df_test_raw[TEXT_COL].apply(clean_team_name)\n",
    "    \n",
    "    le_text = LabelEncoder()\n",
    "    # Fitta solo su 'unknown' + train + test per evitare crash\n",
    "    all_text = pd.concat([df_features_raw[TEXT_COL], df_test_raw[TEXT_COL]], axis=0).unique()\n",
    "    le_text.fit(all_text)\n",
    "    \n",
    "    df_features_raw[TEXT_COL] = le_text.transform(df_features_raw[TEXT_COL])\n",
    "    df_test_raw[TEXT_COL] = le_text.transform(df_test_raw[TEXT_COL])\n",
    "    \n",
    "    TEXT_VOCAB_SIZE = len(le_text.classes_)\n",
    "    print(f\"Vocabolario Text Size: {TEXT_VOCAB_SIZE}\")\n",
    "else:\n",
    "    print(\"Nessuna colonna 'Team Name' trovata.\")\n",
    "\n",
    "# --- 2. FEATURE ENGINEERING (Come 0.9425, DINAMICO) ---\n",
    "JOINT_COLS = [f'joint_{i:02d}' for i in range(30)] \n",
    "SURVEY_COLS = ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
    "TIME_COL = 'time'\n",
    "def engineer_features(df, is_test=False):\n",
    "    df_eng = df.copy()\n",
    "    grouped = df_eng.groupby('sample_index')\n",
    "    for col in JOINT_COLS:\n",
    "        df_eng[f'd_{col}'] = grouped[col].diff().fillna(0)\n",
    "    \n",
    "    # --- LA CORREZIONE FONDAMENTALE (dal V-Originale) ---\n",
    "    max_time_val = df_eng[TIME_COL].max() + 1 \n",
    "    if max_time_val <= 1: \n",
    "        max_time_val = 160 # Fallback\n",
    "    \n",
    "    if is_test:\n",
    "        print(f\"  Calcolato max_time_val (Test): {max_time_val}\")\n",
    "    else:\n",
    "        print(f\"  Calcolato max_time_val (Train): {max_time_val}\")\n",
    "    # --- FINE CORREZIONE ---\n",
    "    \n",
    "    df_eng['sin_time'] = np.sin(2 * np.pi * df_eng[TIME_COL] / max_time_val)\n",
    "    df_eng['cos_time'] = np.cos(2 * np.pi * df_eng[TIME_COL] / max_time_val)\n",
    "    if 'joint_30' in df_eng.columns:\n",
    "        df_eng = df_eng.drop(columns=['joint_30'])\n",
    "    return df_eng\n",
    "\n",
    "print(\"Calcolo Feature Engineering (Train)...\")\n",
    "df_features_engineered = engineer_features(df_features_raw, is_test=False)\n",
    "print(\"Calcolo Feature Engineering (Test)...\")\n",
    "df_test_engineered = engineer_features(df_test_raw, is_test=True) \n",
    "\n",
    "DELTA_COLS = [f'd_{col}' for col in JOINT_COLS]\n",
    "CONTINUOUS_COLS = JOINT_COLS + DELTA_COLS + ['sin_time', 'cos_time']\n",
    "print(f\"Numero colonne continue: {len(CONTINUOUS_COLS)}\")\n",
    "survey_vocab_sizes = [int(df_features_engineered[c].max() + 1) for c in SURVEY_COLS]\n",
    "time_vocab_size = int(df_features_engineered[TIME_COL].max() + 1)\n",
    "label_mapping = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "df_labels_raw['label_encoded'] = df_labels_raw['label'].map(label_mapping)\n",
    "\n",
    "# --- 3. DATASET & SAMPLER ---\n",
    "class PiratePainDataset(Dataset):\n",
    "    def __init__(self, features_df, labels_df, sample_indices, text_col=None, augment=False):\n",
    "        self.features_df = features_df\n",
    "        self.labels_df = labels_df.set_index('sample_index') if labels_df is not None else None\n",
    "        self.sample_indices = sample_indices\n",
    "        self.window_size = CONFIG[\"WINDOW_SIZE\"]\n",
    "        self.stride = CONFIG[\"STRIDE\"]\n",
    "        self.text_col = text_col \n",
    "        self.augment = augment \n",
    "        self.grouped_features = dict(tuple(features_df.groupby('sample_index')))\n",
    "        self.indices = self._create_indices()\n",
    "    def _create_indices(self):\n",
    "        indices = []\n",
    "        for sample_idx in self.sample_indices:\n",
    "            if sample_idx not in self.grouped_features: continue\n",
    "            data = self.grouped_features[sample_idx]\n",
    "            n_timesteps = len(data)\n",
    "            for start in range(0, n_timesteps - self.window_size + 1, self.stride):\n",
    "                indices.append((sample_idx, start, start + self.window_size))\n",
    "        return indices\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, idx):\n",
    "        sample_idx, start, end = self.indices[idx]\n",
    "        window_data = self.grouped_features[sample_idx].iloc[start:end]\n",
    "        vals = window_data[CONTINUOUS_COLS].values\n",
    "        if np.isnan(vals).any():\n",
    "            vals = np.nan_to_num(vals)\n",
    "        if self.augment:\n",
    "            noise = np.random.normal(0, 0.02, vals.shape) \n",
    "            vals = vals + noise\n",
    "        x_cont = torch.tensor(vals, dtype=torch.float)\n",
    "        x_survey = torch.tensor((window_data[SURVEY_COLS].values + 1), dtype=torch.long)\n",
    "        x_time = torch.tensor((window_data[TIME_COL].values + 1), dtype=torch.long)\n",
    "        x_text = torch.tensor(0, dtype=torch.long)\n",
    "        if self.text_col:\n",
    "            val = window_data[self.text_col].iloc[0]\n",
    "            x_text = torch.tensor(val, dtype=torch.long) # Come V-Originale\n",
    "        label = torch.tensor(-1, dtype=torch.long)\n",
    "        if self.labels_df is not None:\n",
    "            label = torch.tensor(self.labels_df.loc[sample_idx, 'label_encoded'], dtype=torch.long)\n",
    "        return x_cont, x_survey, x_time, x_text, label\n",
    "def get_weighted_sampler(dataset, labels_df):\n",
    "    sample_to_label = labels_df.set_index('sample_index')['label_encoded'].to_dict()\n",
    "    label_counts = labels_df['label_encoded'].value_counts().sort_index()\n",
    "    class_weights = 1.0 / label_counts\n",
    "    weights = [class_weights[sample_to_label[s_idx]] for s_idx, _, _ in dataset.indices if s_idx in sample_to_label]\n",
    "    return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "# --- 4. LOSS FUNCTION ---\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean', label_smoothing=0.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.label_smoothing = label_smoothing\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha, label_smoothing=self.label_smoothing)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# --- 5. MODELLO (L'ORIGINALE 0.9425) ---\n",
    "class PiratePainModel_Original(nn.Module):\n",
    "    def __init__(self, n_continuous, survey_vocab_sizes, time_vocab_size, text_vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_surveys = nn.ModuleList([nn.Embedding(v+2, 4) for v in survey_vocab_sizes])\n",
    "        self.emb_time = nn.Embedding(time_vocab_size+2, 8)\n",
    "        \n",
    "        self.use_text = (text_vocab_size > 0)\n",
    "        text_dim = 8 if self.use_text else 0\n",
    "        if self.use_text:\n",
    "            self.emb_text = nn.Embedding(text_vocab_size+2, 8) \n",
    "            \n",
    "        total_survey_dim = len(survey_vocab_sizes) * 4\n",
    "        input_dim = n_continuous + total_survey_dim + 8 + text_dim\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(CONFIG[\"CNN_DROPOUT\"]),\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(CONFIG[\"CNN_DROPOUT\"])\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            128, \n",
    "            CONFIG[\"LSTM_HIDDEN\"], \n",
    "            num_layers=CONFIG[\"LSTM_LAYERS\"], \n",
    "            batch_first=True, \n",
    "            dropout=CONFIG[\"LSTM_DROPOUT\"],\n",
    "            bidirectional=False\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(CONFIG[\"CLASSIFIER_DROPOUT\"])\n",
    "        self.classifier = nn.Linear(CONFIG[\"LSTM_HIDDEN\"], 3) \n",
    "\n",
    "    def forward(self, x_cont, x_survey, x_time, x_text):\n",
    "        batch_size, seq_len, _ = x_cont.shape\n",
    "        e_surv = [emb(x_survey[:,:,i]) for i, emb in enumerate(self.emb_surveys)]\n",
    "        e_time = self.emb_time(x_time)\n",
    "        features = [x_cont] + e_surv + [e_time]\n",
    "        if self.use_text:\n",
    "            e_txt = self.emb_text(x_text).unsqueeze(1).repeat(1, seq_len, 1)\n",
    "            features.append(e_txt)\n",
    "        full_input = torch.cat(features, dim=2) \n",
    "        \n",
    "        x = full_input.permute(0, 2, 1)\n",
    "        x = self.cnn(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        lstm_out, _ = self.lstm(x) \n",
    "        \n",
    "        last_step = lstm_out[:, -1, :]\n",
    "        logits = self.classifier(self.dropout(last_step))\n",
    "        return logits\n",
    "\n",
    "# --- 6. TRAINING & VALIDATION UTILS ---\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xc, xs, xt, xtxt, y in loader:\n",
    "        xc, xs, xt, xtxt, y = xc.to(device), xs.to(device), xt.to(device), xtxt.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xc, xs, xt, xtxt)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG[\"GRADIENT_CLIP_VALUE\"])\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# --- 6.5. CLASSE EARLY STOPPING (Dai Lab) ---\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None \n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_f1, model, oof_data=None):\n",
    "        score = val_f1 \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_f1, model)\n",
    "            if oof_data: self.save_oof(oof_data)\n",
    "        elif score < self.best_score + self.delta: \n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else: \n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_f1, model)\n",
    "            if oof_data: self.save_oof(oof_data)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_f1, model):\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation F1 increased ({self.best_score:.6f} --> {val_f1:.6f}).  Saving model to {self.path} ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "\n",
    "    def save_oof(self, oof_data):\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Saving OOF probabilities for {len(oof_data[\"indices\"])} samples...')\n",
    "        oof_probs_all = oof_data['oof_probs_all']\n",
    "        oof_targets_all = oof_data['oof_targets_all']\n",
    "        sample_to_idx = oof_data['sample_to_idx']\n",
    "        current_val_probs = oof_data['current_probs']\n",
    "        current_val_indices = oof_data['indices']\n",
    "        current_val_labels = oof_data['labels']\n",
    "        for idx, s_idx in enumerate(current_val_indices):\n",
    "            global_idx = sample_to_idx[s_idx]\n",
    "            oof_probs_all[global_idx] = current_val_probs[idx]\n",
    "            oof_targets_all[global_idx] = current_val_labels[idx]\n",
    "\n",
    "# --- 7. MAIN LOOP (SENZA SCHEDULER, COME 0.9425) ---\n",
    "print(\"\\n--- Avvio K-Fold (V-FINALE: 0.9425 + TTA + NaN/Time Fix) ---\")\n",
    "start_time_cv = time.time()\n",
    "all_sample_indices = df_labels_raw['sample_index'].unique()\n",
    "all_labels_strat = df_labels_raw.set_index('sample_index').loc[all_sample_indices]['label_encoded'].values\n",
    "oof_probs = np.zeros((len(all_sample_indices), 3))\n",
    "oof_targets = np.zeros(len(all_sample_indices))\n",
    "sample_to_idx = {s: i for i, s in enumerate(all_sample_indices)}\n",
    "models_list = [] \n",
    "skf = StratifiedKFold(n_splits=CONFIG[\"K_FOLDS\"], shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(all_sample_indices, all_labels_strat)):\n",
    "    fold_start_time = time.time()\n",
    "    print(f\"\\n--- Fold {fold+1}/{CONFIG['K_FOLDS']} ---\")\n",
    "    train_samples = all_sample_indices[train_idx]\n",
    "    val_samples = all_sample_indices[val_idx]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_subset = df_features_engineered[df_features_engineered['sample_index'].isin(train_samples)]\n",
    "    scaler.fit(train_subset[CONTINUOUS_COLS])\n",
    "    \n",
    "    df_fold = df_features_engineered.copy()\n",
    "    df_fold[CONTINUOUS_COLS] = scaler.transform(df_fold[CONTINUOUS_COLS])\n",
    "    \n",
    "    train_ds = PiratePainDataset(df_fold, df_labels_raw, train_samples, TEXT_COL, augment=True)\n",
    "    val_ds = PiratePainDataset(df_fold, df_labels_raw, val_samples, TEXT_COL, augment=False)\n",
    "    \n",
    "    sampler = get_weighted_sampler(train_ds, df_labels_raw)\n",
    "    train_loader = DataLoader(train_ds, batch_size=CONFIG[\"BATCH_SIZE\"], sampler=sampler, shuffle=False, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=CONFIG[\"BATCH_SIZE\"], shuffle=False)\n",
    "    \n",
    "    model = PiratePainModel_Original(\n",
    "        n_continuous=len(CONTINUOUS_COLS), \n",
    "        survey_vocab_sizes=survey_vocab_sizes, \n",
    "        time_vocab_size=time_vocab_size,\n",
    "        text_vocab_size=TEXT_VOCAB_SIZE,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG[\"LEARNING_RATE\"], weight_decay=CONFIG[\"WEIGHT_DECAY\"])\n",
    "    criterion = FocalLoss(alpha=None, gamma=2.0, label_smoothing=CONFIG[\"LABEL_SMOOTHING\"])\n",
    "    \n",
    "    # --- NIENTE SCHEDULER (come 0.9425) ---\n",
    "    \n",
    "    model_path = f'model_fold_{fold+1}.pth'\n",
    "    early_stopping = EarlyStopping(patience=CONFIG[\"EARLY_STOPPING_PATIENCE\"], verbose=True, path=model_path)\n",
    "    \n",
    "    for ep in range(CONFIG[\"EPOCHS\"]):\n",
    "        t_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "        model.eval()\n",
    "        val_logits_list = []\n",
    "        window_sample_map_val = [x[0] for x in val_ds.indices]\n",
    "        with torch.no_grad():\n",
    "            for xc, xs, xt, xtxt, y in val_loader:\n",
    "                xc, xs, xt, xtxt = xc.to(device), xs.to(device), xt.to(device), xtxt.to(device)\n",
    "                logits = model(xc, xs, xt, xtxt)\n",
    "                val_logits_list.extend(logits.cpu().numpy())\n",
    "        \n",
    "        df_val_logits = pd.DataFrame(val_logits_list, columns=[0, 1, 2])\n",
    "        df_val_logits['sample_index'] = window_sample_map_val\n",
    "        df_val_probs = df_val_logits.groupby('sample_index').mean()\n",
    "        current_val_probs = torch.softmax(torch.tensor(df_val_probs.values), dim=1).numpy()\n",
    "        current_val_preds = np.argmax(current_val_probs, axis=1)\n",
    "        current_val_indices = df_val_probs.index\n",
    "        current_val_labels = df_labels_raw.set_index('sample_index').loc[current_val_indices]['label_encoded'].values\n",
    "        v_f1 = f1_score(current_val_labels, current_val_preds, average='weighted')\n",
    "        \n",
    "        # --- NIENTE SCHEDULER.STEP() ---\n",
    "\n",
    "        if (ep+1) % 5 == 0 or ep == 0:\n",
    "            print(f\"Epoch {ep+1}/{CONFIG['EPOCHS']}, Train Loss: {t_loss:.4f}, Val F1: {v_f1:.4f}, LR: {optimizer.param_groups[0]['lr']:.1e}\")\n",
    "        \n",
    "        oof_data_dict = {\n",
    "            'oof_probs_all': oof_probs,\n",
    "            'oof_targets_all': oof_targets,\n",
    "            'sample_to_idx': sample_to_idx,\n",
    "            'current_probs': current_val_probs,\n",
    "            'indices': current_val_indices,\n",
    "            'labels': current_val_labels\n",
    "        }\n",
    "        early_stopping(v_f1, model, oof_data_dict)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    fold_time = time.time() - fold_start_time\n",
    "    print(f\"Fold {fold+1} Best Val F1: {early_stopping.best_score:.4f} (Tempo: {fold_time:.1f}s)\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    models_list.append(model)\n",
    "\n",
    "cv_time = time.time() - start_time_cv\n",
    "print(f\"\\n--- CV Completato in {cv_time:.1f} secondi ---\")\n",
    "\n",
    "# --- 8. OTTIMIZZAZIONE SOGLIE ---\n",
    "print(\"\\n--- Ricerca Soglie Ottimali su OOF ---\")\n",
    "oof_preds_argmax = np.argmax(oof_probs, axis=1)\n",
    "f1_oof_base = f1_score(oof_targets, oof_preds_argmax, average='weighted')\n",
    "print(f\"OOF F1 (base, argmax): {f1_oof_base:.4f}\")\n",
    "best_score = f1_oof_base \n",
    "thr_l_best, thr_h_best = 0.5, 0.5 \n",
    "for t_high in np.arange(0.15, 0.50, 0.01):\n",
    "    for t_low in np.arange(0.20, 0.55, 0.01):\n",
    "        if t_low >= t_high: continue\n",
    "        preds = [2 if p[2] > t_high else (1 if p[1] > t_low else 0) for p in oof_probs]\n",
    "        s = f1_score(oof_targets, preds, average='weighted')\n",
    "        if s > best_score:\n",
    "            best_score = s\n",
    "            thr_l_best, thr_h_best = t_low, t_high\n",
    "print(f\"Soglie Trovate: Low>{thr_l_best:.2f}, High>{thr_h_best:.2f} -> OOF F1 (ottimizzato): {best_score:.4f}\")\n",
    "\n",
    "# --- 9. INFERENZA FINALE (TTA) ---\n",
    "print(\"\\n--- Generazione Submission (Ensemble + TTA) ---\")\n",
    "final_scaler = StandardScaler()\n",
    "final_scaler.fit(df_features_engineered[CONTINUOUS_COLS])\n",
    "df_test_scaled = df_test_engineered.copy()\n",
    "df_test_scaled[CONTINUOUS_COLS] = final_scaler.transform(df_test_scaled[CONTINUOUS_COLS])\n",
    "sub_indices = sample_sub_df['sample_index'].unique()\n",
    "test_ds_final = PiratePainDataset(df_test_scaled, None, sub_indices, TEXT_COL, augment=False)\n",
    "test_loader_final = DataLoader(test_ds_final, batch_size=CONFIG[\"BATCH_SIZE\"], shuffle=False)\n",
    "window_sample_map_test = [x[0] for x in test_ds_final.indices]\n",
    "\n",
    "ensemble_logits = None\n",
    "TTA_STEPS = CONFIG[\"TTA_STEPS\"]\n",
    "TTA_NOISE = CONFIG[\"TTA_NOISE_LEVEL\"]\n",
    "for i, model in enumerate(models_list):\n",
    "    print(f\"Eseguo TTA per Fold {i+1}/{len(models_list)}...\")\n",
    "    model.eval()\n",
    "    fold_avg_logits = None\n",
    "    for tta_run in range(TTA_STEPS):\n",
    "        fold_logits = []\n",
    "        with torch.no_grad():\n",
    "            for xc, xs, xt, xtxt, _ in test_loader_final:\n",
    "                if tta_run > 0: \n",
    "                    noise = torch.normal(0.0, TTA_NOISE, size=xc.shape).to(device)\n",
    "                    xc = xc.to(device) + noise\n",
    "                else:\n",
    "                    xc = xc.to(device)\n",
    "                xs, xt, xtxt = xs.to(device), xt.to(device), xtxt.to(device)\n",
    "                logits = model(xc, xs, xt, xtxt)\n",
    "                fold_logits.extend(logits.cpu().numpy())\n",
    "        df_tmp = pd.DataFrame(fold_logits, columns=[0, 1, 2])\n",
    "        df_tmp['sample_index'] = window_sample_map_test\n",
    "        df_avg_tta_run = df_tmp.groupby('sample_index').mean()\n",
    "        if fold_avg_logits is None:\n",
    "            fold_avg_logits = df_avg_tta_run.reindex(sub_indices, fill_value=0)\n",
    "        else:\n",
    "            fold_avg_logits = fold_avg_logits.add(df_avg_tta_run.reindex(sub_indices, fill_value=0), fill_value=0)\n",
    "    fold_avg_logits /= TTA_STEPS\n",
    "    if ensemble_logits is None:\n",
    "        ensemble_logits = fold_avg_logits\n",
    "    else:\n",
    "        ensemble_logits = ensemble_logits.add(fold_avg_logits, fill_value=0)\n",
    "\n",
    "ensemble_logits /= CONFIG[\"K_FOLDS\"]\n",
    "final_probs = torch.softmax(torch.tensor(ensemble_logits.values), dim=1).numpy()\n",
    "final_preds_list = []\n",
    "thr_l, thr_h = thr_l_best, thr_h_best\n",
    "for p in final_probs:\n",
    "    if p[2] > thr_h: final_preds_list.append(2)\n",
    "    elif p[1] > thr_l: final_preds_list.append(1)\n",
    "    else: final_preds_list.append(0)\n",
    "final_series = pd.Series(final_preds_list, index=ensemble_logits.index)\n",
    "inv_map = {v: k for k, v in label_mapping.items()}\n",
    "submission = final_series.map(inv_map).reset_index()\n",
    "submission.columns = ['sample_index', 'label']\n",
    "submission = submission.set_index('sample_index').reindex(sample_sub_df['sample_index']).reset_index()\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Fatto! Submission creata.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8722895,
     "sourceId": 13711641,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5720.847606,
   "end_time": "2025-11-17T15:05:53.515274",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-17T13:30:32.667668",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

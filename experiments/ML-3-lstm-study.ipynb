{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34ecfb08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T22:48:59.909817Z",
     "iopub.status.busy": "2025-11-12T22:48:59.909553Z",
     "iopub.status.idle": "2025-11-12T22:49:06.033329Z",
     "shell.execute_reply": "2025-11-12T22:49:06.032442Z"
    },
    "papermill": {
     "duration": 6.128541,
     "end_time": "2025-11-12T22:49:06.034841",
     "exception": false,
     "start_time": "2025-11-12T22:48:59.906300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in uso: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Ignora avvisi non critici\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Parametri Globali ---\n",
    "JOINT_COLS = [f'joint_{i:02d}' for i in range(30)]\n",
    "SURVEY_COLS = ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
    "STATIC_COLS = ['n_legs', 'n_hands', 'n_eyes'] \n",
    "TIME_COL = 'time'\n",
    "\n",
    "WINDOW_SIZE = 40\n",
    "STRIDE = 10\n",
    "\n",
    "# --- Parametri Training ---\n",
    "SEED = 42\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 50 \n",
    "GRADIENT_CLIP_VALUE = 1.0 \n",
    "K_FOLDS = 5 \n",
    "\n",
    "# Setup Riproducibilità\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device in uso: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95225e18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T22:49:06.040506Z",
     "iopub.status.busy": "2025-11-12T22:49:06.040026Z",
     "iopub.status.idle": "2025-11-12T22:49:10.822569Z",
     "shell.execute_reply": "2025-11-12T22:49:10.821458Z"
    },
    "papermill": {
     "duration": 4.786747,
     "end_time": "2025-11-12T22:49:10.823790",
     "exception": false,
     "start_time": "2025-11-12T22:49:06.037043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Caricamento e Pulizia Iniziale ---\n",
      "Nessuna colonna 'Team Name' trovata (a parte le feature numeriche).\n",
      "Calcolo Delta Features...\n",
      "Preprocessing Completato.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Caricamento e Pulizia Iniziale ---\")\n",
    "\n",
    "df_features_raw = pd.read_csv('/kaggle/input/pirate/pirate_pain_train.csv')\n",
    "df_labels_raw = pd.read_csv('/kaggle/input/pirate/pirate_pain_train_labels.csv')\n",
    "df_test_raw = pd.read_csv('/kaggle/input/pirate/pirate_pain_test.csv')\n",
    "\n",
    "# 1. FIX: Forziamo le colonne statiche a essere Interi (per evitare che Pandas le legga come stringhe)\n",
    "for col in STATIC_COLS:\n",
    "    df_features_raw[col] = pd.to_numeric(df_features_raw[col], errors='coerce').fillna(0).astype(int)\n",
    "    df_test_raw[col] = pd.to_numeric(df_test_raw[col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# 2. COMMUNICATION HINT: Cerca una VERA colonna di testo (Team Name)\n",
    "exclude_cols = ['label', 'sample_index']\n",
    "string_cols = df_features_raw.select_dtypes(include=['object']).columns.tolist()\n",
    "string_cols = [c for c in string_cols if c not in exclude_cols]\n",
    "\n",
    "TEXT_COL = None\n",
    "TEXT_VOCAB_SIZE = 0\n",
    "\n",
    "if len(string_cols) > 0:\n",
    "    TEXT_COL = string_cols[0] \n",
    "    print(f\"Trovata colonna 'Team Name': {TEXT_COL}\")\n",
    "    \n",
    "    def clean_team_name(text):\n",
    "        if pd.isna(text): return \"unknown\"\n",
    "        return re.sub(r'[^a-z0-9]', '', str(text).lower())\n",
    "\n",
    "    df_features_raw[TEXT_COL] = df_features_raw[TEXT_COL].apply(clean_team_name)\n",
    "    df_test_raw[TEXT_COL] = df_test_raw[TEXT_COL].apply(clean_team_name)\n",
    "    \n",
    "    le_text = LabelEncoder()\n",
    "    all_text = pd.concat([df_features_raw[TEXT_COL], df_test_raw[TEXT_COL]], axis=0)\n",
    "    le_text.fit(all_text)\n",
    "    \n",
    "    df_features_raw[TEXT_COL] = le_text.transform(df_features_raw[TEXT_COL])\n",
    "    df_test_raw[TEXT_COL] = le_text.transform(df_test_raw[TEXT_COL])\n",
    "    \n",
    "    TEXT_VOCAB_SIZE = len(le_text.classes_)\n",
    "    print(f\"Vocabolario Team Name: {TEXT_VOCAB_SIZE} squadre uniche.\")\n",
    "else:\n",
    "    print(\"Nessuna colonna 'Team Name' trovata (a parte le feature numeriche).\")\n",
    "\n",
    "# 3. Feature Engineering Delta\n",
    "def engineer_features(df):\n",
    "    df_eng = df.copy()\n",
    "    grouped = df_eng.groupby('sample_index')\n",
    "    for col in JOINT_COLS:\n",
    "        df_eng[f'd_{col}'] = grouped[col].diff().fillna(0)\n",
    "    \n",
    "    if 'joint_30' in df_eng.columns:\n",
    "        df_eng = df_eng.drop(columns=['joint_30'])\n",
    "    return df_eng\n",
    "\n",
    "print(\"Calcolo Delta Features...\")\n",
    "df_features_engineered = engineer_features(df_features_raw)\n",
    "df_test_engineered = engineer_features(df_test_raw)\n",
    "\n",
    "DELTA_JOINT_COLS = [f'd_{col}' for col in JOINT_COLS]\n",
    "CONTINUOUS_COLS = JOINT_COLS + DELTA_JOINT_COLS\n",
    "\n",
    "# --- Preparazione Vocabolari per Embedding (CORREZIONE QUI) ---\n",
    "# Usiamo int(...) invece di .astype(int)\n",
    "survey_vocab_sizes = [int(df_features_engineered[c].max() + 1) for c in SURVEY_COLS]\n",
    "time_vocab_size = int(df_features_engineered[TIME_COL].max() + 1)\n",
    "static_vocab_sizes = [int(df_features_engineered[c].max() + 1) for c in STATIC_COLS]\n",
    "\n",
    "print(\"Preprocessing Completato.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4df5c9d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T22:49:10.829059Z",
     "iopub.status.busy": "2025-11-12T22:49:10.828821Z",
     "iopub.status.idle": "2025-11-12T22:49:10.840817Z",
     "shell.execute_reply": "2025-11-12T22:49:10.840060Z"
    },
    "papermill": {
     "duration": 0.016043,
     "end_time": "2025-11-12T22:49:10.842000",
     "exception": false,
     "start_time": "2025-11-12T22:49:10.825957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mappatura Label\n",
    "label_mapping = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "df_labels_raw['label_encoded'] = df_labels_raw['label'].map(label_mapping)\n",
    "\n",
    "class PiratePainDataset(Dataset):\n",
    "    def __init__(self, features_df, labels_df, sample_indices, window_size, stride, text_col=None, augment=False):\n",
    "        self.features_df = features_df\n",
    "        # Se labels_df è None, siamo in fase di test\n",
    "        self.labels_df = labels_df.set_index('sample_index') if labels_df is not None else None\n",
    "        self.sample_indices = sample_indices\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.text_col = text_col\n",
    "        \n",
    "        # Raggruppamento per accesso veloce\n",
    "        self.grouped_features = dict(tuple(features_df.groupby('sample_index')))\n",
    "        self.indices = self._create_indices()\n",
    "\n",
    "        self.augment = augment # Salva il parametro\n",
    "\n",
    "    def _create_indices(self):\n",
    "        indices = []\n",
    "        for sample_idx in self.sample_indices:\n",
    "            if sample_idx not in self.grouped_features: continue\n",
    "            data = self.grouped_features[sample_idx]\n",
    "            n_timesteps = len(data)\n",
    "            for start in range(0, n_timesteps - self.window_size + 1, self.stride):\n",
    "                indices.append((sample_idx, start, start + self.window_size))\n",
    "        return indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_idx, start, end = self.indices[idx]\n",
    "        window_data = self.grouped_features[sample_idx].iloc[start:end]\n",
    "\n",
    "        # 1. Continui (Modifica qui!)\n",
    "        vals = window_data[CONTINUOUS_COLS].values\n",
    "        \n",
    "        # --- INIZIO AUGMENTATION ---\n",
    "        if self.augment:\n",
    "            # Aggiunge rumore casuale (Gaussian Noise)\n",
    "            noise = np.random.normal(0, 0.02, vals.shape) \n",
    "            vals = vals + noise\n",
    "        # --- FINE AUGMENTATION ---\n",
    "\n",
    "        # 1. Continui\n",
    "        x_cont = torch.tensor(window_data[CONTINUOUS_COLS].values, dtype=torch.float)\n",
    "        # 2. Survey (+1 sicurezza)\n",
    "        x_survey = torch.tensor((window_data[SURVEY_COLS].values + 1), dtype=torch.long)\n",
    "        # 3. Time (+1 sicurezza)\n",
    "        x_time = torch.tensor((window_data[TIME_COL].values + 1), dtype=torch.long)\n",
    "        # 4. Static (Legs, Hands, Eyes) - Prendi la prima riga (sono costanti)\n",
    "        x_static = torch.tensor((window_data[STATIC_COLS].iloc[0].values + 1), dtype=torch.long)\n",
    "        \n",
    "        # 5. Text (Opzionale)\n",
    "        x_text = torch.tensor(0, dtype=torch.long)\n",
    "        if self.text_col:\n",
    "            val = window_data[self.text_col].iloc[0]\n",
    "            x_text = torch.tensor(val, dtype=torch.long)\n",
    "\n",
    "        label = torch.tensor(-1, dtype=torch.long)\n",
    "        if self.labels_df is not None:\n",
    "            label = torch.tensor(self.labels_df.loc[sample_idx, 'label_encoded'], dtype=torch.long)\n",
    "\n",
    "        return x_cont, x_survey, x_time, x_static, x_text, label\n",
    "\n",
    "# --- Weighted Sampler (Per Advice 08/11 Advanced) ---\n",
    "def get_weighted_sampler(dataset, labels_df):\n",
    "    # Mappa sample -> label\n",
    "    sample_to_label = labels_df.set_index('sample_index')['label_encoded'].to_dict()\n",
    "    # Calcola frequenza inversa classi\n",
    "    label_counts = labels_df['label_encoded'].value_counts().sort_index()\n",
    "    class_weights = 1.0 / label_counts\n",
    "    \n",
    "    # Assegna peso a ogni finestra nel dataset\n",
    "    weights = []\n",
    "    for idx_tuple in dataset.indices:\n",
    "        s_idx = idx_tuple[0]\n",
    "        if s_idx in sample_to_label:\n",
    "            l = sample_to_label[s_idx]\n",
    "            weights.append(class_weights[l])\n",
    "        else:\n",
    "            weights.append(0) # Non dovrebbe accadere in train\n",
    "            \n",
    "    return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37986277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T22:49:10.846647Z",
     "iopub.status.busy": "2025-11-12T22:49:10.846263Z",
     "iopub.status.idle": "2025-11-12T22:49:10.856284Z",
     "shell.execute_reply": "2025-11-12T22:49:10.855728Z"
    },
    "papermill": {
     "duration": 0.013468,
     "end_time": "2025-11-12T22:49:10.857294",
     "exception": false,
     "start_time": "2025-11-12T22:49:10.843826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Focal Loss Custom ---\n",
    "class FocalLoss(nn.Module):\n",
    "    # Aggiungiamo il parametro label_smoothing (default 0.0, consigliato 0.1)\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean', label_smoothing=0.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.label_smoothing = label_smoothing # Salva il valore\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # QUI applichiamo l'Advice 09/11: label_smoothing=self.label_smoothing\n",
    "        ce_loss = F.cross_entropy(\n",
    "            inputs, \n",
    "            targets, \n",
    "            reduction='none', \n",
    "            weight=self.alpha, \n",
    "            label_smoothing=self.label_smoothing  # <--- ECCOLO!\n",
    "        )\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean() if self.reduction == 'mean' else focal_loss.sum()\n",
    "\n",
    "# --- Modello Completo ---\n",
    "class PiratePainModel(nn.Module):\n",
    "    def __init__(self, n_continuous, survey_vocab_sizes, time_vocab_size, \n",
    "                 static_vocab_sizes, text_vocab_size, lstm_hidden=128, n_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding Survey (4 features)\n",
    "        self.emb_surveys = nn.ModuleList([nn.Embedding(v+2, 4) for v in survey_vocab_sizes])\n",
    "        # Embedding Time\n",
    "        self.emb_time = nn.Embedding(time_vocab_size+2, 8)\n",
    "        # Embedding Static (Legs, Hands, Eyes)\n",
    "        self.emb_static = nn.ModuleList([nn.Embedding(v+2, 4) for v in static_vocab_sizes])\n",
    "        \n",
    "        # Embedding Text (Team Name) - Opzionale\n",
    "        self.use_text = (text_vocab_size > 0)\n",
    "        text_dim = 0\n",
    "        if self.use_text:\n",
    "            self.emb_text = nn.Embedding(text_vocab_size+2, 8)\n",
    "            text_dim = 8\n",
    "            \n",
    "        # Calcolo Input LSTM\n",
    "        total_survey_dim = len(survey_vocab_sizes) * 4\n",
    "        total_static_dim = len(static_vocab_sizes) * 4\n",
    "        \n",
    "        lstm_input_dim = n_continuous + total_survey_dim + 8 + total_static_dim + text_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(lstm_input_dim, lstm_hidden, num_layers=2, batch_first=True, dropout=0.3)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(lstm_hidden, n_classes)\n",
    "\n",
    "    def forward(self, x_cont, x_survey, x_time, x_static, x_text):\n",
    "        # x_cont: (Batch, Seq, Feat)\n",
    "        batch_size, seq_len, _ = x_cont.shape\n",
    "        \n",
    "        # 1. Time-Varying Embeddings\n",
    "        e_surv = [emb(x_survey[:,:,i]) for i, emb in enumerate(self.emb_surveys)] # List of (B, S, 4)\n",
    "        e_time = self.emb_time(x_time) # (B, S, 8)\n",
    "        \n",
    "        # 2. Static Embeddings (Processati come vettori statici poi ripetuti)\n",
    "        e_stat = [emb(x_static[:,i]) for i, emb in enumerate(self.emb_static)] # List of (B, 4)\n",
    "        e_stat_cat = torch.cat(e_stat, dim=1) # (B, 12)\n",
    "        \n",
    "        if self.use_text:\n",
    "            e_txt = self.emb_text(x_text) # (B, 8)\n",
    "            e_stat_cat = torch.cat([e_stat_cat, e_txt], dim=1)\n",
    "            \n",
    "        # Ripeti statico per ogni timestep\n",
    "        e_stat_seq = e_stat_cat.unsqueeze(1).repeat(1, seq_len, 1) # (B, S, Tot_Static)\n",
    "        \n",
    "        # 3. Concatena\n",
    "        full_input = torch.cat([x_cont] + e_surv + [e_time, e_stat_seq], dim=2)\n",
    "        \n",
    "        # 4. LSTM\n",
    "        out, _ = self.lstm(full_input)\n",
    "        \n",
    "        # 5. Classifica sull'ultimo step\n",
    "        logits = self.classifier(self.dropout(out[:, -1, :]))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "774aabbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T22:49:10.862240Z",
     "iopub.status.busy": "2025-11-12T22:49:10.861675Z",
     "iopub.status.idle": "2025-11-12T22:49:10.868095Z",
     "shell.execute_reply": "2025-11-12T22:49:10.867540Z"
    },
    "papermill": {
     "duration": 0.009938,
     "end_time": "2025-11-12T22:49:10.869135",
     "exception": false,
     "start_time": "2025-11-12T22:49:10.859197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xc, xs, xt, xst, xtxt, y in loader:\n",
    "        xc, xs, xt, xst, xtxt, y = xc.to(device), xs.to(device), xt.to(device), xst.to(device), xtxt.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xc, xs, xt, xst, xtxt)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_VALUE)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xc, xs, xt, xst, xtxt, y in loader:\n",
    "            xc, xs, xt, xst, xtxt, y = xc.to(device), xs.to(device), xt.to(device), xst.to(device), xtxt.to(device), y.to(device)\n",
    "            \n",
    "            logits = model(xc, xs, xt, xst, xtxt)\n",
    "            loss = criterion(logits, y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "            labels.extend(y.cpu().numpy())\n",
    "            \n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    return total_loss / len(loader), f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7aadeed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T22:49:10.873954Z",
     "iopub.status.busy": "2025-11-12T22:49:10.873730Z",
     "iopub.status.idle": "2025-11-13T00:58:20.980413Z",
     "shell.execute_reply": "2025-11-13T00:58:20.979605Z"
    },
    "papermill": {
     "duration": 7750.11274,
     "end_time": "2025-11-13T00:58:20.983732",
     "exception": false,
     "start_time": "2025-11-12T22:49:10.870992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Avvio K-Fold con Advanced Imbalance Strategy ---\n",
      "Focal Loss Alpha: [0.25556704 0.35009953 0.39433342]\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Ep 10: Train Loss 0.0263 | Val F1 0.9060\n",
      "Ep 20: Train Loss 0.0201 | Val F1 0.9188\n",
      "Ep 30: Train Loss 0.0221 | Val F1 0.9386\n",
      "Ep 40: Train Loss 0.0190 | Val F1 0.9291\n",
      "Ep 50: Train Loss 0.0189 | Val F1 0.9309\n",
      "Fold 1 Best F1: 0.9397\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Ep 10: Train Loss 0.0278 | Val F1 0.9073\n",
      "Ep 20: Train Loss 0.0191 | Val F1 0.9106\n",
      "Ep 30: Train Loss 0.0190 | Val F1 0.9134\n",
      "Ep 40: Train Loss 0.0189 | Val F1 0.9128\n",
      "Ep 50: Train Loss 0.0189 | Val F1 0.9112\n",
      "Fold 2 Best F1: 0.9353\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Ep 10: Train Loss 0.0254 | Val F1 0.9154\n",
      "Ep 20: Train Loss 0.0197 | Val F1 0.9138\n",
      "Ep 30: Train Loss 0.0191 | Val F1 0.9113\n",
      "Ep 40: Train Loss 0.0190 | Val F1 0.9090\n",
      "Ep 50: Train Loss 0.0189 | Val F1 0.9100\n",
      "Fold 3 Best F1: 0.9208\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Ep 10: Train Loss 0.0224 | Val F1 0.9173\n",
      "Ep 20: Train Loss 0.0232 | Val F1 0.9105\n",
      "Ep 30: Train Loss 0.0204 | Val F1 0.9124\n",
      "Ep 40: Train Loss 0.0190 | Val F1 0.9109\n",
      "Ep 50: Train Loss 0.0189 | Val F1 0.9114\n",
      "Fold 4 Best F1: 0.9273\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Ep 10: Train Loss 0.0257 | Val F1 0.9014\n",
      "Ep 20: Train Loss 0.0261 | Val F1 0.8703\n",
      "Ep 30: Train Loss 0.0190 | Val F1 0.9017\n",
      "Ep 40: Train Loss 0.0190 | Val F1 0.8998\n",
      "Ep 50: Train Loss 0.0193 | Val F1 0.9114\n",
      "Fold 5 Best F1: 0.9173\n",
      "Media F1 Score: 0.9281\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Avvio K-Fold con Advanced Imbalance Strategy ---\")\n",
    "\n",
    "# Calcolo pesi 'smoothed' per Focal Loss Alpha\n",
    "labels_array = df_labels_raw['label_encoded'].values\n",
    "counts = np.bincount(labels_array)\n",
    "weights_smooth = 1. / np.log1p(counts) \n",
    "focal_alpha = torch.tensor(weights_smooth / weights_smooth.sum(), dtype=torch.float).to(device)\n",
    "print(f\"Focal Loss Alpha: {focal_alpha.cpu().numpy()}\")\n",
    "\n",
    "all_sample_indices = df_labels_raw['sample_index'].unique()\n",
    "all_labels_strat = df_labels_raw.set_index('sample_index').loc[all_sample_indices]['label_encoded'].values\n",
    "\n",
    "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED)\n",
    "fold_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(all_sample_indices, all_labels_strat)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{K_FOLDS} ---\")\n",
    "    \n",
    "    train_samples = all_sample_indices[train_idx]\n",
    "    val_samples = all_sample_indices[val_idx]\n",
    "    \n",
    "    # 1. Scaling (Fit su Train del fold)\n",
    "    scaler = StandardScaler()\n",
    "    train_subset = df_features_engineered[df_features_engineered['sample_index'].isin(train_samples)]\n",
    "    scaler.fit(train_subset[CONTINUOUS_COLS])\n",
    "    \n",
    "    df_fold = df_features_engineered.copy()\n",
    "    df_fold[CONTINUOUS_COLS] = scaler.transform(df_fold[CONTINUOUS_COLS])\n",
    "    \n",
    "    # 2. Dataset\n",
    "    train_ds = PiratePainDataset(df_fold, df_labels_raw, train_samples, WINDOW_SIZE, STRIDE, TEXT_COL, augment=True)\n",
    "    val_ds = PiratePainDataset(df_fold, df_labels_raw, val_samples, WINDOW_SIZE, STRIDE, TEXT_COL, augment=False)\n",
    "    \n",
    "    # 3. Sampler & Loader\n",
    "    # IMPORTANTE: Sampler va solo nel Train. Shuffle deve essere False quando c'è il sampler.\n",
    "    sampler = get_weighted_sampler(train_ds, df_labels_raw)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, shuffle=False, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # 4. Init Modello\n",
    "    model = PiratePainModel(len(CONTINUOUS_COLS), survey_vocab_sizes, time_vocab_size, \n",
    "                            static_vocab_sizes, TEXT_VOCAB_SIZE, lstm_hidden=128).to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    criterion = FocalLoss(alpha=None, gamma=2.0, label_smoothing=0.1)\n",
    "    \n",
    "    best_fold_f1 = 0\n",
    "    for ep in range(EPOCHS):\n",
    "        t_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "        v_loss, v_f1 = validate(model, val_loader, criterion)\n",
    "        \n",
    "        if v_f1 > best_fold_f1:\n",
    "            best_fold_f1 = v_f1\n",
    "        \n",
    "        if (ep+1) % 10 == 0:\n",
    "            print(f\"Ep {ep+1}: Train Loss {t_loss:.4f} | Val F1 {v_f1:.4f}\")\n",
    "            \n",
    "    print(f\"Fold {fold+1} Best F1: {best_fold_f1:.4f}\")\n",
    "    fold_scores.append(best_fold_f1)\n",
    "\n",
    "print(f\"Media F1 Score: {np.mean(fold_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c4dd83a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T00:58:20.991119Z",
     "iopub.status.busy": "2025-11-13T00:58:20.990645Z",
     "iopub.status.idle": "2025-11-13T01:19:30.706027Z",
     "shell.execute_reply": "2025-11-13T01:19:30.705314Z"
    },
    "papermill": {
     "duration": 1269.723476,
     "end_time": "2025-11-13T01:19:30.710194",
     "exception": false,
     "start_time": "2025-11-13T00:58:20.986718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Finale Full Dataset & Submission ---\n",
      "Addestramento in corso...\n",
      "Ep 10: Loss 0.0281\n",
      "Ep 20: Loss 0.0215\n",
      "Ep 30: Loss 0.0190\n",
      "Ep 40: Loss 0.0189\n",
      "Generazione Predizioni...\n",
      "Fatto! File 'submission.csv' creato.\n",
      "   sample_index    label\n",
      "0             0  no_pain\n",
      "1             1  no_pain\n",
      "2             2  no_pain\n",
      "3             3  no_pain\n",
      "4             4  no_pain\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Training Finale Full Dataset & Submission ---\")\n",
    "\n",
    "# 1. Scaling Finale (Fit su tutto il train)\n",
    "final_scaler = StandardScaler()\n",
    "final_scaler.fit(df_features_engineered[CONTINUOUS_COLS])\n",
    "\n",
    "df_train_final = df_features_engineered.copy()\n",
    "df_train_final[CONTINUOUS_COLS] = final_scaler.transform(df_train_final[CONTINUOUS_COLS])\n",
    "\n",
    "df_test_final = df_test_engineered.copy()\n",
    "df_test_final[CONTINUOUS_COLS] = final_scaler.transform(df_test_final[CONTINUOUS_COLS])\n",
    "\n",
    "# 2. Dataset Finale\n",
    "# Anche nel training finale usiamo il Sampler per mantenere l'equilibrio appreso\n",
    "train_ds_final = PiratePainDataset(df_train_final, df_labels_raw, all_sample_indices, WINDOW_SIZE, STRIDE, TEXT_COL)\n",
    "sampler_final = get_weighted_sampler(train_ds_final, df_labels_raw)\n",
    "train_loader_final = DataLoader(train_ds_final, batch_size=BATCH_SIZE, sampler=sampler_final, shuffle=False, drop_last=True)\n",
    "\n",
    "# 3. Dataset Test\n",
    "sub_indices = pd.read_csv('/kaggle/input/pirate/sample_submission.csv')['sample_index'].unique()\n",
    "test_ds_final = PiratePainDataset(df_test_final, None, sub_indices, WINDOW_SIZE, STRIDE, TEXT_COL)\n",
    "test_loader_final = DataLoader(test_ds_final, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "\n",
    "# 4. Modello Finale\n",
    "final_model = PiratePainModel(len(CONTINUOUS_COLS), survey_vocab_sizes, time_vocab_size, \n",
    "                              static_vocab_sizes, TEXT_VOCAB_SIZE, lstm_hidden=128).to(device)\n",
    "optimizer = optim.AdamW(final_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "criterion = FocalLoss(alpha=None, gamma=2.0, label_smoothing=0.1)\n",
    "\n",
    "# 5. Train Loop\n",
    "final_model.train()\n",
    "print(\"Addestramento in corso...\")\n",
    "for ep in range(40): # 40 epoche bastano di solito per il full fit\n",
    "    loss = train_epoch(final_model, train_loader_final, optimizer, criterion)\n",
    "    if (ep+1)%10 == 0: print(f\"Ep {ep+1}: Loss {loss:.4f}\")\n",
    "\n",
    "# 6. INFERENZA CON SOFT VOTING (Advice 09/11)\n",
    "print(\"Generazione Predizioni...\")\n",
    "final_model.eval()\n",
    "all_logits = []\n",
    "window_sample_map = [x[0] for x in test_ds_final.indices]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xc, xs, xt, xst, xtxt, _ in test_loader_final:\n",
    "        xc, xs, xt, xst, xtxt = xc.to(device), xs.to(device), xt.to(device), xst.to(device), xtxt.to(device)\n",
    "        logits = final_model(xc, xs, xt, xst, xtxt)\n",
    "        all_logits.extend(logits.cpu().numpy())\n",
    "\n",
    "# Creazione DataFrame Logits\n",
    "df_logits = pd.DataFrame(all_logits, columns=[0, 1, 2])\n",
    "df_logits['sample_index'] = window_sample_map\n",
    "\n",
    "# Media dei logits per sample_index (Soft Voting)\n",
    "df_avg_logits = df_logits.groupby('sample_index').mean()\n",
    "\n",
    "# 1. Converti logits medi in probabilità\n",
    "probs = torch.softmax(torch.tensor(df_avg_logits.values), dim=1).numpy()\n",
    "\n",
    "# 2. Applica soglie manuali (Sperimentale: favorisci le classi rare)\n",
    "final_preds_list = []\n",
    "for p in probs:\n",
    "    # Se la probabilità di High Pain è > 25% (non 50%!), predici High Pain\n",
    "    if p[2] > 0.25:\n",
    "        final_preds_list.append(2)\n",
    "    # Altrimenti, se Low Pain è > 30%, predici Low Pain\n",
    "    elif p[1] > 0.30:\n",
    "        final_preds_list.append(1)\n",
    "    # Altrimenti No Pain\n",
    "    else:\n",
    "        final_preds_list.append(0)\n",
    "\n",
    "final_preds = pd.Series(final_preds_list, index=df_avg_logits.index)\n",
    "\n",
    "# Mappatura Inversa e Salvataggio\n",
    "inv_map = {v: k for k, v in label_mapping.items()}\n",
    "submission = final_preds.map(inv_map).reset_index()\n",
    "submission.columns = ['sample_index', 'label']\n",
    "\n",
    "# Ordina come da sample_submission\n",
    "sample_sub = pd.read_csv('/kaggle/input/pirate/sample_submission.csv')\n",
    "submission = submission.set_index('sample_index').reindex(sample_sub['sample_index']).reset_index()\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Fatto! File 'submission.csv' creato.\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8722895,
     "sourceId": 13711641,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9036.162106,
   "end_time": "2025-11-13T01:19:32.533893",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-12T22:48:56.371787",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

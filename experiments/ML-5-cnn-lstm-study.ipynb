{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "959ae1e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T11:29:42.855826Z",
     "iopub.status.busy": "2025-11-13T11:29:42.855366Z",
     "iopub.status.idle": "2025-11-13T12:55:28.425578Z",
     "shell.execute_reply": "2025-11-13T12:55:28.424742Z"
    },
    "papermill": {
     "duration": 5145.57618,
     "end_time": "2025-11-13T12:55:28.428214",
     "exception": false,
     "start_time": "2025-11-13T11:29:42.852034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in uso: cuda\n",
      "--- 1. Caricamento e Pulizia Iniziale ---\n",
      "Trovata colonna 'Team Name': n_legs\n",
      "Calcolo Feature Engineering...\n",
      "\n",
      "--- Avvio K-Fold con Ensemble Strategy ---\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Fold 1 Best Val F1: 0.9687\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Fold 2 Best Val F1: 0.9632\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Fold 3 Best Val F1: 0.9452\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Fold 4 Best Val F1: 0.9477\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Fold 5 Best Val F1: 0.9456\n",
      "\n",
      "--- Ricerca Soglie Ottimali su OOF ---\n",
      "Soglie Trovate: Low>0.45, High>0.45 -> OOF F1: 0.9542\n",
      "\n",
      "--- Generazione Submission (Ensemble) ---\n",
      "Fatto! Submission creata.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "import re\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "# Ignora avvisi non critici\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- CONFIGURAZIONE HARDWARE ---\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device in uso: {device}\")\n",
    "\n",
    "# --- PARAMETRI GLOBALI ---\n",
    "# 1. Definiamo le colonne\n",
    "JOINT_COLS = [f'joint_{i:02d}' for i in range(30)]\n",
    "SURVEY_COLS = ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
    "STATIC_COLS = []  # SVUOTATO (Data Analysis: erano cheat code inaffidabili)\n",
    "TIME_COL = 'time'\n",
    "\n",
    "# 2. Iperparametri Architettura & Training\n",
    "WINDOW_SIZE = 40        # (Advice 11/11)\n",
    "STRIDE = 10\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 50\n",
    "GRADIENT_CLIP_VALUE = 1.0 # (Advice 10/11)\n",
    "K_FOLDS = 5\n",
    "LABEL_SMOOTHING = 0.1     # (Advice 09/11)\n",
    "\n",
    "# --- 1. CARICAMENTO E PULIZIA ---\n",
    "print(\"--- 1. Caricamento e Pulizia Iniziale ---\")\n",
    "try:\n",
    "    df_features_raw = pd.read_csv('/kaggle/input/pirate/pirate_pain_train.csv')\n",
    "    df_labels_raw = pd.read_csv('/kaggle/input/pirate/pirate_pain_train_labels.csv')\n",
    "    df_test_raw = pd.read_csv('/kaggle/input/pirate/pirate_pain_test.csv')\n",
    "except:\n",
    "    # Fallback per path locale\n",
    "    df_features_raw = pd.read_csv('pirate_pain_train.csv')\n",
    "    df_labels_raw = pd.read_csv('pirate_pain_train_labels.csv')\n",
    "    df_test_raw = pd.read_csv('pirate_pain_test.csv')\n",
    "\n",
    "# Gestione Text Column (Team Name)\n",
    "exclude_cols = ['label', 'sample_index']\n",
    "string_cols = df_features_raw.select_dtypes(include=['object']).columns.tolist()\n",
    "string_cols = [c for c in string_cols if c not in exclude_cols]\n",
    "\n",
    "TEXT_COL = None\n",
    "TEXT_VOCAB_SIZE = 0\n",
    "\n",
    "if len(string_cols) > 0:\n",
    "    TEXT_COL = string_cols[0] \n",
    "    print(f\"Trovata colonna 'Team Name': {TEXT_COL}\")\n",
    "    \n",
    "    def clean_team_name(text):\n",
    "        if pd.isna(text): return \"unknown\"\n",
    "        return re.sub(r'[^a-z0-9]', '', str(text).lower())\n",
    "\n",
    "    df_features_raw[TEXT_COL] = df_features_raw[TEXT_COL].apply(clean_team_name)\n",
    "    df_test_raw[TEXT_COL] = df_test_raw[TEXT_COL].apply(clean_team_name)\n",
    "    \n",
    "    le_text = LabelEncoder()\n",
    "    all_text = pd.concat([df_features_raw[TEXT_COL], df_test_raw[TEXT_COL]], axis=0)\n",
    "    le_text.fit(all_text)\n",
    "    \n",
    "    df_features_raw[TEXT_COL] = le_text.transform(df_features_raw[TEXT_COL])\n",
    "    df_test_raw[TEXT_COL] = le_text.transform(df_test_raw[TEXT_COL])\n",
    "    \n",
    "    TEXT_VOCAB_SIZE = len(le_text.classes_)\n",
    "else:\n",
    "    print(\"Nessuna colonna 'Team Name' trovata.\")\n",
    "\n",
    "# --- 2. FEATURE ENGINEERING (Advice 12/11 + Delta) ---\n",
    "def engineer_features(df):\n",
    "    df_eng = df.copy()\n",
    "    grouped = df_eng.groupby('sample_index')\n",
    "    \n",
    "    # A. Delta (VelocitÃ )\n",
    "    for col in JOINT_COLS:\n",
    "        df_eng[f'd_{col}'] = grouped[col].diff().fillna(0)\n",
    "    \n",
    "    # B. Time Cyclicity (Advice 12/11)\n",
    "    # Normalizziamo il tempo su un ciclo presunto (es. max del dataset o fisso 180)\n",
    "    max_time_val = df_eng[TIME_COL].max() + 1 \n",
    "    df_eng['sin_time'] = np.sin(2 * np.pi * df_eng[TIME_COL] / max_time_val)\n",
    "    df_eng['cos_time'] = np.cos(2 * np.pi * df_eng[TIME_COL] / max_time_val)\n",
    "\n",
    "    # C. Drop Costante\n",
    "    if 'joint_30' in df_eng.columns:\n",
    "        df_eng = df_eng.drop(columns=['joint_30'])\n",
    "        \n",
    "    return df_eng\n",
    "\n",
    "print(\"Calcolo Feature Engineering...\")\n",
    "df_features_engineered = engineer_features(df_features_raw)\n",
    "df_test_engineered = engineer_features(df_test_raw)\n",
    "\n",
    "# Definizione Colonne Continue Finali\n",
    "DELTA_JOINT_COLS = [f'd_{col}' for col in JOINT_COLS]\n",
    "CONTINUOUS_COLS = JOINT_COLS + DELTA_JOINT_COLS + ['sin_time', 'cos_time']\n",
    "\n",
    "# Vocabolari per Embedding (Advice 07/11)\n",
    "survey_vocab_sizes = [int(df_features_engineered[c].max() + 1) for c in SURVEY_COLS]\n",
    "time_vocab_size = int(df_features_engineered[TIME_COL].max() + 1)\n",
    "\n",
    "# Mappatura Label\n",
    "label_mapping = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "df_labels_raw['label_encoded'] = df_labels_raw['label'].map(label_mapping)\n",
    "\n",
    "# --- 3. DATASET & SAMPLER ---\n",
    "class PiratePainDataset(Dataset):\n",
    "    def __init__(self, features_df, labels_df, sample_indices, window_size, stride, text_col=None, augment=False):\n",
    "        self.features_df = features_df\n",
    "        self.labels_df = labels_df.set_index('sample_index') if labels_df is not None else None\n",
    "        self.sample_indices = sample_indices\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.text_col = text_col\n",
    "        self.augment = augment \n",
    "        \n",
    "        self.grouped_features = dict(tuple(features_df.groupby('sample_index')))\n",
    "        self.indices = self._create_indices()\n",
    "\n",
    "    def _create_indices(self):\n",
    "        indices = []\n",
    "        for sample_idx in self.sample_indices:\n",
    "            if sample_idx not in self.grouped_features: continue\n",
    "            data = self.grouped_features[sample_idx]\n",
    "            n_timesteps = len(data)\n",
    "            for start in range(0, n_timesteps - self.window_size + 1, self.stride):\n",
    "                indices.append((sample_idx, start, start + self.window_size))\n",
    "        return indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_idx, start, end = self.indices[idx]\n",
    "        window_data = self.grouped_features[sample_idx].iloc[start:end]\n",
    "\n",
    "        # 1. Continui + Augmentation\n",
    "        vals = window_data[CONTINUOUS_COLS].values\n",
    "        if self.augment:\n",
    "            noise = np.random.normal(0, 0.02, vals.shape) \n",
    "            vals = vals + noise\n",
    "        x_cont = torch.tensor(vals, dtype=torch.float)\n",
    "        \n",
    "        # 2. Embeddings (Survey + Time)\n",
    "        x_survey = torch.tensor((window_data[SURVEY_COLS].values + 1), dtype=torch.long)\n",
    "        x_time = torch.tensor((window_data[TIME_COL].values + 1), dtype=torch.long)\n",
    "        \n",
    "        # 3. Text\n",
    "        x_text = torch.tensor(0, dtype=torch.long)\n",
    "        if self.text_col:\n",
    "            val = window_data[self.text_col].iloc[0]\n",
    "            x_text = torch.tensor(val, dtype=torch.long)\n",
    "\n",
    "        # 4. Label\n",
    "        label = torch.tensor(-1, dtype=torch.long)\n",
    "        if self.labels_df is not None:\n",
    "            label = torch.tensor(self.labels_df.loc[sample_idx, 'label_encoded'], dtype=torch.long)\n",
    "\n",
    "        # Nota: Static rimosso\n",
    "        return x_cont, x_survey, x_time, x_text, label\n",
    "\n",
    "def get_weighted_sampler(dataset, labels_df):\n",
    "    sample_to_label = labels_df.set_index('sample_index')['label_encoded'].to_dict()\n",
    "    label_counts = labels_df['label_encoded'].value_counts().sort_index()\n",
    "    class_weights = 1.0 / label_counts\n",
    "    \n",
    "    weights = []\n",
    "    for idx_tuple in dataset.indices:\n",
    "        s_idx = idx_tuple[0]\n",
    "        if s_idx in sample_to_label:\n",
    "            l = sample_to_label[s_idx]\n",
    "            weights.append(class_weights[l])\n",
    "        else:\n",
    "            weights.append(0)\n",
    "    return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "# --- 4. LOSS FUNCTION (Advice 09/11) ---\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean', label_smoothing=0.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(\n",
    "            inputs, targets, reduction='none', weight=self.alpha, \n",
    "            label_smoothing=self.label_smoothing\n",
    "        )\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean() if self.reduction == 'mean' else focal_loss.sum()\n",
    "\n",
    "# --- 5. MODELLO CNN-LSTM (Advice 13/11) ---\n",
    "class PiratePainModel(nn.Module):\n",
    "    def __init__(self, n_continuous, survey_vocab_sizes, time_vocab_size, text_vocab_size, lstm_hidden=128, n_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.emb_surveys = nn.ModuleList([nn.Embedding(v+2, 4) for v in survey_vocab_sizes])\n",
    "        self.emb_time = nn.Embedding(time_vocab_size+2, 8)\n",
    "        \n",
    "        self.use_text = (text_vocab_size > 0)\n",
    "        text_dim = 8 if self.use_text else 0\n",
    "        if self.use_text:\n",
    "            self.emb_text = nn.Embedding(text_vocab_size+2, 8)\n",
    "            \n",
    "        # Dimensione Input Totale\n",
    "        total_survey_dim = len(survey_vocab_sizes) * 4\n",
    "        input_dim = n_continuous + total_survey_dim + 8 + text_dim\n",
    "        \n",
    "        # CNN Block (Feature Extraction)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # LSTM Block (Temporal Memory)\n",
    "        self.lstm = nn.LSTM(128, lstm_hidden, num_layers=2, batch_first=True, dropout=0.3)\n",
    "        \n",
    "        # Classifier\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(lstm_hidden, n_classes)\n",
    "\n",
    "    def forward(self, x_cont, x_survey, x_time, x_text):\n",
    "        batch_size, seq_len, _ = x_cont.shape\n",
    "        \n",
    "        # Embeddings\n",
    "        e_surv = [emb(x_survey[:,:,i]) for i, emb in enumerate(self.emb_surveys)]\n",
    "        e_time = self.emb_time(x_time)\n",
    "        \n",
    "        # Concatenazione Base\n",
    "        features = [x_cont] + e_surv + [e_time]\n",
    "        if self.use_text:\n",
    "            e_txt = self.emb_text(x_text).unsqueeze(1).repeat(1, seq_len, 1)\n",
    "            features.append(e_txt)\n",
    "            \n",
    "        full_input = torch.cat(features, dim=2) # (B, Seq, Feat)\n",
    "        \n",
    "        # CNN Pass (Requires Permute: B, Feat, Seq)\n",
    "        x = full_input.permute(0, 2, 1)\n",
    "        x = self.cnn(x)\n",
    "        \n",
    "        # LSTM Pass (Requires Permute back: B, Seq, Feat)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        out, _ = self.lstm(x)\n",
    "        \n",
    "        # Classify last step\n",
    "        logits = self.classifier(self.dropout(out[:, -1, :]))\n",
    "        return logits\n",
    "\n",
    "# --- 6. TRAINING & VALIDATION UTILS ---\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xc, xs, xt, xtxt, y in loader:\n",
    "        xc, xs, xt, xtxt, y = xc.to(device), xs.to(device), xt.to(device), xtxt.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xc, xs, xt, xtxt)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_VALUE)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# --- 7. MAIN LOOP (K-FOLD + ENSEMBLE) ---\n",
    "print(\"\\n--- Avvio K-Fold con Ensemble Strategy ---\")\n",
    "\n",
    "all_sample_indices = df_labels_raw['sample_index'].unique()\n",
    "all_labels_strat = df_labels_raw.set_index('sample_index').loc[all_sample_indices]['label_encoded'].values\n",
    "\n",
    "# Setup per OOF\n",
    "oof_probs = np.zeros((len(all_sample_indices), 3))\n",
    "oof_targets = np.zeros(len(all_sample_indices))\n",
    "sample_to_idx = {s: i for i, s in enumerate(all_sample_indices)}\n",
    "models_list = [] \n",
    "\n",
    "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(all_sample_indices, all_labels_strat)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{K_FOLDS} ---\")\n",
    "    \n",
    "    train_samples = all_sample_indices[train_idx]\n",
    "    val_samples = all_sample_indices[val_idx]\n",
    "    \n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    train_subset = df_features_engineered[df_features_engineered['sample_index'].isin(train_samples)]\n",
    "    scaler.fit(train_subset[CONTINUOUS_COLS])\n",
    "    \n",
    "    df_fold = df_features_engineered.copy()\n",
    "    df_fold[CONTINUOUS_COLS] = scaler.transform(df_fold[CONTINUOUS_COLS])\n",
    "    \n",
    "    # Dataset\n",
    "    train_ds = PiratePainDataset(df_fold, df_labels_raw, train_samples, WINDOW_SIZE, STRIDE, TEXT_COL, augment=True)\n",
    "    val_ds = PiratePainDataset(df_fold, df_labels_raw, val_samples, WINDOW_SIZE, STRIDE, TEXT_COL, augment=False)\n",
    "    \n",
    "    sampler = get_weighted_sampler(train_ds, df_labels_raw)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, shuffle=False, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Init\n",
    "    model = PiratePainModel(\n",
    "        n_continuous=len(CONTINUOUS_COLS), \n",
    "        survey_vocab_sizes=survey_vocab_sizes, \n",
    "        time_vocab_size=time_vocab_size,\n",
    "        text_vocab_size=TEXT_VOCAB_SIZE\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    criterion = FocalLoss(alpha=None, gamma=2.0, label_smoothing=LABEL_SMOOTHING)\n",
    "    \n",
    "    best_v_f1 = 0\n",
    "    best_model_wts = None\n",
    "    \n",
    "    # Epochs\n",
    "    for ep in range(EPOCHS):\n",
    "        t_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "        \n",
    "        # Validazione OOF\n",
    "        model.eval()\n",
    "        val_logits_list, val_sample_indices_list = [], []\n",
    "        window_sample_map_val = [x[0] for x in val_ds.indices]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for xc, xs, xt, xtxt, y in val_loader:\n",
    "                xc, xs, xt, xtxt = xc.to(device), xs.to(device), xt.to(device), xtxt.to(device)\n",
    "                logits = model(xc, xs, xt, xtxt)\n",
    "                val_logits_list.extend(logits.cpu().numpy())\n",
    "        \n",
    "        # Soft Voting\n",
    "        df_val_logits = pd.DataFrame(val_logits_list, columns=[0, 1, 2])\n",
    "        df_val_logits['sample_index'] = window_sample_map_val\n",
    "        df_val_probs = df_val_logits.groupby('sample_index').mean()\n",
    "        \n",
    "        current_val_probs = torch.softmax(torch.tensor(df_val_probs.values), dim=1).numpy()\n",
    "        current_val_preds = np.argmax(current_val_probs, axis=1)\n",
    "        \n",
    "        current_val_indices = df_val_probs.index\n",
    "        current_val_labels = df_labels_raw.set_index('sample_index').loc[current_val_indices]['label_encoded'].values\n",
    "        \n",
    "        v_f1 = f1_score(current_val_labels, current_val_preds, average='weighted')\n",
    "        \n",
    "        if v_f1 > best_v_f1:\n",
    "            best_v_f1 = v_f1\n",
    "            best_model_wts = model.state_dict()\n",
    "            # Save OOF\n",
    "            for idx, s_idx in enumerate(current_val_indices):\n",
    "                global_idx = sample_to_idx[s_idx]\n",
    "                oof_probs[global_idx] = current_val_probs[idx]\n",
    "                oof_targets[global_idx] = current_val_labels[idx]\n",
    "\n",
    "    print(f\"Fold {fold+1} Best Val F1: {best_v_f1:.4f}\")\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    models_list.append(model)\n",
    "\n",
    "# --- 8. OTTIMIZZAZIONE SOGLIE ---\n",
    "print(\"\\n--- Ricerca Soglie Ottimali su OOF ---\")\n",
    "best_thresh = (0.0, 0.0)\n",
    "best_score = 0.0\n",
    "\n",
    "for t_high in np.arange(0.15, 0.50, 0.01):\n",
    "    for t_low in np.arange(0.20, 0.55, 0.01):\n",
    "        if t_low >= t_high: continue\n",
    "        \n",
    "        preds = []\n",
    "        for p in oof_probs:\n",
    "            if p[2] > t_high: preds.append(2)\n",
    "            elif p[1] > t_low: preds.append(1)\n",
    "            else: preds.append(0)\n",
    "            \n",
    "        s = f1_score(oof_targets, preds, average='weighted')\n",
    "        if s > best_score:\n",
    "            best_score = s\n",
    "            best_thresh = (t_low, t_high)\n",
    "\n",
    "print(f\"Soglie Trovate: Low>{best_thresh[0]:.2f}, High>{best_thresh[1]:.2f} -> OOF F1: {best_score:.4f}\")\n",
    "\n",
    "# --- 9. INFERENZA FINALE ---\n",
    "print(\"\\n--- Generazione Submission (Ensemble) ---\")\n",
    "final_scaler = StandardScaler()\n",
    "final_scaler.fit(df_features_engineered[CONTINUOUS_COLS])\n",
    "df_test_scaled = df_test_engineered.copy()\n",
    "df_test_scaled[CONTINUOUS_COLS] = final_scaler.transform(df_test_scaled[CONTINUOUS_COLS])\n",
    "\n",
    "sub_indices = pd.read_csv('/kaggle/input/pirate/sample_submission.csv')['sample_index'].unique()\n",
    "test_ds_final = PiratePainDataset(df_test_scaled, None, sub_indices, WINDOW_SIZE, STRIDE, TEXT_COL, augment=False)\n",
    "test_loader_final = DataLoader(test_ds_final, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "window_sample_map_test = [x[0] for x in test_ds_final.indices]\n",
    "\n",
    "ensemble_logits = None\n",
    "\n",
    "for i, model in enumerate(models_list):\n",
    "    model.eval()\n",
    "    fold_logits = []\n",
    "    with torch.no_grad():\n",
    "        for xc, xs, xt, xtxt, _ in test_loader_final:\n",
    "            xc, xs, xt, xtxt = xc.to(device), xs.to(device), xt.to(device), xtxt.to(device)\n",
    "            logits = model(xc, xs, xt, xtxt)\n",
    "            fold_logits.extend(logits.cpu().numpy())\n",
    "    \n",
    "    df_tmp = pd.DataFrame(fold_logits, columns=[0, 1, 2])\n",
    "    df_tmp['sample_index'] = window_sample_map_test\n",
    "    df_avg = df_tmp.groupby('sample_index').mean()\n",
    "    \n",
    "    if ensemble_logits is None:\n",
    "        ensemble_logits = df_avg\n",
    "    else:\n",
    "        ensemble_logits = ensemble_logits.add(df_avg, fill_value=0)\n",
    "\n",
    "ensemble_logits /= K_FOLDS\n",
    "final_probs = torch.softmax(torch.tensor(ensemble_logits.values), dim=1).numpy()\n",
    "\n",
    "final_preds_list = []\n",
    "thr_l, thr_h = best_thresh\n",
    "\n",
    "for p in final_probs:\n",
    "    if p[2] > thr_h: final_preds_list.append(2)\n",
    "    elif p[1] > thr_l: final_preds_list.append(1)\n",
    "    else: final_preds_list.append(0)\n",
    "\n",
    "final_series = pd.Series(final_preds_list, index=ensemble_logits.index)\n",
    "\n",
    "inv_map = {v: k for k, v in label_mapping.items()}\n",
    "submission = final_series.map(inv_map).reset_index()\n",
    "submission.columns = ['sample_index', 'label']\n",
    "\n",
    "sample_sub = pd.read_csv('/kaggle/input/pirate/sample_submission.csv')\n",
    "submission = submission.set_index('sample_index').reindex(sample_sub['sample_index']).reset_index()\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Fatto! Submission creata.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8720958,
     "sourceId": 13708868,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5151.768856,
   "end_time": "2025-11-13T12:55:31.101731",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-13T11:29:39.332875",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
